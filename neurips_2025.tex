\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025


% ready for submission
% \usepackage{neurips_2025}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2025}


% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2025}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}

\usepackage{makecell}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}

\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage[most]{tcolorbox}

% if you use cleveref..
\usepackage[capitalize,noabbrev] {cleveref}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{assumption}{Assumption}[section]

% Theorem environment (standard)
%\newtheorem{theorem}{Theorem}[section]  % or [subsection], etc.

% Remark environment with separate numbering
\newtheorem{remarkx}{Remark} [section] % no link to theorem counter
\newenvironment{remark}{\begin{remarkx}\normalfont}{\end{remarkx}}

%\theoremstyle{remark}
%\newtheorem{remark}[theorem]{Remark}
\newcommand{\zs}[1]{\textcolor{blue}{Shuai: #1}}

 \newcommand{\mw}[1]{\textcolor{blue}{MW: #1}}

\title{Contrastive Learning with Data Misalignment: Feature Purity, Training Dynamics and Theoretical Generalization Guarantees}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


% \author{%
%   David S.~Hippocampus\thanks{Use footnote for providing further information
%     about author (webpage, alternative address)---\emph{not} for acknowledging
%     funding agencies.} \\
%   Department of Computer Science\\
%   Cranberry-Lemon University\\
%   Pittsburgh, PA 15213 \\
%   \texttt{hippo@cs.cranberry-lemon.edu} \\
%   % examples of more authors
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \AND
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
% }

\author{%
  Jiawei Sun \\
  % Department of Electrical, Computer, and Systems Engineering \\
  Rensselaer Polytechnic Institute \\
  \texttt{sunj11@rpi.edu} \\
  \And
  Shuai Zhang \\
  % Assistant Professor \\
  % Department of Data Science \\
  New Jersey Institute of Technology \\
  \texttt{sz457@njit.edu} \\
  \AND
  Hongkang Li \\
  % Department of Electrical and Systems Engineering \\
  University of Pennsylvania \\
  \texttt{lihk@seas.upenn.edu} \\
  \And
  Meng Wang \\
  % Department of Electrical, Computer, and Systems Engineering \\
  Rensselaer Polytechnic Institute \\
  \texttt{wangm7@rpi.edu} \\
}



\begin{document}


\maketitle
% \vspace{-7mm}
\begin{abstract}
%  
Contrastive learning is a powerful framework for learning discriminative representations from image-text pairs. 
Despite its success, its theoretical foundations, especially when the image-text pair exhibits misalignment, remain underexplored. 
This paper provides the first theoretical analysis of contrastive learning under data misalignment, proving how the ground-truth modality-paired features are amplified while spurious features are suppressed through the training dynamics analysis. 
Specifically, we study two nonlinear encoders trained jointly with a contrastive loss and demonstrate that noisy (or misaligned) data pairs 
result in mixed representations and degrade the model's generalization ability.
In contrast, recaptioning and filtering improve the data alignment, 
which in turn purifies the features learned by neurons and subsequently enhances generalization.  
Our analysis identifies feature purity as a key factor in the success of contrastive learning and offers insights into how data quality and training procedures impact representation learning and downstream generalization. Theoretical insights are supported by experiments on standard benchmarks.
\end{abstract}


%  
\section{Introduction}
%  
Vision-language models (VLMs) have achieved strong performance across diverse multimodal tasks such as vision-language understanding and generation. State-of-the-art methods like CLIP~\cite{radford2021learning} and SimVLM~\cite{wang2021simvlm} use contrastive learning to train dual encoders on large-scale image-text pairs scraped from the web, aligning embeddings by pulling paired samples closer in a shared space. These models excel in zero-shot scenarios, requiring no task-specific fine-tuning.

However, web-sourced captions are often noisy or misaligned, containing irrelevant or spurious details that hinder cross-modal alignment and reduce representation quality. For example,~\cite{nguyen2024improving} cites an image of a blue Mercedes-Benz in a parking lot paired with the caption: "2003 Mercedes-Benz C240 sedan, Leather, MUST BE SEEN -- \$6199." The price information in this caption is only superficially correlated with the image and does not contribute meaningfully to understanding the image context.
To mitigate this issue, many works~\cite{fan2023improving, nguyen2024improving, wang2022ofa, an2022cont, rotstein2024fusecap, hu2022scaling, wang2022git} adopt text generation methods during VLM training to produce high-quality synthetic captions more faithful to the corresponding images. Models like LaCLIP~\cite{fan2023improving} and BLIP~\cite{li2022blip} show that such recaptioning improves both the quality and diversity of training data, leading to significantly better performance. Further,~\cite{nguyen2024improving} demonstrates that the cosine similarities between BLIP2 generated captions~\cite{li2023blip2} and their paired images is higher than that of raw captions.
\cite{levi2024double} analyzes conformity on MSCOCO and finds that it correlates with how common or rare an image–caption embedding is, reflecting its degree of alignment within the dataset.


Despite the impressive success of VLMs and the practical advancements driven by recaptioned texts, their theoretical foundations remain relatively underdeveloped. Several critical questions remain mostly open:  
\begin{tcolorbox}[
  colback=gray!10,
  colframe=black,
  boxsep=0pt,       % no inner padding
  left=2pt,         % small left margin
  right=2pt,        % small right margin
  top=2pt,          % small top margin
  bottom=2pt,       % small bottom margin
    before skip=2pt,
  after skip=2pt,
  enhanced
]
 
 How do contrastively pre-trained VLMs align modalities, extract feature representations, and achieve zero-shot capabilities? 
How does text recaptioning on noisy image-text pairs provably enhance generalization performance?
\end{tcolorbox} 

Notably, even the theory of vanilla multimodal contrastive learning is still incomplete. For instance, \cite{haochen2021provable, zhang2023generalization} extend spectral contrastive loss to multimodal settings, showing that the objective can be related to matrix factorization.
\cite{pareekunderstanding} provides a theoretical characterization of when data filtering improves multimodal contrastive learning, offering a complementary, data-centric perspective to objective-level analyses.
Also, \cite{huang2021makes, lee2021predicting, zadeh2020foundations} show that, under certain conditions, multimodal models outperform unimodal ones with better representations. However, these works assume an optimal solution to the non-convex problem without analyzing the training dynamics that lead to strong generalization.
The zero-shot ability of VLMs also lacks full theoretical study. To the best of our knowledge, only \cite{chen2023understanding} analyzes CLIP’s zero-shot performance, showing it learns shared features while ignoring modality-specific ones. Yet, their setup does not consider real-world issues like misalignment between image and text. Beyond standard contrastive learning, \cite{nakada2023understanding} proposes a modified loss using unpaired data to detect ground-truth pairs and improve results, but only for linear models. So far, no work has theoretically studied the effect of text recaptioning on VLMs.

\textbf{Contributions: }
To the best of our knowledge, this is the first theoretical work explaining why text recaptioning improves zero-shot generalization in VLMs, especially under image-text misalignment, where text may include spurious or missing features.
We analyze the training dynamics of stochastic gradient descent (SGD) in multimodal contrastive learning and derive the generalization behavior of the learned model. Our analysis uses a one-hidden-layer ReLU network, which remains the state-of-the-art model in theoretical studies of contrastive \cite{wen2021toward} and supervised learning \cite{ allen2022feature, zhang2023joint}.
All findings are validated empirically on practical VLMs like CLIP. A comparison to prior theory works is shown in Table~\ref{table:1}. Key contributions include:


\begin{table*}[t]
\centering
\caption{Comparison with existing theoretical works on contrastive learning.}
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l|c|c|c|c|c|c}
\toprule
Work & \makecell{Train \\ Dyn.} & Nonlinear & \makecell{Zero-shot \\ Gen.} & Recaption & \makecell{Multi- \\ modal} & \makecell{Joint \\ Encoder} \\
\midrule
(Wen \& Li, 2021) \cite{wen2021toward} & $\checkmark$ & $\checkmark$ & & & & \\
(Nakada et al., 2023) \cite{nakada2023understanding} & $\checkmark$ & & & & $\checkmark$ & $\checkmark$ \\
(Chen et al., 2024) \cite{chen2023understanding} & $\checkmark$ & & $\checkmark$ & & $\checkmark$ & \\
(Lee et al., 2021) \cite{lee2021predicting} & & $\checkmark$ & & & $\checkmark$ & \\
(Zhang et al., 2023a) \cite{zhang2023generalization} & & & & & $\checkmark$ & \\
(Pareek et al., 2025) \cite{pareekunderstanding} & $\checkmark$ & & & $\checkmark$ & $\checkmark$ & $\checkmark$ \\
\midrule
\textbf{This paper} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ \\
\bottomrule
\end{tabular}
\label{table:1}
\end{table*}

\textbf{1. Theoretical training dynamics and generalization analysis of contrastive learning in nonlinear VLMs.}
We provide a theoretical analysis of jointly training two nonlinear encoders with contrastive loss. % to extract correlated features from image-text pairs. 
Prior works on training dynamics in contrastive learning~\cite{wen2021toward, chen2023understanding, nakada2023understanding} either analyze a single encoder or are restricted to linear neural networks. In contrast, our analysis captures the joint learning behavior of both nonlinear encoders with ReLU activation functions.

\textbf{2. Theoretical characterization of the impact of misaligned image-text pairs on pre-training performance.}  
We analyze a data model with modality misalignment, where some texts may contain features spuriously correlated with the image and others may omit relevant features. We show that spurious and missing features cause neurons to entangle true and irrelevant representations, which hinders the ability of the vision-language model to disentangle semantic components, ultimately degrading generalization performance.


\textbf{3. Theoretical justification of enhanced out-of-domain generalization through pre-training with text recaptioning.} 
%Rather than assuming that synthetic text is of higher quality, 
We first analyze the training dynamics of the text generation process %and show that the resulting generator inherently favors relevant features over spurious ones. Combined with filtering, we 
and formally prove that the resulting text after recaptioning has reduced spurious correlation and enhanced   % the generated synthetic texts suppress spurious features and enhance
semantic relevance with the corresponding images.
When these filtered texts are used for contrastive pre-training, the resulting model exhibits improved feature purity and succeeds in out-of-domain zero-shot classification, whereas the model trained on raw data provably fails.

%  
\subsection{Related Works}
%  
\textbf{Vision-Language Models:}  
VLMs~\cite{yu2022coca, wang2023image, radford2021learning, jia2021scaling, li2020oscar, li2021align} are trained via contrastive learning on large web-sourced image-text pairs. Following CLIP, later models~\cite{li2022supervision, alayrac2022flamingo, yao2022filip} aim to boost zero-shot performance. 
Data quality has become a key bottleneck, leading to recent filtering efforts~\cite{fan2023improving, li2023blip2, wang2024cliploss, kim2025hype, li2023backtranslation}. 
For example, LaCLIP~\cite{fan2023improving} uses LLM-generated caption rewrites as augmentation, and BLIP~\cite{li2022blip} leverages synthetic captions to drop noisy pairs, enhancing feature quality and robustness.


\textbf{Other Theoretical Exploration on Contrastive Learning.}
Recent studies explore why contrastive learning yields effective representations.
\cite{wang2020understanding} identifies alignment and uniformity as key properties of contrastive loss.
\cite{haochen2021provable} shows that solving auxiliary prediction tasks improves contrastive representations.
\cite{tian2020understanding} highlights the role of inductive biases in shaping learning dynamics.
\cite{li2023identifiability} proves that multimodal contrastive learning can recover shared latent factors under a generative model.





% \mw{The bullet of "Theory of contrastive learning" is too broad. I actually do not see the reason that some references are discussed in the paragraph line 42 after the question block, but these references are discussed here. At least   the wording you use at both places are similar. If you want to divide into more relevant and less relevant papers, provide a better reasoning and revise the bullet title more accurately. }

%  
\section{Problem Formulation and Algorithm}
%  
VLMs leverage large-scale web-based datasets containing paired visual and textual data to pre-train two separate encoders: an image encoder $f$ and a text encoder $h$, parameterized by weights $\mathbf{W}$ and $\mathbf{V}$, respectively. Contrastive learning serves as the core framework, ensuring the learned embeddings of matching pairs are closer while separating mismatched pairs.

Specifically, let $S$ be the indices of the image-text pairs, e.g.,  \((x_p, y_p)\) with \(p \in S\).  \((x_p, y_p)\) is referred to as a positive pair, while \((x_p, y_n)\) with \(p \neq n\) is referred to as a negative pair. 
We minimize  the following spectral loss function:
\begin{equation}
% \small
\label{eqn:contrastive}
L(f, h) = \sum_{p \in S} \left[ 
    -\langle f(x_p), h(y_p) \rangle + 
    \sum_{n \in S \setminus \{p\}} \frac{ \left( \langle f(x_n), h(y_p) \rangle \right)^2 }{2\tau} + 
    \sum_{n \in S \setminus \{p\}} \frac{ \left( \langle f(x_p), h(y_n) \rangle \right)^2 }{2\tau}
\right]
\end{equation}
where the hyper-parameter \(\tau > 0\) is referred as the temperature.
The spectral contrastive loss $L$ in (\ref{eqn:contrastive})
  has been extensively utilized in recent theoretical works \cite{haochen2021provable, shen2022connect, zhang2023generalization}. Although it
 differs from the commonly used SimCLR \cite{chen2020simple} in practice, the spectral contrastive loss closely resembles SimCLR numerically, as shown in \cite{haochen2021provable}.


 
\subsection{Training Framework}
 
\label{sec:Framework}
Let \( S= S_h \cup S_w \) include  
  human-annotated high-quality image-text pairs with indices in \(S_h\) and noisy web low-quality dataset with indices in \(S_w\).
Due to the inherently noisy nature of web data, the learned embeddings from~\eqref{eqn:contrastive} may be suboptimal. To mitigate this, many practical training methods~\cite{li2022blip, fan2023improving} incorporate recaptioned text to improve the quality and diversity of image-text pairs. While specific implementations vary, most frameworks follow a similar four-stage approach:

(S1) \textbf{Image-text contrastive pre-training (ITCP) on raw data:} 
    The image encoder \(f\) and text encoder \(h\) are trained using the image-text pairs \(\{(x_p, y_p)\}_{p\in S}\)  by minimizing the contrastive loss as in (\ref{eqn:contrastive}). Let \(\overline{\mathbf{W}}\) and \(\overline{\mathbf{V}}\) denote the learned weights in   \(f\) and   \(h\). 
 We then estimate the image and text embeddings of $(x_p, y_p)$ by \( z_{x_p}^{\prime} = f_{\overline{\mathbf{W}}}(x_p)\) and \( z_{y_p}^{\prime} = h_{\overline{\mathbf{V}}}(y_p)\). Due to the low-quality data in $S_w$ when training the encoders, these estimations might not be accurate.

(S2) \textbf{Generating text captions:} 
The high-quality data pairs in \(S_h\) are used to finetune an image-grounded text decoder \(G\), which maps an image \(x_p\) to text through \(G(x_p)\). Then, the learned \(G\) is  applied to every image $x_p$ %-text pair \((x_p, y_p)\) 
in \(S_w\) to generate a synthetic caption \(\hat{y}_p = G(x_p)\). Next, the  estimated text embedding of $\hat{y}_p$ is computed as
\( \hat{z}_{y_p} = h_{\overline{\mathbf{V}}}(\hat{y}_p) =h_{\overline{\mathbf{V}}}(G(x_p))\),
where $\overline{\mathbf{V}}$ represents the  weights of $h$ learned from Stage (S1). 

(S3) \textbf{Filtering:}  
For every $(x_p, y_p)$ in \(S_w\), we compute the cosine similarity between the   image embedding   $z_{x_p}^{\prime}$ and the text embeddings of the original caption $z_{y_p}^{\prime}$ and the synthetic caption  $\hat{z}_{y_p}$, respectively. If the pair $(z_{x_p}^{\prime}, \hat{z}_{y_p})$ has higher similarity to each other than the pair  $(z_{x_p}^{\prime}, z_{y_p}^{\prime})$, $(x_p, y_p)$ is replaced with $(x_p, \hat{y}_p)$. Let $\tilde{S}_w$ denote the index set of the resulting data pairs. By filtering noisy captions in \( S_w \) with synthetic captions that better align with image embeddings, \( \tilde{S}_w \) becomes a cleaner dataset.

(S4) \textbf{ITCP on filtered data:} 
The image encoder \(f\) and text encoder \(h\) are trained by minimizing the contrastive loss in (\ref{eqn:contrastive}), repeating the procedure from Stage (S1) with the only difference being that the original dataset \(S\) is replaced by   \(\tilde{S} = S_h \cup \tilde{S}_w\). The resulting loss is denoted by \(\tilde{L}(f,h)\).
Let $\widetilde{\mathbf{W}}$ and $\widetilde{\mathbf{V}}$ denote the resulting learned weights. $f_{\widetilde{\mathbf{W}}}$ and $g_{\widetilde{\mathbf{V}}}$ can produce improved embeddings compared with $f_{\overline{\mathbf{W}}}$ and $g_{\overline{\mathbf{V}}}$. 

We employ stochastic gradient descent (SGD) with step size $\eta$ and batch size $B$, following standard practice. Despite the non-convexity of \eqref{eqn:contrastive}, we present a detailed analysis of the resulting training dynamics and establish convergence guarantees in Section \ref{sec:main_results}. This stands in contrast to existing works \cite{saunshi2022understanding,xue2022investigating,chen2023understanding} that assume the attainability of a global optimum.

%Since the contrastive loss in (\ref{eqn:contrastive}) is nonconvex, we apply vanilla stochastic gradient descent (SGD) with step size \(\eta\) and batch size \(B\) and later provide the training dynamics analysis and convergence guarantee in Section \ref{sec:main_results}.


 
\subsection{Downstream Tasks}\label{subsec: zero-shot}
 
As a demonstration of the performance of the learned model $(f_{\widetilde{\mathbf{W}}}, g_{\widetilde{\mathbf{V}}})$, we consider a downstream image classification task in a zero-shot setting. Unlike the regression and binary classification tasks to evaluate the uni-modal contrastive learning in \cite{wen2021toward}, we consider a $K$-classification problem for any constant $K \geq 2$. Each class label is associated with a given text prompt $y_k$, where $k \in [K]$. For any image $x$ with its  ground-truth label $l_x \in [K]$, 
 the zero-shot predicted label by the pre-trained models ($f_{\widetilde{\mathbf{W}}}$, $g_{\widetilde{\mathbf{V}}}$) is computed as \(\arg\max_{k \in [K]} \langle f_{\widetilde{\mathbf{W}}}(x), g_{\widetilde{\mathbf{V}}}(y_k) \rangle\). This approach follows the typical setting of zero-shot image classification using VLMs \cite{chen2023understanding, jia2021scaling, li2022blip}. The prediction is considered accurate if and only  if \(\arg\max_{k \in [K]} \langle f_{\widetilde{\mathbf{W}}}(x), g_{\widetilde{\mathbf{V}}}(y_k) \rangle=l_x\).




 
\section{Technical Assumptions and Setups}
 
%In this section, 
We introduce a set of assumptions % to model our data and encoder. These assumptions 
that are either derived conceptually from the real data distribution or follow existing approaches in contrastive learning theory.

 
\subsection{Backbone of the Encoders} 
 
 We use a two-layer neural network with ReLU activation functions as the image and text encoders, respectively. Formally, we have  
\begin{definition}
 The image encoder \( f_{\mathbf{W}} : \mathbb{R}^{d_1} \rightarrow \mathbb{R}^m \) and text encoder \( h_{\mathbf{V}}: \mathbb{R}^{d_1} \rightarrow \mathbb{R}^m \) is
\begin{equation}\label{eqn:f}
% \small
f(x) = \left(f_{1}(x), \ldots, f_{m}(x)\right)^{\top} \in \mathbb{R}^{m}, \quad \textit{with}\quad 
f_{i}(x) = \sigma\left(\left\langle w_{i}, x\right\rangle-b_{i}\right) 
          - \sigma\left(-\left\langle w_{i}, x\right\rangle-b_{i}\right),
\end{equation}
\begin{equation}
% \small
\label{eqn:g}
h(y) = \left(h_{1}(y), \ldots, h_{m}(y)\right)^{\top} \in \mathbb{R}^{m}, \quad \textit{with}\quad
h_{i}(y) = \sigma\left(\left\langle v_{i}, y\right\rangle-b_{i}\right) 
- \sigma\left(-\left\langle v_{i}, y\right\rangle-b_{i}\right),
\end{equation}
where \( \sigma \) is ReLU function, and \(\mathbf{W} = [w_1, w_2, \ldots, w_m]^{\top}\), \(\mathbf{V} = [v_1, v_2, \ldots, v_m]^{\top} \in \mathbb{R}^{m\times d_1}\).
\end{definition}

Because deep neural networks are highly nonlinear, analyzing the training dynamics and resulting generalization performance of learned models remains challenging. As a result, existing theoretical studies are largely limited to one-hidden-layer neural networks~\cite{allen2022feature, zhang2023joint, wen2021toward, nakada2023understanding}, where the learning problem is already nonconvex. In this paper, we extend this line of research to a more complex setting, where two such encoders are jointly trained for image and text modalities.

 
\subsection{Data Model for ITCP}
 
Our data model in Assumption~\ref{assum:data} builds on the sparse coding framework, which has been widely used in both uni-modal contrastive learning for images~\cite{allen2022feature, wen2021toward} and multi-modal image-text contrastive learning~\cite{chen2023understanding}. 
This sparse coding model has been employed in  theoretical analyses \cite{arora2014new, barak2015dictionary, gregor2010learning} % and effective in modeling both 
because it effectively models the practical NLP \cite{arora2016latent, arora2018linear, prokhorov2021learning, deng2023measuring} and image data \cite{yang2011robust, xiao2025sc, yang2009linear}.



% \mw{I agree with Shuai, especially that some discussion is needed after Assumption 3.2. Like the noise is higher than the signal, etc. }
% \begin{definition}[Sparse coding model for image-text pairs]\label{def:data}
% For an image-text pairs \((x_p, y_p)\) where \(p \in S\),
%  the image $x_p \in \mathbb{R}^{d_1}$ and the text $y_p \in \mathbb{R}^{d_1}$ are generated i.i.d. from the following sparse coding form:
% \begin{equation}\label{eqn:x_p}
% \small
%     x_p = \mathbf{M}z_{x_p} + \xi_{x_p}, \quad
%     y_p = \mathbf{H}z_{y_p} + \xi_{y_p}.
% \end{equation}
% where $z_{x_p}$ and $z_{y_p}$ $\in \mathbb{R}^d$. We refer to $z_{x_p}$ and $z_{y_p}$ as the sparse signal and $\xi$ as the noise. We assume $d_1 = \text{poly}(d)$ for simplicity. 
% \end{definition}

% \begin{assumption}\label{assum:data}
% We assume the following  on $\mathbf{M}, \mathbf{H}, z, \xi$, respectively:

% (a) \textbf{Image dictionary:} $\mathbf{M} = [\mathbf{M}_1, \dots, \mathbf{M}_d] \in \mathbb{R}^{d_1 \times d}$ is column-orthonormal
% % with $\left\|\mathbf{M}_{j}\right\|_{\infty} \leq \widetilde{O}\left(1/\sqrt{d_{1}}\right)$

% (b) \textbf{Text dictionary:} $\mathbf{H} = [\mathbf{H}_1, \dots, \mathbf{H}_d] \in \mathbb{R}^{d_1 \times d}$ is column-orthonormal.

% (c) \textbf{Noisy features:} $\mathbf{M}^\perp = [\mathbf{M}_j]_{j \in [d_1] \setminus [d]}$ and $\mathbf{H}^\perp$ are orthogonal complements of $\mathbf{M}$ and $\mathbf{H}$.

% (d) \textbf{Additive noise:} $\xi_{x_p}, \xi_{y_p} \sim \mathcal{N}(0, \sigma_\xi^2 \mathbf{I}_{d_1})$ with $\omega(1/d_1) \le \sigma_\xi^2 \le O(1/d)$.

% (e) \textbf{Latent codes:} $z_{x_p} = (z_{x_p}^1, \dots, z_{x_p}^d)$ with $z_{x_p}^j \in \{0, \pm1\}$, where $|z_{x_p}^j| \sim \text{Bernoulli}(C_z/d)$.

% Since \( \omega(1/d_1) < \sigma_\xi^2 \le O\big(\sqrt{\log d / d^{1+c_0}}\big) \)\footnote{The columns \(\mathbf{M}_j\) and \(\mathbf{H}_j\) are column-orthonormal and each entry is bounded by \(\widetilde{O}(1/\sqrt{d_1})\), ensuring that inner products with noise \(\xi\) are sufficiently small.}, we have \( \|\xi\|_2^2 \gg \Theta(1) \gg \|\mathbf{M}z\|_2 \). Moreover, for any \( z_j \ne 0 \), it holds with high probability that \( |\langle \mathbf{M}z, \mathbf{M}_j \rangle| = \Theta(1) \) and \( |\langle \xi, \mathbf{M}_j \rangle| \le O(1/\sqrt{d}) \).
% \end{assumption}


 
\begin{assumption}[Sparse coding model for image-text pairs]\label{assum:data}
Each image-text pair \((x_p, y_p)\), \(p \in S\), is generated i.i.d. from the following sparse coding form:
\begin{equation}\label{eqn:x_p}
% \small
x_p = \mathbf{M} z_{x_p} + \xi_{x_p}, \quad
y_p = \mathbf{H} z_{y_p} + \xi_{y_p},
\end{equation}
where \(x_p, y_p \in \mathbb{R}^{d_1}\), \(z_{x_p}, z_{y_p} \in \mathbb{R}^d\), and \(d_1 = \mathrm{poly}(d)\). We assume:

(a) Image dictionary: $\mathbf{M} = [\mathbf{M}_1, \dots, \mathbf{M}_d] \in \mathbb{R}^{d_1 \times d}$ is column-orthonormal.

(b) Text dictionary: $\mathbf{H} = [\mathbf{H}_1, \dots, \mathbf{H}_d] \in \mathbb{R}^{d_1 \times d}$ is column-orthonormal.

(c) Additive noise: $\xi_{x_p}, \xi_{y_p} \sim \mathcal{N}(0, \sigma_\xi^2 \mathbf{I}_{d_1})$ with $\omega(1/d_1) \le \sigma_\xi^2 \le O\big(\sqrt{\log d / d^{1+c_0}}\big)$.

(d) Sparse latent vector: $z_{x_p} = (z_{x_p}^1, \dots, z_{x_p}^d)$ with $z_{x_p}^j \in \{0, \pm1\}$, where $|z_{x_p}^j| \sim \text{Bernoulli}(C_z/d)$.
\end{assumption}

Notably, we operate in a regime where the noise magnitude can dominate the signal: since \( \omega(1/d_1) < \sigma_\xi^2 \le O\big(\sqrt{\log d / d^{1+c_0}}\big) \)\footnote{The columns \(\mathbf{M}_j\) and \(\mathbf{H}_j\) are column-orthonormal with each entry bounded by \(\widetilde{O}(1/\sqrt{d_1})\), ensuring small inner products with isotropic noise.}, we have \( \|\xi\|_2^2 \gg \Theta(1) \gg \|\mathbf{M}z\|_2 \), indicating that the overall noise energy significantly exceeds that of the signal. 
 Nevertheless, we will show that contrastive learning remains effective even under such high-noise conditions, due to the encoders’ ability to extract denoised and purified features, as characterized in Theorem~\ref{theorem:retrain}. An intuitive explanation for why feature recovery is still possible lies in the different alignment properties of the signal and noise:  for any active feature \( z_j \ne 0 \), the signal aligns well with its corresponding basis: \( |\langle \mathbf{M}z, \mathbf{M}_j \rangle| = \Theta(1) \), while the noise contribution remains small, \( |\langle \xi, \mathbf{M}_j \rangle| \le O(1/\sqrt{d}) \).


We introduce Assumptions~\ref{assumption:high} and~\ref{assumption:low} to capture the characteristics of the dataset $S = S_h \cup S_w$. 
Notably, the number of high-quality pairs in \( S_h \) may be significantly fewer than that of low-quality pairs in \( S_w \), with \( |S_h| = \Theta(d^2) \) and \( |S_w| = \text{poly}(d) \gg \omega(d^2) \).
%\mw{For high-quality data \(S_h\), we assume the existence of learnable latent features, consistent with the standard assumption in prior work~\cite{chen2023understanding} that focuses exclusively on such data in contrastive learning.}




\begin{assumption}[High-quality image-text pairs]
\label{assumption:high} 
Every high-quality image-text pair \((x_p, y_p)\) with \(p \in S_h\)  satisfies \( z_{x_p} = z_{y_p} \), i.e., the image and text have the same latent vector. %where \( z_{x_p} \) and \( z_{y_p} \) denote the shared feature representations underlying the image and text, respectively. 
\end{assumption}
  
Compared to high-quality pairs in \(S_h\), low-quality pairs in \(S_w\) show modality misalignment due to spurious image-text correlations and missing descriptions of key visual features.

\begin{assumption}[Low-quality misaligned image-text pairs]
\label{assumption:low}
%Compared to high-quality pairs in \(S_h\), low-quality pairs in \(S_w\) exhibit modality misalignment in the learnable latent feature. 
%The misalignment arises from both  spurious correlations and less informative raw text relative to the image.
%Specifically, 
There exists a constant \(C_s \in (\omega(1/\log d), 1/2)\) such that for every low-quality pair \((x_p,y_p)\) in \(S_w\) and every image feature \(\mathbf{M}_j\) (\(j \in [d]\)) in \(x_p\), we have  
\begin{equation}
\label{eqn:cross}
% \small
\Pr\left(z_{y_p}^{j'} = z_{x_p}^j \mid |z_{x_p}^j| = 1\right) = C_s, \quad
\Pr\left(z_{y_p}^{j} = 0 \mid |z_{x_p}^j| = 1\right) = C_s,
\end{equation}
where the first term in (\ref{eqn:cross}) is the probability that a text feature \(\mathbf{H}_{j'}\) (\(j' \neq j\)) is spuriously correlated to the image feature \(\mathbf{M}_j\), and the second term is the probability that \(\mathbf{H}_j\) is missing in the text while the image feature \(\mathbf{M}_j\) exists.
\end{assumption}
Consider the blue Mercedes-Benz example from \cite{nguyen2024improving}. Here, \(\mathbf{M}_j\) denotes the car's visual feature, while \(\mathbf{H}_{j'}\) refers to unrelated price information spuriously correlated with \(\mathbf{M}_j\), illustrating the first term in \eqref{eqn:cross}. The correct text feature ``Mercedes-Benz'' is \(\mathbf{H}_j\); its absence reflects the omission of a relevant feature, as captured by the second term in \eqref{eqn:cross}. We focus on a single spurious pair \((j, j')\) for simplicity. Since our analysis depends on the total spurious feature probability (bounded by \(C_s\)), the results extend to multiple spurious features as long as their total probability stays within \(C_s\).

\iffalse 
\begin{assumption}[Low-quality misaligned image-text pairs]
\label{assumption:low}
Compared to high-quality pairs in \(S_h\), low-quality pairs in \(S_w\) exhibit modality misalignment in the learnable latent feature. 
The misalignment arises from both spurious correlations and missing relevant features in raw text.
Specifically, there exists a constant \(C_s \in (\omega(1/\log d), 1/2)\) such that for each image feature \(\mathbf{M}_j\) (\(j \in [d]\)) in a low-quality pair \((x_p, y_p)\), \mw{at least one of the following two equations hold}
\begin{equation}
\label{eqn:cross1}
\Pr\left(z_{y_p}^{j'} = z_{x_p}^j \mid z_{x_p}^j = 1\right) = C_s, \quad j' \ne j,
\end{equation}
\begin{equation}
\label{eqn:cross2}
\Pr\left(z_{y_p}^{j} = 0 \mid z_{x_p}^j = 1\right) = C_s.
\end{equation}
That is, each relevant image feature may be spuriously mapped to another and entirely omitted with probability \(C_s\).
\end{assumption}

\fi 

%\mw{(6) and (7) must happen together in one sample? Every sample must have spurious correlations and must have missing features? Could it be relaxed, like less than or equal to?}
%Equations~\eqref{eqn:cross1}–\eqref{eqn:cross2} define feature-level spurious correlations and missing features respectively.
%We consider 

%corruption probabilities conditioned on the presence of that feature in the image. We do not require that every low-quality sample exhibits both types of misalignment simultaneously.

%\mw{Do you want to change "and" to "or" or "at least one of the following holds?" These three are different, but be precise.

%Does (6) and (7) have to be equal to Cs? Could it be less than or equal to}
 
\subsection{Image-Grounded Text Decoder $G$ in Stage (S2)} 
 
Recall that $G$ is employed in Stage (S2) to generate synthetic text captions. In practice,  the core idea behind the widely adopted approaches \cite{li2022blip,yu2022coca, wang2023image} is to train the encoder-decoder model \( G \) and
leverage the high-quality image-text pairs \( S_h \) to improve its performance. In this paper, we consider a simplified form of \(G\), given by:
\begin{equation}
\label{eqn:G}
G(x_p) = \mathbf{V}^{\top} \sigma(\mathbf{W}x_p),
\end{equation}
where \(\sigma\) denotes the ReLU function. The parameters \(\mathbf{W}\) and \(\mathbf{V}\) are learned by solving  
\begin{equation}
\label{eqn:LC}
% \small
\min_{\mathbf{W}, \mathbf{V}} L_C  = \sum_{p \in S_h} \frac{1}{2} \left\| \mathbf{V}^{\top} \sigma(\mathbf{W}x_p) - y_p \right\|_2^2,
\end{equation}
  initialized at \(\overline{\mathbf{W}}\) and \(\overline{\mathbf{V}}\), using SGD with step size $\eta$. 
Although \( G \) in (\ref{eqn:G}) is a conceptual simplification, where \(\sigma(\mathbf{W}x_p)\) acts as the encoder and \(\mathbf{V}^{\top}\) as the decoder, it serves as a realistic abstraction to illustrate the underlying advantages of synthetic text caption generation.



 
\subsection{Zero-Shot Generalization on Image Classification}
 
\label{sec:zeroshot}
We consider an out-of-domain (OOD) setting for testing images and text prompts as follows.

\textbf{Image:} Each test image \( x \) can be approximated by a sparse coding model with dictionary $\mathbf{M}'$,  %is generated as
\begin{equation}
    x = \mathbf{M}' z_x' + \xi_x, \quad \| z_x' \|_0 = \Theta(1), \quad \| z_x' \|_{\max} = \Theta(1),
\end{equation}
where \( \mathbf{M}' = \mathbf{M}\mathbf{P}_1 \), and \( \max_{i,j} |(\mathbf{P}_1)_{ij} - \delta_{ij}| \le O(1/\sqrt{d}) \) . The noise \( \xi_x \) matches the training distribution (Assumption~\ref{assum:data}(d)) and \(\delta_{ij}\) denotes the Kronecker delta function.

\textbf{Text:} Each class \( k \in [K] \) has a prompt that has a sparse decomposition
\begin{equation}
    y_k = \mathbf{H} z_{y_k}' + \xi_{y_k}, \quad \| z_{y_k}' \|_0 = \Theta(1), \quad \| z_{y_k}' \|_{\max} = \Theta(1).
\end{equation}

If \( x \) belongs to class \( k \), then among all \( K \) binary vectors $z_{y_{k'}}'$, \( z_x' \) is maximally aligned with \( z_{y_k}' \), %satisfying
\begin{equation}
    \| (z_x')^{\top} z_{y_k}'  \|_2 >  \| (z_x')^{\top} z_{y_{k'}}' \|_2, \quad \forall k' \neq k
\end{equation}
This formulation reflects the intuition that \( x \) belongs to class \( k \) if its sparse representation is most similar to the sparse representation of class \( k \)'s text prompt.


 
\section{Main Results}
\label{sec:main_results}
 
\subsection{Intuition and Informal Insights}
 
Before presenting our main results, we first offer an intuitive explanation of the encoder-learner’s success. To learn the latent representation \( z \) from input pair \((x, y)\), a well-trained image encoder \( f \) and text encoder \( g \) must ensure that each feature pair \((\mathbf{M}_j, \mathbf{H}_j)\) is captured by at least one neuron pair \((w_i, v_i)\), without interference from spurious signals. We call this a \emph{purified feature}, meaning the neuron pair encodes only one true feature with no contamination. In this case, \( \langle w_i, x \rangle \approx z_x^j \) and \( \langle v_i, y \rangle \approx z_y^j \), so \( f \) and \( g \) recover the full latent space \( z \). But in real data, where high-quality pairs in \( S_h \) are rare and noisy pairs with misaligned image-text pairs in \( S_w \) dominate, achieving this is difficult. See Appendix~\ref{sec:ProofScratch} for proof sketches and we summarize main findings below:

\textbf{(I) SGD provably solves the nonconvex training problems (\ref{eqn:contrastive}).} The existing training dynamics and convergence analyses are limited to either single-modal contrastive learning  \cite{wen2021toward}  or linear networks \cite{chen2023understanding,nakada2023understanding}. Theorem \ref{theorem:convergence} provides a convergence analysis of SGD for solving the nonconvex ITCG problem when the network contains nonlinear activations for both modalities.

\textbf{(II) Failure of learning due to spurious correlations.} 
Theorem \ref{theorem:pre-train} provides a negative result: if \( f \) and \( g \) are directly trained on the raw data \(S\), the model inevitably learns \( \mathbf{M}_j \) and \( \mathbf{M}_{j'} \) together via some \( w_i \), and \( \mathbf{H}_j \) and \( \mathbf{H}_{j'} \) together via some \( v_i \). As a result, the model fails to distinguish between these spuriously correlated features.

 \textbf{(III) Successful learning with recaptioning and filtering}. Theorem~\ref{theorem:caption_filtering} demonstrates that recaptioned texts significantly suppress spurious features and enhance relevant feature alignment.  Building on this, Theorem~\ref{theorem:retrain} states that training \( f \) and \( g \) on the recaptioned data \( \tilde{S} \) enables the resulting encoder pair to learn purified representations of \( \mathbf{M}_j \) and \( \mathbf{H}_j \) accurately, as if trained solely on sufficient high-quality data. 
This highlights the advantage of leveraging the recaptioned data \( \tilde{S}_w \).

\textbf{(IV) Enhanced zero-shot image classification accuracy due to text recaptioning}. 
The advantage of using synthetic text captions is further validated in downstream tasks. As shown in Theorem \ref{theorem:downstream}, for a zero-shot out-of-domain multi-class image classification task, ITCP trained using \(\tilde{S}\) achieves high accuracy, whereas ITCP directly using \( S \) fails to generalize accurately.


 
\subsection{Feature Purity Improvements in Converged Models via Recaptioned Data}
 
We first characterize the training dynamics and convergence of solving  (\ref{eqn:contrastive}) using SGD in Stage (S1) and (S4) in  Section~\ref{sec:Framework}. 
Let \(L^*\) and \(\tilde{L}^*\) denote the optimal values of the contrastive loss on the raw dataset \(S\) %(Stage S1 in Section~\ref{sec:Framework}) 
and the filtered dataset \(\tilde{S}\), %(Stage S2 in Section~\ref{sec:Framework}), 
respectively.
Note that \((\overline{\mathbf{W}}, \overline{\mathbf{V}})\) and $(\widetilde{\mathbf{W}}, \widetilde{\mathbf{V}})$ are the converged weights from contrastive training on \(S\) and \(\tilde{S}\) in Stage (S1) and (S4), respectively. 


\begin{theorem}[\textbf{Convergence of ITCP}]
\label{theorem:convergence}
Suppose Assumptions \ref{assum:data} to \ref{assumption:low} hold. 
Let the model complexity be \(m = d^{1.01}\), initialized at 
\(w_i^{(0)},v_i^{(0)} \sim \mathcal{N}(0, \sigma_0^2 \mathbf{I}_{d_1})\), where \(\sigma_0^2 = \Theta\left(\frac{1}{d_1 \text{poly}(d)}\right)\).
After \(T = \Theta\left(d^2 \log d\right)\) iterations with batch size $B=\Omega(d)$
and $\eta = O(1)$, the returned weights %to solve (\ref{eqn:contrastive}) using $S$, denoted by  $\overline{\mathbf{W}}$ and  $\overline{\mathbf{V}}$,  
achieve  a loss that is sufficiently close to the optimal loss in Stage (S1) and (S4), respectively,  %$L^*$, 
i.e., 
\begin{equation}
(L(f_{\overline{\mathbf{W}}}, h_{\overline{\mathbf{V}}}) - L^*)/\left| L^* \right| \leq o(1), \quad (\tilde{L}(f_{\widetilde{\mathbf{W}}}, h_{\widetilde{\mathbf{V}}}) - \tilde{L}^*)/\left| \tilde{L}^* \right| \leq o(1).
\end{equation}

\end{theorem}

\begin{remark}
     Theorem \ref{theorem:convergence} demonstrates that SGD iterations can converge to weights that achieve a  near optimal loss of (\ref{eqn:contrastive}), respectively. This result is of independent interest, as existing training dynamics and convergence analyses for contrastive loss are limited to linear networks. Here, we extend such analysis to nonconvex optimization settings where the network contains nonlinear ReLU activations. Next, we characterize the feature purity of the learned models.
\end{remark}

\begin{theorem}[\textbf{Unsuccessful learning of ITCP on
raw data \(S\) with low feature purity}]
\label{theorem:pre-train}
 
For each neuron pair \((\bar{w}_i, \bar{v}_i)\) in \((\overline{\mathbf{W}}, \overline{\mathbf{V}})\), there exists a spurious feature pair \( (j, j') \in [d] \) such that
\begin{equation}
\bar{w}_i = \alpha_{i,j} \mathbf{M}_j + \alpha_{i,j'} \mathbf{M}_{j'} + \mathbf{r}_i, \quad
\bar{v}_i = \alpha_{i,j} \mathbf{H}_j + \alpha_{i,j'} \mathbf{H}_{j'} + \mathbf{s}_i
\end{equation}
where \( \alpha_{i,j}^2 , \alpha_{i,j'}^2 = \Theta\left( \| \bar{w}_i \|_2^2 + \| \bar{v}_i \|_2^2 \right) \) and  
\( \|\mathbf{r}_i\|_2^2, \|\mathbf{s}_i\|_2^2 \le O( (\| \bar{w}_i \|_2^2 + \| \bar{v}_i \|_2^2)/d) \).
Moreover, for every spuriously correlated pair \((j, j')\), there exist at least \(\Omega(1)\) neuron pairs \((\bar{w}_i, \bar{v}_i)\) that primarily learn the mixed feature pair \((\mathbf{M}_j, \mathbf{M}_{j'},  \mathbf{H}_j,  \mathbf{H}_{j'})\).
\end{theorem}

\begin{remark}
Theorem \ref{theorem:pre-train} indicates that the model learned by ITCP on raw data achieves only limited feature purity. Specifically, a neuron pair \((\bar{w}_i, \bar{v}_i)\) learns a mixture of image and text features, respectively. \(\mathbf{M}_j\) and \(\mathbf{M}_{j'}\) are always mixed together, as are \(\mathbf{H}_j\) and \(\mathbf{H}_{j'}\). As a result, the learned weights $\overline{\mathbf{W}}$ and $\overline{\mathbf{V}}$ fail to produce purified representations, making it difficult to distinguish between features \(j\) and \(j'\), which ultimately degrades downstream performance shown in (\ref{eqn:downstream_pre-train}).
\end{remark}

\begin{theorem}[\textbf{Spurious feature suppression and relevant feature preservation by recaptioned texts}]
\label{theorem:caption_filtering}
After \(T = \Theta(d \log d)\) steps of SGD, the decoder \(G\) in \eqref{eqn:G}, finetuned by solving \eqref{eqn:LC}, converges to weights \((\hat{\mathbf{W}}, \hat{\mathbf{V}})\) with expected loss \(L_C \le \Theta(1/d)\). The recaptioned texts in  \(\tilde{S}_w\) are computed by  \(\hat{y}_p = G(x_p)\). 
Then for any index \(j \in [d]\) such that \(|z_{x_p}^j| = 1\), the decoder output satisfies:
\begin{equation}
\Pr(z_{\hat{y}_p}^{j} = 1 \mid |z_{x_p}^j| = 1) \ge 1 - \Theta\left(1/d\right), \quad 
\Pr(z_{\hat{y}_p}^{j'} = 1 \mid |z_{x_p}^j| = 1) \le \Theta\left(1/d\right), \quad \forall j' \ne j.
\end{equation}
\end{theorem}
\begin{remark}
After captioning and filtering, the resulting text contains fewer spurious features and more aligned feature pairs than raw data. Compared with Assumption~\ref{assumption:low}, the probability of spurious features can be reduced from a constant \(C_s\) in $S_w$ to \(\Theta(1/d)\) in \(\tilde{S}_w\), while the probability of retaining all aligned feature pairs increases from \(C_s\)  in \(S_w\) to \(1 - \Theta\left(1/d\right)\) in \(\tilde{S}_w\). The resulting dataset \(\tilde{S} = S_h \cup \tilde{S}_w\) has better-aligned image-text pairs, enabling higher feature purity in contrastive training. We next show how ITCP trained on \(\tilde{S}\) improves feature purity.
\end{remark}


\begin{theorem}[\textbf{Successful learning of ITCP on filtered data \(\tilde{S}\) with high feature purity}]
\label{theorem:retrain}
%Let \((\widetilde{\mathbf{W}}, \widetilde{\mathbf{V}})\) be the converged weights from contrastive training on \(\tilde{S}\).  
%Then, 
For each neuron pair \((\tilde{w}_i, \tilde{v}_i)\) in \((\widetilde{\mathbf{W}}, \widetilde{\mathbf{V}})\), there exists      \( j \in [d] \) such that  \((\tilde{w}_i, \tilde{v}_i)\)
primarily learns \((\mathbf{M}_j, \mathbf{H}_j)\)
\begin{equation}
\label{eqn:separation}
\tilde{w}_i = \tilde{\alpha}_{i,j} \mathbf{M}_j + \tilde{\mathbf{r}}_i, \quad
\tilde{v}_i = \tilde{\alpha}_{i,j} \mathbf{H}_j + \tilde{\mathbf{s}}_i
\end{equation}
where \( \tilde{\alpha}_{i,j}^2 = \Theta(\| \tilde{w}_i \|_2^2 + \| \tilde{v}_i \|_2^2) \) and 
\( \| \tilde{\mathbf{r}}_i \|_2^2, \| \tilde{\mathbf{s}}_i \|_2^2 \le O\left((\| \tilde{w}_i \|_2^2 + \| \tilde{v}_i \|_2^2)/d\right) \). Moreover, for every feature \(j \in [d]\), there exist at least \(\Omega(1)\) neuron pairs \((\tilde{w}_i, \tilde{v}_i)\) that primarily learn purified feature pair \((\mathbf{M}_j, \mathbf{H}_j)\).
 

\end{theorem}
\begin{remark}
\label{remark:compare} 
Theorem \ref{theorem:retrain} indicates that the model learned by ITCP on filtered data achieves a purified representation. Specifically, a neuron pair 
\((\tilde{w}_i, \tilde{v}_i)\) learns one single feature pair \((\mathbf{M}_j, \mathbf{H}_j)\), respectively. As a result, $\widetilde{\mathbf{W}}$ and $\widetilde{\mathbf{V}}$ yield purified representations that effectively separate individual features, enabling improved downstream performance shown in (\ref{eqn:downstream_retrain}).
\end{remark}



 
\subsection{Performance Comparison on Downstream Tasks}
 

We next compare the performance of the models ($f_{\overline{\mathbf{W}}}$, $g_{\overline{\mathbf{V}}}$) %, pre-trained on raw data $S$,
and ($f_{\widetilde{\mathbf{W}}}$, $g_{\widetilde{\mathbf{V}}}$) %, pre-trained on synthetic data $\tilde{S}$, 
on the zero-shot image classification problem with out-of-domain data described in Sections \ref{subsec: zero-shot} and \ref{sec:zeroshot}.
\begin{theorem} [Zero-Shot Image Classification]\label{theorem:downstream}   

For the OOD zero-shot $K$-class image classification problem, % with a setup in Section \ref{sec:zeroshot}, %the generalization performance is characterized as follows:
  the model ($f_{\overline{\mathbf{W}}}$, $g_{\overline{\mathbf{V}}}$) from ITCP using raw data has a constant failure probability:
   \begin{equation}
    \Pr\left( \arg\max_{k \in [K]} \langle f_{\overline{\mathbf{W}}}(x), g_{\overline{\mathbf{V}}}(y_k) \rangle = l_x \right) = 1 - \Theta(1);. \label{eqn:downstream_pre-train}
 \end{equation}
     In contrast, the model ($f_{\widetilde{\mathbf{W}}}$, $g_{\widetilde{\mathbf{V}}}$) from ITCP using filtered caption succeeds with high probability:
     \begin{equation}
    \Pr\left( \arg\max_{k \in [K]} \langle f_{\widetilde{\mathbf{W}}}(x), g_{\widetilde{\mathbf{V}}}(y_k) \rangle = l_x \right) = 1 - o(1). 
    \label{eqn:downstream_retrain}
  \end{equation}   
 \end{theorem}

\begin{remark}
 Theorem \ref{theorem:downstream} first demonstrates that the zero-shot performance of \((f_{\overline{\mathbf{W}}}, g_{\overline{\mathbf{V}}})\) is unsatisfactory, resulting from the low feature purity in \((f_{\overline{\mathbf{W}}}, g_{\overline{\mathbf{V}}})\), as established in Theorem \ref{theorem:pre-train}. 
Theorem \ref{theorem:downstream} further shows that \((f_{\widetilde{\mathbf{W}}}, g_{\widetilde{\mathbf{V}}})\) achieves accurate classification. This success is attributed to high feature purity in \((f_{\widetilde{\mathbf{W}}}, g_{\widetilde{\mathbf{V}}})\), as described in Theorem \ref{theorem:retrain}.  Note that Theorem \ref{theorem:downstream} holds for image data with a distribution shift from the training data. 
\end{remark}



 
\section{Experiment}
 
\subsection{Simulated  Experiment}
\label{sec:synthetic}


\begin{figure}[h]
    \centering
    % \setlength{\abovecaptionskip}{1pt}   % 图像和caption之间（上方）
    % \setlength{\belowcaptionskip}{-8pt}  % caption和正文之间（下方）
    \subfigure[]{
        \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{image/feature_number.png}
        \end{minipage}
    }
    ~ 
    \subfigure[]{
        \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{image/feature_magnitude.png}
        \end{minipage}
    }
    ~
    \subfigure[]{
        \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{image/classification_accuracy.png}
        \end{minipage}
    }
    ~
    \subfigure[]{
        \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{image/silhouette_score.png}
        \end{minipage}
    }
    \caption{Performance comparison of ITCP on raw data and filtered (recaptioned) data when the probability of spurious correlation $C_s$ changes. (a) Number of features that have purified representation in the model (b) Average magnitude of purified presentations (c) Zero-shot out-of-domain classification accuracy (d) Silhouette Score with cosine distance.}
    \label{fig:performance}
\end{figure}
 

\textbf{Experiment Setup.} We first validate our results via simulated experiments, using the same framework from Section~\ref{sec:Framework}. We adopt a more general spurious correlation model than Assumption~\ref{assumption:low}, allowing each \(\mathbf{M}_j\) to be spuriously linked with multiple \(\mathbf{H}_{j'}\) (\(j' \ne j\)), while keeping the total spurious correlation probability at \(C_s\).
We set \(d_1 = 2500\), \(d = 50\), \(|S_w| = 5000\), \(|S_h| = 1000\), and use \(m = 80\) neurons. Matrices \(\mathbf{M}, \mathbf{H}\) are drawn from standard Gaussians and orthonormalized via QR decomposition. Sparse codes \(z_x\) follows Bernoulli\((0.1)\) Noise variance \(\sigma_\xi^2 = 1/d\). SGD runs with batch size 500 and step size 0.001.
Downstream evaluation uses 5-way classification with test \(z_x \sim \text{Bernoulli}(0.2)\); class codes \(z_{y_k}\) partition the \(d\)-dim space. Results are averaged over 20 trials. Models \((\overline{\mathbf{W}}, \overline{\mathbf{V}})\) and \((\widetilde{\mathbf{W}}, \widetilde{\mathbf{V}})\) are trained on raw and filtered data, respectively.




% 
\textbf{Improved feature representation using filtered (recaptioned) data.} 
We say a weight \(\bar{w}_i\) learn a \textit{purified representation} of \(\mathbf{M}_j\) if its projection along \(\mathbf{M}_j\) achieves the largest magnitude and satisfies \(|\langle \bar{w}_i, \mathbf{M}_j \rangle|/\|\bar{w}_i\| > 0.5\). The same applies to \((\widetilde{\mathbf{W}}, \widetilde{\mathbf{V}})\). % trained on synthetic data, replacing \(\bar{w}_i\) with \(\tilde{w}_i\).
Figure~\ref{fig:performance}(a) shows the number of features \(\mathbf{M}_j\) (out of \(d = 50\) total features) for which at least one neuron in \(\overline{\mathbf{W}}\) (or \(\widetilde{\mathbf{W}}\), respectively) learns a purified representation. 
The results show that ITCP trained on filtered data learns purified representations for nearly all features, even at high spurious correlation levels (\(C_s = 0.3\)). In contrast, ITCP on raw data degrades significantly, with purity dropping faster as \(C_s\) increases.
Moreover, Figure~\ref{fig:performance}(b) shows the average of the largest projection magnitudes among neurons that learn purified features. The magnitude from \(\widetilde{\mathbf{W}}\) (ITCP on filtered data) is consistently higher than that from \(\overline{\mathbf{W}}\), indicating stronger purified representations. This aligns with Theorems~\ref{theorem:pre-train},~\ref{theorem:retrain} and Remark~\ref{remark:compare}.

\textbf{Improved zero-shot out-of-domain performace using filtered (recaptioned) data.}
Figure~\ref{fig:performance}(c) compares the classification accuracy of both models on zero-shot out-of-domain data. The model trained on filtered data consistently outperforms the one trained on raw data, with the performance gap widening as spurious correlations in the raw data increase.
We also adopt the widely used Silhouette Score (SS) with cosine distance \cite{yu2023contextually, mo2024revisiting,zhang2022self} to evaluate
feature embedding quality in different clusters, as shown in Figure~\ref{fig:performance}(d). A higher SS indicates better intra-class alignment and inter-class orthogonality, reflecting more purified representations. These results verify Theorem \ref{theorem:downstream}. 


\textbf{Impact of feature purity.} 
%As the spurious alignment probability \(C_s\) increases, both the Silhouette Score (SS) and zero-shot classification accuracy degrade. Notably,
When \(C_s\) reaches 0.35 in Figure~\ref{fig:performance}, even the filtered data fails to maintain full feature purification: the number of neurons learning disentangled representations of all \(d = 50\) features drops significantly (Figure~\ref{fig:performance}(a)), and the SS (Figure~\ref{fig:performance}(d)) and classification accuracy (Figure~\ref{fig:performance}(c)) both decline sharply. This highlights that \emph{feature purity}—the extent to which each neuron aligns to a single semantic direction—is a key bottleneck in contrastive pretraining and downstream generalization. %, supporting our theoretical findings in Theorems~\ref{theorem:pre-train} and~\ref{theorem:retrain}.  
We provide extra results in Appendix~\ref{sec:extra-sim}.


%  
\begin{table*}[t]
\scriptsize
\caption{  of CLIP and LaCLIP on Accuracy (\%) and Silhouette Score.}
\label{tab:clip_laclip_short}
\renewcommand{\arraystretch}{1}
\setlength{\tabcolsep}{5pt}
\begin{tabular}{l|cc|cc|cc|cc|cc|cc}
\toprule
 & \multicolumn{2}{c|}{Food-101} & \multicolumn{2}{c|}{CIFAR-10} & \multicolumn{2}{c|}{Caltech-101} & \multicolumn{2}{c|}{CIFAR-100} & \multicolumn{2}{c|}{Pets} & \multicolumn{2}{c}{STL-10} \\
\textbf{Model} & Acc & SS & Acc & SS & Acc & SS & Acc & SS & Acc & SS & Acc & SS \\
\midrule
CC12M CLIP   & 50.8 & 0.034 & 64.9 & 0.113 & 77.4 & 0.225 & 38.5 & 0.005 & 64.1 & 0.069 & 91.0 & 0.195 \\
CC12M LaCLIP & $\mathbf{60.7}$ & $\mathbf{0.038}$ & $\mathbf{75.1}$ & $\mathbf{0.157}$ & $\mathbf{83.3}$ & $\mathbf{0.276}$ & $\mathbf{43.9}$ & $\mathbf{0.029}$ & $\mathbf{72.4}$ & $\mathbf{0.070}$ & $\mathbf{95.1}$ & $\mathbf{0.273}$ \\
\midrule
RedCaps CLIP & 81.5 & 0.125 & 70.4 & 0.100 & 72.8 & 0.210 & 39.9 & $-0.002$ & $\mathbf{82.7}$ & $\mathbf{0.091}$ & $\mathbf{92.8}$ & 0.226 \\
RedCaps LaCLIP & $\mathbf{85.0}$ & $\mathbf{0.175}$ & $\mathbf{74.8}$ & $\mathbf{0.107}$ & $\mathbf{76.4}$ & $\mathbf{0.233}$ & $\mathbf{40.7}$ & $\mathbf{0.011}$ & 78.2 & 0.074 & 91.4 & $\mathbf{0.275}$ \\
\midrule
LAION CLIP  & 85.5 & 0.116 & 93.0 & 0.181 & 91.2 & 0.258 & 71.7 & 0.078 & 90.1 & 0.122 & 97.3 & 0.223 \\
LAION LaCLIP & $\mathbf{86.5}$ & $\mathbf{0.148}$ & $\mathbf{93.5}$ & $\mathbf{0.215}$ & $\mathbf{92.4}$ & $\mathbf{0.306}$ & $\mathbf{73.9}$ & $\mathbf{0.108}$ & $\mathbf{90.9}$ & $\mathbf{0.152}$ & $\mathbf{98.4}$ & $\mathbf{0.260}$ \\
\bottomrule
\end{tabular}
\end{table*}


 
\subsection{Experiments on Practical Data and Models}
\label{sec:blip}
 
\textbf{LaCLIP improves generalization over CLIP via recaption.}
Tables~\ref{tab:clip_laclip_short} compare CLIP~\cite{radford2021learning} and LaCLIP~\cite{fan2023improving}, which share the same architecture and datasets, except LaCLIP replaces part of the original captions with LLM-generated rewrites. 
``CC12M CLIP'' denotes a CLIP model pretrained on raw CC12M~\cite{changpinyo2021conceptual}, while ``CC12M LaCLIP'' uses the same model and data but with LLM-rewritten captions. Other models are obtained similarly using RedCaps~\cite{desai2021redcaps} and LAION~\cite{schuhmann2021laion} datasets.
We evaluate their zero-shot classification accuracy and Silhouette Scores on various downstream datasets.  
LaCLIP generally outperforms CLIP in both metrics, empirically validating that higher-quality captions improve zero-shot generalization. 
Additional ImageNet results are reported in Table~\ref{tab:imagenet_results} of Appendix~\ref{sec:clip-laclip-extra}.


Next, we study the feature purity using a CLIP model pretrained on CC3M~\cite{sharma2018conceptual}. Both the image and text encoders are 12-layer transformers that produce features in \(\mathbb{R}^{768}\), which are subsequently projected into a shared embedding space of \(\mathbb{R}^{512}\) through final linear projection layers, as illustrated in Figure~\ref{fig:clipchart} of Appendix~\ref{sec:clip-laclip-extra}. 
The final linear projection layer has 512 neurons and is functionally aligned with  \(\mathbf{V}\) in our theoretical model. We now present two key findings from this setting:

\textbf{Purified neurons enhance generalization.}  
To investigate the effect of feature purity on generalization, we prune the neurons in the final linear layer in different ways and evaluate the resulting zero-shot classification performance. % perform neuron selection in the final projection layer for zero-shot classification. 
Specifically, we rank the 512 neurons by their average pairwise absolute cosine similarity to all other neurons, from lowest to highest. The absolute cosine similarity of neurons   $v_j$, $v_{j^\prime}$ is computed as 
$|\langle v_j, v_{j^\prime} \rangle|/\|v_j\| \|v_{j^\prime}\|$ for all $j, j' \in \{1,2,\dots,512\}$.  
A lower average indicates higher feature purity (i.e., more orthogonal representations), while a higher value suggests feature mixing. We evaluate three pruning strategies: (1) retaining \textbf{high-purity}  neurons, i.e., with lowest similarity, (2) retaining \textbf{low-purity} neurons, i.e.,  with highest similarity, and (3) retaining a \textbf{random} subset of neurons. The number of retained neurons is varied from 200 to 500. As shown in Figure~\ref{fig:combined_results} (a-c,e-g), downstream performance is the best  when retaining high-purity neurons, followed by random selection, with low-purity neurons performing the worst. These results highlight the critical role of purified features in downstream generalization.

%in the 512-dimensional embedding space based on their minimum pairwise cosine distance---larger distances indicate more orthogonal (i.e., purified) representations. We compare three strategies: \textit{High Purity} (select top-ranked neurons), \textit{Low Purity} (select bottom-ranked), and \textit{Random}. Unselected neurons are pruned to zero before computing image-text similarity. We vary the number of selected neurons from 200 to 500 and evaluate performance on standard datasets. Figure~\ref{fig:combined_results} (Left) shows that high-purity neurons consistently improve accuracy, confirming the importance of purified features for downstream generalization.

\textbf{Data misalignment reduces feature purity.}
To study how image-text misalignment affects feature purity, we %simulate a spurious correlation probability \(C_s\) by 
randomly shuffling texts across image-text pairs in CC3M with probability \(C_m\), as illustrated in Figure~\ref{fig:misalignment_shuffle} of Appendix~\ref{sec:clip-laclip-extra}, thereby introducing a controlled probability of modality misalignment.
We then use the shuffled dataset to fine-tune the last linear projection layer only % the image and text projection layers
of the pretrained CLIP model, freezing other layers.
%After fine-tuning, we analyze the text linear projection layer—functionally aligned with \(\mathbf{V}\) in our theory—by 
We then compute the cosine similarities of all 512 neuron weight vectors \(v_j \in \mathbb{R}^{768}\) of the fine-tuned model. Figure~\ref{fig:combined_results} %(g-h) presents analyses of the text projection layer under varying spurious correlation probabilities \(C_s\). 
(d) reports the average absolute cosine similarity of all neuron pairs, %$|\langle v_j, v_{j^\prime} \rangle|/\|v_j\| \|v_{j^\prime}\|$ for all $j, j' \in \{1,2,\dots,512\}$, 
while (h) presents a histogram of cosine similarity $\langle v_{j}, v_{j’}\rangle / (\|v_j\|\|v_{j'}\|)$. One can see that as \(C_m\) increases, the average absolute cosine similarity increases, and the neurons become less orthogonal to each other and tend to encode mixed representations, resulting in lower feature purity. This coincides with the decreases classification accuracy in downstream tasks, as shown in Table~\ref{tab:laclip_full512_cm} of Appendix~\ref {sec:clip-laclip-extra}. 
 % become less orthogonal and broader histograms centered away from zero—indicating low feature purity and neurons with mixed representation.



% As a baseline, we use the full set of 512 neurons without pruning for downstream zero-shot classification. 
% To further explore, we compare two neuron selection strategies:
% In the "High Purity" setting, we sort neurons by their minimum cosine distance to other neurons in descending order and select the top ones, corresponding to neurons with purified feature representations.
% In the "Low Purity" setting, we sort in ascending order and select neurons with mixed feature representations.
% Unselected neurons are set to zero during inference.
% For each model variant, we vary the number of selected neurons from 200 to 500 and evaluate the downstream zero-shot classification accuracy on CIFAR-10, CIFAR-100, and Caltech-101 datasets.

% The experimental results are presented in Figure.~\ref{fig:neuron_selection}. We observe that selecting high purity neurons consistently improves zero-shot classification accuracy across different datasets compared to the baseline. In contrast, selecting low purity neurons leads to a notable degradation in performance. These findings confirm that purified feature representations are critical for better downstream generalization.


% \vspace{-5pt}
% \begin{figure}[h]
%     \centering
%     \subfigure[CIFAR-100 Accuracy]{
%         \begin{minipage}{0.22\textwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{image/cifar100_accuracy_vs_sam.png}
%         \end{minipage}
%     }
%     ~ 
%     \subfigure[CIFAR-10 Accuracy]{
%         \begin{minipage}{0.22\textwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{image/cifar10_accuracy_vs_sam.png}
%         \end{minipage}
%     }
%     ~
%     \subfigure[Caltech-101 Accuracy]{
%         \begin{minipage}{0.22\textwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{image/caltech101_accuracy_vs_sam.png}
%         \end{minipage}
%     }
%     \\
%     \vspace{3pt}
%     \subfigure[CIFAR-100 SS]{
%         \begin{minipage}{0.22\textwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{image/cifar100_silhouette_vs_sam.png}
%         \end{minipage}
%     }
%     ~
%     \subfigure[CIFAR-10 SS]{
%         \begin{minipage}{0.2\textwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{image/cifar10_silhouette_vs_sam.png}
%         \end{minipage}
%     }
%     ~
%     \subfigure[Caltech-101 SS]{
%         \begin{minipage}{0.22\textwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{image/caltech101_silhouette_vs_sam.png}
%         \end{minipage}
%     }
%     \caption{Comparison of downstream accuracy and SS across different neuron selection strategies.}
%     \label{fig:neuron_selection}
% \end{figure}


% ----------------

% To further understand how data alignment affects the geometry of learned feature spaces, we conduct a controlled study by introducing varying levels of spurious alignment \( C_s \) into the training data. Specifically, we randomly shuffle the captions for a proportion \( C_s \in \{0.0, 0.1, 0.3, 0.5, 0.8\} \) of image-text pairs from the CC3M dataset, while keeping the remaining pairs unchanged. This procedure introduces misaligned supervision at controlled levels.

% We then fine-tune only the image and text projection layers of a pretrained CLIP model on these corrupted datasets, keeping the rest of the model frozen. After fine-tuning, we analyze the neurons of the projection layers by computing the average absolute cosine similarity between neuron weight vectors. 


% The final text projection layer in CLIP consists of 512 neurons and bears functional similarity to $\mathbf{V}$ in our simplified model in (\ref{eqn:g}). Each neuron is associated with a weight vector $v_j \in \mathbb{R}^{768}$. 

% Figure~\ref{fig:cosine_similarity_analysis} presents two complementary analyses: 
% (a) shows the mean absolute value of the normalized inner products, computed as $|\langle v_j, v_{j^\prime} \rangle|/\|v_j\| \|v_{j^\prime}\|$, averaged over all neuron pairs $(j, j^\prime)$; 
% (b) displays the histogram of the normalized inner products for all $j, j^\prime \in \{1,2,\dots,512\}$. 
% Both plots reveal that higher levels of spurious alignment (\(C_s\)) lead to increased feature entanglement and reduced neuron purity.

% \vspace{-5pt}
% \begin{figure}[h]
%     \centering
%     \subfigure{
%         \begin{minipage}{0.22\textwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{image/avg_abs_cos_sim_vs_cs.png}
%         \end{minipage}
%     }
%     ~ 
%     \subfigure{
%         \begin{minipage}{0.22\textwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{image/vvt_distribution_cs.png}
%         \end{minipage}
%     }
%     \caption{Comparison of neuron feature orthogonality across different spurious alignment probabilities \(C_s\).}
%     \label{fig:cosine_similarity_analysis}
% \end{figure}



 
\begin{figure}[h]
    \centering
    % \setlength{\abovecaptionskip}{1pt}
    % \setlength{\belowcaptionskip}{-5pt}

    % First row: a c e g
    \subfigure[Food-101 Acc.]{
        \includegraphics[width=0.22\textwidth]{image/food101_accuracy_vs_sam.png}
    }
    \subfigure[CIFAR-10 Acc.]{
        \includegraphics[width=0.22\textwidth]{image/cifar10_accuracy_vs_sam.png}
    }
    \subfigure[Caltech-101 Acc.]{
        \includegraphics[width=0.22\textwidth]{image/caltech101_accuracy_vs_sam.png}
    }
    \subfigure[Avg. Cos. Sim.]{
        \includegraphics[width=0.22\textwidth]{image/avg_abs_cos_sim_vs_cs.png}
    }

    % Second row: b d f h
    \subfigure[Food-101 SS]{
        \includegraphics[width=0.22\textwidth]{image/food101_silhouette_vs_sam.png}
    }
    \subfigure[CIFAR-10 SS]{
        \includegraphics[width=0.22\textwidth]{image/cifar10_silhouette_vs_sam.png}
    }
    \subfigure[Caltech-101 SS]{
        \includegraphics[width=0.22\textwidth]{image/caltech101_silhouette_vs_sam.png}
    }
    \subfigure[Hist of Cos. Sim.]{
        \includegraphics[width=0.22\textwidth]{image/vvt_distribution_cs.png}
    }

    \caption{Left
    (a–c,e-g): Retaining high-purity neurons outperform random and low-purity neurons in downstream tasks.
    More datasets shown in Figure~\ref{fig:combined_results2}.
    Right(d,h): When $C_m$ increases, the neurons have higher cosine similarity and reduced feature purity. %analysis under different spurious correlation probabilities \(C_s\), showing degradation in feature purity. 
    }
    \label{fig:combined_results}
\end{figure}




 
\section{Conclusion}
 
This paper provides a theoretical analysis of contrastive learning with nonlinear networks, linking training dynamics to generalization. We identify feature purity as central to generalization and show that text recaptioning enhances purity and zero-shot performance. The theory is empirically validated on benchmarks. Future work includes extending to Transformer models and tasks like retrieval and visual question answering.


\subsection*{Acknowledgments}
This work was supported by National Science Foundation (NSF) \#2430223, Army Research Office (ARO) W911NF-25-1-0020, and the Rensselaer-IBM Future of Computing Research Collaboration (http://airc.rpi.edu). The work of Shuai Zhang was supported by NSF \#2349879. We also thank all anonymous reviewers for their constructive comments.



\bibliography{example_paper}
\bibliographystyle{plain}












\newpage
\appendix
\onecolumn

The overall structure of the appendix is as follows. Each appendix provides supplementary information that supports the main content of this document but is not included in the main body to maintain clarity and flow.


\begin{itemize}
    \item \textbf{Appendix A: Extra Experiments} \\
    Additional experiments including both synthetic simulations and CLIP/LaCLIP evaluations on omitted datasets.
    \begin{itemize}
        \item \textbf{A.1 Extra Simulated Experiment} \\
        Complements Section~\ref{sec:synthetic} with further analysis of neuron behavior trained on simulated data.
        \item \textbf{A.2 Extra CLIP/LaCLIP Experiment} \\
        Complements Section~\ref{sec:blip} by evaluating on datasets omitted due to space.
    \end{itemize}

    \item \textbf{Appendix B: Preliminaries} \\
    Mathematical preliminaries and notation used throughout the paper. A \textit{proof sketch} is also provided to outline the key ideas behind the main results.

    \item \textbf{Appendix C: Technical Lemmas} \\
    Full statements and proofs of supporting lemmas used in the theoretical analysis.

\item \textbf{Appendix D–J: Proofs and Theoretical Analysis} 
\begin{itemize}
    \item \textbf{Appendix D–F: ITCP on Raw Data (Phase I–III)} \\
    Theoretical proof of ITCP across three training phases on raw data.

    \item \textbf{Appendix G: Captioning} \\
    Theoretical proof of reception using high quality data.

    \item \textbf{Appendix H: Filtering} \\
    Theoretical proof of filtering noisy caption-text pairs.

    \item \textbf{Appendix I: ITCP on Synthetic (Recaptioned) Data} \\
    Theoretical proof of training dynamics when using synthetic recaptions.

    \item \textbf{Appendix J: Downstream Task Evaluation} \\
    Theoretical implications for performance on downstream tasks.   
\end{itemize}
\item \textbf{Checklist}
\end{itemize}


\newpage
 
\section{Extra Experiment}
 
All experiments were conducted on an internal compute cluster using 8 NVIDIA A5000 GPUs with 24 GB memory each, and each run completed within 50 GPU-hours. No large-scale pretraining or resource-intensive tuning was performed beyond the reported experiments.
 
\subsection{Extra Simulated  Experiment}
 
\label{sec:extra-sim}
This section extends the analysis in Section~\ref{sec:synthetic} by providing additional simulated experiments on neuron behavior under synthetic data training.

\textbf{Neurons trained on filtered data exhibit a more concentrated distribution.}  %We normalize the weights of $\overline{\mathbf{V}}$ and $\widetilde{\mathbf{V}}$ such that $\|\bar{v}_i\|_2 = 1$ and $\|\tilde{v}_i\|_2 = 1$ for all $i \in [m]$.
Figure~\ref{fig:vh_noise} visualizes the histograms of $|\langle \bar{v}_i, \mathbf{H}_j \rangle|/\|\bar{v}_i\|$ and $|\langle \tilde{v}_i, \mathbf{H}_j \rangle|/\|\tilde{v}_i\|$ for all $i \in [m]$ and $j \in [d]$. %Figure~\ref{fig:vh_noise} visualizes the histograms of these values.  We observe that in the model trained with ITCP on synthetic data,
The values of $|\langle \tilde{v}_i, \mathbf{H}_j \rangle|/\|\tilde{v}_i\|$ are more concentrated, typically around $0.05$ and $0.7$. In contrast, the values for $|\langle \bar{v}_i, \mathbf{H}_j \rangle|/\|\bar{v}_i\|$ are less concentrated. This phenomenon is consistent with  Theorem \ref{theorem:retrain}, which indicates that for every $\mathbf{H}_j$,  certain neurons $\tilde{v}_i$ in $\widetilde{\mathbf{V}}$ %, $\tilde{N}_i=\{j\}$, meaning that $\tilde{v}_i$ 
predominately learns $\mathbf{H}_j$. In such cases, $|\langle \tilde{v}_i, \mathbf{H}_j \rangle|$ approaches $1$, while $|\langle \tilde{v}_i, \mathbf{H}_{j'} \rangle|/\|\tilde{v}_i\|$ approaches $0$  for $j' \neq j$. The concentrated values of $0.05$ and $0.7$ observed in Figure~\ref{fig:vh_noise} are due to noise in the data.
In contrast, feature alignment is less significant for $\overline{\mathbf{V}}$, leading to less concentration of the corresponding values.  Similar results are obtained for image encoder %$\overline{\mathbf{W}},\widetilde{\mathbf{W}}$， %
\(|\langle w_i, \mathbf{M}_j \rangle|\),   
deferred to Figure~\ref{fig:wm_noise}.


\vspace{-2pt}
\begin{figure}[h]
    \centering
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{image/vh_histogram_small_noise_0.3.png}
    \end{minipage}
    ~
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{image/vh_histogram_large_noise_0.3.png}
    \end{minipage}
    \caption{Histogram of $|\langle \bar{v}_i, \mathbf{H}_j \rangle|/\|\bar{v}_i\|$ for ITCP on raw data and $|\langle \tilde{v}_i, \mathbf{H}_j \rangle|/\|\tilde{v}_i\|$ for ITCP on filtered data (split into two figures to highlight the significant differences in the value distributions).}
    \label{fig:vh_noise}
\end{figure}
 \vspace{-2pt}


\begin{figure}[h]
    \centering
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{image/wm_histogram_small_noise_0.3.png}
    \end{minipage}
    ~
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{image/wm_histogram_large_noise_0.3.png}
    \end{minipage}
    \caption{Histogram of $|\langle \bar{w}_i, \mathbf{M}_j \rangle|/|\bar{w}_i|$ for ITCP on raw data and $|\langle \tilde{w}_i, \mathbf{M}_j \rangle|/\tilde{w}_i$ for ITCP on filtered data (split into two figures to highlight the significant differences in the value distributions).}
    \label{fig:wm_noise}
\end{figure}



\textbf{Enhanced class separation of downstream tasks by ITCP with recaptioned data}. 
Figure~\ref{fig:trained_tsne} visualizes the t-distributed stochastic neighbor embedding (t-SNE) of the  feature embeddings generated by the two models, computed as \(f_{\overline{\mathbf{W}}}(x_p)\) and \(f_{\widetilde{\mathbf{W}}}(x_p)\) for each   \(x_p\), respectively. The t-SNE method projects the high-dimensional embeddings onto a two-dimensional map. One can see that the embeddings from different groups are more distinctly separated in the model trained using ITCP on recaptioned data, indicating that this approach achieves better feature alignment.

\vspace{-5pt}
\begin{figure}[h]
    \centering
    \setlength{\abovecaptionskip}{1pt}
    \setlength{\belowcaptionskip}{-5pt}
    \subfigure[Raw ($C_s=0.3$)]{
        \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{image/pretrained_tsne.png}
        \end{minipage}
    }
    \subfigure[Recaption ($C_s=0.3$)]{
        \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{image/retrained_tsne.png}
        \end{minipage}
    }
    \subfigure[Raw ($C_s=0.5$)]{
        \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{image/pretrained_tsne5.png}
        \end{minipage}
    }
    \subfigure[Recaption ($C_s=0.5$)]{
        \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{image/retrained_tsne5.png}
        \end{minipage}
    }
    \caption{t-SNE visualization of text embedding with spurious correlation probability \(C_s\).}
    \label{fig:trained_tsne}
\end{figure}



\subsection{Extra Experiment on CLIP and LaCLIP}
\label{sec:clip-laclip-extra}
To complement the results in Section~\ref{sec:blip}, we report additional experiments on CLIP and LaCLIP using datasets omitted from the main text due to space constraints. 


\textbf{ImageNet Results.}
The LaCLIP variants consistently surpass their CLIP counterparts on both Top-1 and Top-5 accuracy. Higher silhouette scores further indicate cleaner feature separation after recaptioning, in line with our theoretical predictions.
\begin{table*}[t]
\centering
\caption{Comparison of CLIP and LaCLIP on ImageNet: Top-1 (\%), Top-5 (\%), and Silhouette Score.}
\label{tab:imagenet_results}
\renewcommand{\arraystretch}{1}
\setlength{\tabcolsep}{6pt}
\begin{tabular}{l|ccc}
\toprule
\textbf{Model} & \textbf{Top-1 (\%)} & \textbf{Top-5 (\%)} & \textbf{Silhouette} \\
\midrule
CC12M CLIP        & 35.04 & 62.10 & -0.014639 \\
CC12M LaCLIP      & $\mathbf{42.62}$ & $\mathbf{70.17}$ & $\mathbf{-0.008141}$ \\
\midrule
LAION-400M CLIP   & 58.34 & 84.73 & -0.029893 \\
LAION-400M LaCLIP & $\mathbf{62.27}$ & $\mathbf{86.34}$ & $\mathbf{-0.056593}$ \\
\midrule
RedCaps CLIP      & 37.66 & 63.31 & -0.022045 \\
RedCaps LaCLIP    & $\mathbf{39.66}$ & $\mathbf{66.06}$ & $\mathbf{-0.012269}$ \\
\bottomrule
\end{tabular}
\vspace{-5mm}
\end{table*}




\textbf{CLIP architecture.}
Figure~\ref{fig:clipchart} illustrates the CLIP architecture used in our experiments. Both image and text inputs are independently encoded by 12-layer transformer backbones, each producing a 768-dimensional feature vector. These features are then projected into a shared 512-dimensional embedding space through learned linear projection matrices \( \mathbf{W} \in \mathbb{R}^{768 \times 512} \) and \( \mathbf{V} \in \mathbb{R}^{768 \times 512} \), corresponding to the image and text encoders in our theorem, defined in Eq.~(\ref{eqn:f}). The resulting embeddings are aligned via a contrastive loss that maximizes similarity for matched image-text pairs while minimizing similarity for unmatched pairs. This architecture forms the foundation for our analyses on neuron selection and feature purity in the shared embedding space.
\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{image/CLIPchart.png}
\caption{Architecture of CLIP used in our experiments. Both image and text encoders are 12-layer transformers that output features in \(\mathbb{R}^{768}\), which are then projected into a shared \(\mathbb{R}^{512}\) embedding space via final linear projection layers \( \mathbf{W} \) and \( \mathbf{V} \), corresponding to Eq.~(\ref{eqn:f}) and Eq.~(\ref{eqn:g}) in our theoretical analysis. Contrastive loss is computed between the resulting image and text embeddings.}
\label{fig:clipchart}
\end{figure}


\textbf{Simulating Modality Misalignment via Caption Shuffling.}  
Figure~\ref{fig:misalignment_shuffle} illustrates how modality misalignment is introduced by randomly shuffling text captions across image-text pairs with probability \(C_m\), resulting in noisy supervision for contrastive learning.
\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{image/misalignment_shuffle_diagram.png}
\caption{Simulating Modality Misalignment via Caption Shuffling. Starting from original aligned image-text pairs, a controlled probability \(C_m\) of misalignment is introduced by randomly shuffling the text captions. This results in noisy pairs that reflect varying levels of spurious correlations.}
\label{fig:misalignment_shuffle}
\end{figure}



\begin{figure}[h]
    \centering
    % ---- First row: Accuracy ----
    \subfigure[CIFAR-100 Acc.]{
        \includegraphics[width=0.26\textwidth]{image/cifar100_accuracy_vs_sam.png}
    }
    \subfigure[Pets Acc.]{
        \includegraphics[width=0.26\textwidth]{image/pets_accuracy_vs_sam.png}
    }
    \subfigure[STL-10 Acc.]{
        \includegraphics[width=0.26\textwidth]{image/stl10_accuracy_vs_sam.png}
    }

    \vspace{2pt}

    % ---- Second row: Silhouette Score ----
    \subfigure[CIFAR-100 SS]{
        \includegraphics[width=0.26\textwidth]{image/cifar100_silhouette_vs_sam.png}
    }
    \subfigure[Pets SS]{
        \includegraphics[width=0.26\textwidth]{image/pets_silhouette_vs_sam.png}
    }
    \subfigure[STL-10 SS]{
        \includegraphics[width=0.26\textwidth]{image/stl10_silhouette_vs_sam.png}
    }

    \caption{
    Zero-shot classification accuracy (top) and Silhouette Score (bottom) under different neuron selection strategies for CIFAR-100, Pets, and STL-10 datasets.
    }
    \label{fig:combined_results2}
\end{figure}
\textbf{Purified neuron selection enhances generalization.} Figure~\ref{fig:combined_results2} presents additional experimental results on CIFAR-100, Pets, and STL-10, complementing the main results reported in Figure~\ref{fig:combined_results}. Due to space constraints, we include only Food-101, CIFAR-10, and Caltech-101 in the main text. All experiments follow the same protocol, evaluating zero-shot classification accuracy and Silhouette Score under different neuron selection strategies. These results consistently support our core finding: selecting high-purity neurons leads to improved downstream performance across diverse datasets.




\textbf{Higher shuffling probability leads to reduced generalization and feature purity.} Table~\ref{tab:laclip_full512_cm} presents additional experimental results on CLIP models finetuned with different levels of randomly shuffling probability \(C_m\) to simulate spurious correlation, showing that both accuracy and Silhouette Score consistently decrease as \(C_m\) increases.

\begin{table*}[t]
\centering
\small
\caption{Accuracy (\%) and Silhouette Score of CLIP models finetuned with varying $C_m$ on six datasets.}
\label{tab:laclip_full512_cm}
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l
|cc|cc|cc|cc|cc}
\toprule
\textbf{Dataset} 
& \multicolumn{2}{c|}{$C_m=0$}
& \multicolumn{2}{c|}{$C_m=0.1$} 
& \multicolumn{2}{c|}{$C_m=0.3$} 
& \multicolumn{2}{c|}{$C_m=0.5$} 
& \multicolumn{2}{c}{$C_m=0.8$} \\
& Acc & SS & Acc & SS & Acc & SS & Acc & SS & Acc & SS \\
\midrule
Caltech101 & 59.7 & 0.160 & 48.2 & 0.124 & 47.9 & 0.121 & 43.6 & 0.117 & 44.5 & 0.115 \\
CIFAR-10   & 57.9 & 0.030 & 50.7 & 0.012 & 49.5 & 0.013 & 46.5 & 0.013 & 44.1 & 0.011 \\
CIFAR-100  & 26.4 & $-0.038$ & 19.5 & $-0.042$ & 17.8 & $-0.043$ & 17.4 & $-0.044$ & 16.2 & $-0.048$ \\
Food-101   & 12.9 & $-0.073$ & 10.9 & $-0.052$ & 10.9 & $-0.056$ & 11.1 & $-0.057$ & 11.1 & $-0.059$ \\
Pets       & 13.9 & $-0.005$ & 13.3 & $-0.006$ & 13.2 & $-0.009$ & 13.4 & $-0.011$ & 12.6 & $-0.012$ \\
STL-10     & 86.3 & 0.164 & 79.8 & 0.103 & 79.2 & 0.102 & 78.8 & 0.100 & 78.3 & 0.097 \\
\bottomrule
\end{tabular}
\end{table*}








\section{Preliminaries}
We first restate some important notations used in the Appendix, which are summarized in Table~\ref{tbl:notations}.

\renewcommand{\arraystretch}{1.5}
\begin{table}[h!]
  \begin{center}
        \caption{Summary of Notations}
        \label{tbl:notations}
\begin{tabularx}{\textwidth}{lX} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between

\toprule
Notations & Annotation \\
\hline
\small $\mathbf{M} \in \mathbb{R}^{d_{1}\times d}$, $\mathbf{H}\in \mathbb{R}^{d_{1}\times d}$  & $\mathbf{M}$ is the image dictionary matrix, $\mathbf{H}$ is the text dictionary matrix.  \\
\hline
\small $\mathbf{W}\in \mathbb{R}^{m\times d_1}$, $\mathbf{V}\in \mathbb{R}^{m\times d_1}$ & $\mathbf{W}$ is the weight of image encoder, $\mathbf{V}$ is the weight of text encoder.\\
\hline
\small $x_p \in \mathbb{R}^{d_1}, y_p \in \mathbb{R}^{d_1}$ & $x_p$ and $y_p$ represent an image and a text data, respectively.\\
\hline
\small $z_{x_p}, z_{y_p} \in \mathbb{R}^d$ & $z_{x_p}$ and $z_{y_p}$ are the sparse signals of image and text, respectively. $z_{y_k}$ is the sparse signal for the text prompt $y_k$.\\
\hline
\small $z_{x_p}^j, z_{y_p}^j$ & $z_{x_p}^j$ is the $j$-th coordinate of $z_{x_p}$; $z_{y_p}^j$ is the $j$-th coordinate of $z_{y_p}$.\\
\hline
\small $L$, $L_C$ & $L$ is the loss for ITCP; $L_C$ is the loss for Image-grounded Text Decoding.\\
\hline
\small $S = S_h \cup S_w$ & $S_w$ is the noisy web low-quality dataset; $S_h$ is the human-annotated high-quality dataset.\\
\hline
\small $\tilde{S} = S_h \cup \tilde{S}_w$ & $\tilde{S}_w$ replaces noisy captions in $S_w$ with synthetic captions.\\
\hline
\small \(T_1\) & Phase I of ITCP with \(b_i^{(t)}=0\). \\
\hline
\small \(T_2\) & Phase II of ITCP with \(b_i^{(t+1)}=(1+\frac{\eta}{d})b_i^{(t)}\). \\
\hline
\small \(T_3\) & Phase III of ITCP with \(b_i^{(t+1)}=b_i^{(T_2)}\). \\
\hline
\small $T_C$ & Stage of training caption generators.\\
\hline
\small $\mathcal{S}_{j, \text{sure}}$ & The set of well-initialized neurons $(w_i, v_i)$ on features $(\mathbf{M}_j, \mathbf{H}_j)$.\\
\bottomrule
\end{tabularx}
\end{center}
\end{table}





 
\subsection{Proof Scratch}
\label{sec:ProofScratch}
 

Theorem~\ref{theorem:convergence} is proven by integrating the convergence analyses in Appendix~\ref{appendix:itcp_raw_stage3} and Appendix~\ref{appendix:itcp_synthetic}. Appendix~\ref{appendix:itcp_raw_stage3} establishes convergence for ITCP on raw data, while Appendix~\ref{appendix:itcp_synthetic} extends the convergence result to ITCP on synthetic data. Together, they verify that SGD with ReLU networks achieves near-optimal contrastive loss on both datasets.

Theorem~\ref{theorem:pre-train} is proven across Appendix~\ref{appendix:itcp_raw_stage1}, Appendix~\ref{appendix:itcp_raw_stage2}, and Appendix~\ref{appendix:itcp_raw_stage3}. Specifically, Appendix~\ref{appendix:itcp_raw_stage1} models Phase I training ($t \leq T_1$) and proves that neurons simultaneously align with true features and spuriously correlated features due to comparable gradient contributions, preventing pure feature separation. Appendix~\ref{appendix:itcp_raw_stage2} analyzes Phase II training ($T_1 < t \leq T_2$) and shows that this spurious alignment continues to strengthen, as neurons with initial mixed alignment further amplify their entanglement during continued SGD updates. Appendix~\ref{appendix:itcp_raw_stage3} establishes the convergence behavior during Phase III ($T_2 < t \leq T_3$), showing that the network stabilizes into mixed solutions where each neuron represents a combination of multiple features. These detailed stages collectively prove the failure of purified feature alignment as formalized in Theorem~\ref{theorem:pre-train}.


Theorem~\ref{theorem:caption_filtering} is proven across Appendix~\ref{appendix:captioning} and Appendix~\ref{appendix:filtering}. Specifically, Appendix~\ref{appendix:captioning} analyzes the captioning stage, where the decoder is fine-tuned on clean data to generate synthetic captions. It proves that for neurons aligned with true features, the alignment towards the true features grows exponentially while the alignment towards spurious features remains negligible. This ensures that the synthetic captions preserve relevant features and suppress spurious ones. Appendix~\ref{appendix:filtering} then formalizes the filtering process, demonstrating that after replacing noisy captions with synthetic ones, the resulting dataset satisfies much stronger feature purity conditions, with spurious correlations suppressed to \( \Theta(1/d) \) and true features preserved with probability \( 1 - \Theta(1/d) \). These results directly support the purified feature learning described in Theorem~\ref{theorem:caption_filtering}.


Theorem~\ref{theorem:retrain} is proven in Appendix~\ref{appendix:itcp_synthetic}, which integrates the proofs of Phase I, Phase II, and Phase III for ITCP on synthetic data. Specifically, Appendix~\ref{appendix:itcp_synthetic} first establishes in Phase I that purified training pairs allow neurons aligned with true features to grow exponentially without spurious interference. It then shows in Phase II that these alignments continue to strengthen while suppressing non-informative neurons, leading to clear feature separation. Finally, it proves in Phase III that the model converges, achieving a bounded final loss and dominant true feature alignment. Since the overall proof structure closely mirrors that of Theorem~\ref{theorem:pre-train} (which was proven separately across Appendix~\ref{appendix:itcp_raw_stage1}, Appendix~\ref{appendix:itcp_raw_stage2}, and Appendix~\ref{appendix:itcp_raw_stage3}), we consolidate all stages into a single appendix for brevity and clarity.


Theorem~\ref{theorem:downstream} is proven in Appendix~\ref{appendix:downstream}, which analyzes the downstream zero-shot classification. Appendix~\ref{appendix:downstream} shows that for ITCP on raw data, spurious features cause a constant classification error, while for ITCP on synthetic data, true and spurious features become separable with high probability, leading to an $o(1)$ error rate. This directly supports the main text conclusion on downstream generalization.

















\subsection{Feature Coupling and Expected Values in \(S_w\)}

The following Assumption~\ref{assumption:data} corresponds to the more specific forms of Assumptions~\ref{assumption:high} and~\ref{assumption:low} discussed earlier.
\begin{assumption}[High and low quality pairs]
\label{assumption:data}
The high-quality image-text pairs  in $S_h$ have size \( \left | S_h \right | =\Theta (d^2)\). The low-quality image-text pairs   in $S_w$ have size \( \left | S_w \right | = \text{poly}(d)|\gg\omega(d^2) \). 

%Due to the annotation cost, the entire dataset \( S= S_h \cup S_w \) includes a limited number of high-quality image-text pairs \((x_p, y_p)\) where \( p \in S_h \) and \( \left | S_h \right | =\Theta (d^2)\), and a large number of low-quality image-text pairs \((x_p, y_p)\) where \(p \in S_w\) and \( \left | S_w \right | = \text{poly}(d)>\omega(d^2) \) that are automatically collected from the web, and exist misaligned pairs.
In \( S_h \), for a positive pair \((x_p, y_p)\), we assume perfect alignment, meaning \( z_{x_p} = z_{y_p} \). % This is characterized by \( \Pr(|z_{y_p}^j| = 1 \mid |z_{x_p}^j| = 1) = 1, \forall p \in S_h \). 
Consequently, % for \( j' \neq j \),  
the following holds:
\begin{equation}
\mathbb{E}\left[ z_{x_p}^j z_{y_p}^j \right] = \frac{C_z}{d}, \quad 
\mathbb{E}\left[ z_{x_p}^j z_{y_p}^{j'} \right] = \Theta\left(\frac{1}{d^2}\right), \quad j' \neq j 
\end{equation}


To model the misaligned features in low-quality pairs in \( S_w \), where spurious misalignment occurs at a non-negligible level, we assume \([d]\) can be divided into \(d/2\) disjoint sets, each containing exactly two entries. Let \((j, j') \subset [d]\) denote one such set, referred to as a ``spuriously correlated set.''
The following assumptions capture the nature of spurious and true alignments:
\begin{equation}
\begin{aligned}
&\Pr(|z_{y_p}^{j'}| = 1 \mid |z_{x_p}^j| = 1) = \Theta(1) < \frac{1}{2}, \\
&\Pr(|z_{y_p}^{j'}| = 1 \mid |z_{x_p}^j| = 1) + \Pr(|z_{y_p}^j| = 1 \mid |z_{x_p}^j| = 1) = 1.
\end{aligned}
\end{equation}
\end{assumption}


These assumptions imply that true alignment dominates, with \( \Pr(|z_{y_p}^j| = 1 \mid |z_{x_p}^j| = 1) > \frac{1}{2} \), while spurious alignment exists at a constant percentage level, making it non-negligible. 
The intuition behind this assumption is that each feature \( j \) is paired with exactly one spuriously correlated feature \( j' \), ensuring that \( j \) is not associated with any other feature \( j'' \neq j' \). This design simplifies the analysis while effectively capturing the key challenges posed by low-quality data.


Then, for a positive pair \((x_p, y_p)\) with $p$ in $S_w$, we have:
\begin{equation}
\begin{aligned}
&\mathbb{E}\left[ z_{x_p}^j z_{y_p}^{j} \right] + \mathbb{E}\left[ z_{x_p}^j z_{y_p}^{j'} \right] = \frac{C_z}{d}, \\
&\mathbb{E}\left[ z_{x_p}^j z_{y_p}^{j'} \right] = \Theta\left(\frac{1}{d}\right) < \frac{C_z}{2d}.
\end{aligned}
\end{equation}
where $(j,j')$ is a spuriously correlated set.  % \( j' \) is the spurious feature of \( j \).

%\textcolor{red}{In this paper, we only consider negative pairs \((x_p, y_q)\) that come from the same set with either $p,q \in S_h$ or $p,q \in S_W$. Is this correct? Can they from different sets? }
%They can from different sets and \(\mathbb{E}\left[ z_{x_p}^j z_{y_q}^{j'} \right] = \Theta\left(\frac{1}{d^2}\right)\) still holds.
%In both \( S_w \) and \( S_h \), 
For negative pairs \((x_p, y_q)\), where \(p \neq q\), and $p,q \in S$, we have:
\begin{equation}
\mathbb{E}\left[ z_{x_p}^j z_{y_q}^{j'} \right] = \Theta\left(\frac{1}{d^2}\right), \quad \forall j, j' \in [d].
\end{equation}
%\textcolor{red}{We do not consider negative pairs that come from different sets }



\label{subsec:feature_coupling_expected_values}
In \(S_w\), mismatched text and image pairs are prevalent compared to \(S_h\). For a postive pair \( (x_p, y_p) \), we assume \( \frac{\log (1/c_0)}{2\log d} < \Pr(|z_{y_p}^{j'}| = 1 \mid |z_{x_p}^{j}| = 1) < \frac{1}{2} \). To model this, we assume that for each primary feature \(j \in [d]\), there exists exactly one spurious feature \(j'\) such that \(j\) and \(j'\) are uniquely coupled. This implies that \(j\) cannot be associated with any other feature \(j'' \neq j'\). Mathematically, the coupling is defined as:
\begin{equation}
\Pr(|z_{y_p}^{j'}| = 1 \mid| z_{x_p}^j| = 1) + \Pr(|z_{y_p}^j| = 1 \mid |z_{x_p}^j| = 1) = 1.
\end{equation}

For a positive pair \((x_p, y_p)\) in \(S_w\), the probabilities of spurious and aligned features are further constrained:
\begin{equation}
\frac{\log (1/c_0)}{2\log d} < \Pr(|z_{y_p}^{j'}| = 1 \mid |z_{x_p}^j| = 1) < \frac{1}{2},
\end{equation}
The lower bound is established in Lemma~\ref{lemma:MisalignmentProbabilityBound}.

and:
\begin{equation}
\Pr(|z_{y_p}^j| = 1 \mid |z_{x_p}^j| = 1) = 1 - \Pr(|z_{y_p}^{j'}| = 1 \mid |z_{x_p}^j| = 1).
\end{equation}

Under these assumptions, the expected values for the aligned and spurious features are calculated as follows:

For the aligned feature \(j\), we have:
\begin{equation}
\begin{aligned}
\mathbb{E}\left[ z_{x_p}^j z_{y_p}^j \right] 
&= \Pr(|z_{y_p}^j| = 1, |z_{x_p}^j| = 1) \\
&= \Pr(|z_{y_p}^j| = 1 \mid |z_{x_p}^j| = 1) \cdot \Pr(|z_{x_p}^j| = 1) \\
&= \Pr(|z_{y_p}^j| = 1 \mid |z_{x_p}^j| = 1) \cdot \frac{C_z}{d}.
\end{aligned}
\end{equation}

For the spurious feature \(j'\), we have:
\begin{equation}
\begin{aligned}
\mathbb{E}\left[ z_{x_p}^j z_{y_p}^{j'} \right] 
&= \Pr(|z_{y_p}^{j'}| = 1, |z_{x_p}^j| = 1) \\
&= \Pr(|z_{y_p}^{j'}| = 1 \mid |z_{x_p}^j| = 1) \cdot \Pr(|z_{x_p}^j| = 1) \\
&= \Pr(|z_{y_p}^{j'}| = 1 \mid |z_{x_p}^j| = 1) \cdot \frac{C_z}{d}
\end{aligned}
\end{equation}

The total expected value across both aligned and spurious features satisfies:
\begin{equation}
\mathbb{E}\left[ z_{x_p}^j z_{y_p}^j \right] + \mathbb{E}\left[ z_{x_p}^j z_{y_p}^{j'} \right] = \frac{C_z}{d}
\end{equation}

Here, \(j'\) denotes the spurious feature associated with \(j\).


\subsection{Gradient}
The contrastive loss in vision-language models (VLM) is defined as follows:
\begin{equation}
\small
\label{eq:loss_L}
\begin{aligned}
L(f^{(t)}, h^{(t)}) = \sum_{p \in S} \Big[ 
& -\langle f^{(t)}(x_p), h^{(t)}(y_p) \rangle  + \sum_{x_n \in \mathfrak{N}'} \frac{\left( \langle f^{(t)}(x_n), h^{(t)}(y_p) \rangle \right)^2}{2\tau} \\
& + \sum_{y_n \in \mathfrak{N}'} \frac{\left( \langle f^{(t)}(x_p), h^{(t)}(y_n) \rangle \right)^2}{2\tau} 
\Big],
\end{aligned}
\end{equation}
where \(\tau > 0\) is a temperature parameter.

We perform stochastic gradient descent (SGD) on this contrastive loss. Let \(f^{(t)}\) and \(h^{(t)}\) be the image encoder and text encoder networks at iteration \(t\), respectively. The network parameters are updated as follows:
\begin{equation}
\label{eq:image_encoder_update}
w_i^{(t+1)} \leftarrow w_i^{(t)} - \eta \nabla_{w_i} L(f^{(t)}, h^{(t)}),
\end{equation}
\begin{equation}
\label{eq:text_encoder_update}
v_i^{(t+1)} \leftarrow v_i^{(t)} - \eta \nabla_{v_i} L(f^{(t)}, h^{(t)}),
\end{equation}
where \(b_i^{(t)}\), the bias term, is manually tuned during training and thus excluded from gradient updates.

The gradient of \(L(f^{(t)}, h^{(t)})\) with respect to \(w_i^{(t)}\) at iteration \(t\) is given by:
\begin{equation}
\begin{aligned}
\label{eq:gradient_wi}
\nabla_{w_{i}} L(f^{(t)}, h^{(t)})  
= & -\langle v_i^{(t)}, y_p \rangle x_p 
\cdot \mathbf{1}_{\left|\langle w_{i}^{(t)}, x_{p}\rangle\right| \geq b_{i}^{(t)}}  
\cdot \mathbf{1}_{\left|\langle v_{i}^{(t)}, y_{p}\rangle\right| \geq b_{i}^{(t)}} \\
& + \sum_{x_n \in \mathfrak{N}} \frac{\langle f^{(t)}(x_n), h^{(t)}(y_p) \rangle 
\langle v_i^{(t)}, y_p \rangle x_n}{\tau} 
\cdot \mathbf{1}_{\left|\langle w_{i}^{(t)}, x_{n}\rangle\right| \geq b_{i}^{(t)}}  
\cdot \mathbf{1}_{\left|\langle v_{i}^{(t)}, y_{p}\rangle\right| \geq b_{i}^{(t)}} \\
& + \sum_{y_n \in \mathfrak{N}} \frac{\langle f^{(t)}(x_p), h^{(t)}(y_n) \rangle 
\langle v_i^{(t)}, y_n \rangle x_p}{\tau} 
\cdot \mathbf{1}_{\left|\langle w_{i}^{(t)}, x_{p}\rangle\right| \geq b_{i}^{(t)}}  
\cdot \mathbf{1}_{\left|\langle v_{i}^{(t)}, y_{n}\rangle\right| \geq b_{i}^{(t)}}.
\end{aligned}
\end{equation}

Similarly, the empirical gradient of \(L(f^{(t)}, h^{(t)})\) with respect to \(v_i^{(t)}\) is:
\begin{equation}
\label{eq:gradient_vi}
\begin{aligned}
\nabla_{v_{i}} L(f^{(t)}, h^{(t)})  
= & -\langle w_i^{(t)}, x_p \rangle y_p 
\cdot \mathbf{1}_{\left|\langle w_{i}^{(t)}, x_{p}\rangle\right| \geq b_{i}^{(t)}}  
\cdot \mathbf{1}_{\left|\langle v_{i}^{(t)}, y_{p}\rangle\right| \geq b_{i}^{(t)}} \\
& + \sum_{x_n \in \mathfrak{N}} \frac{\langle f^{(t)}(x_n), h^{(t)}(y_p) \rangle 
\langle w_i^{(t)}, x_n \rangle y_p}{\tau} 
\cdot \mathbf{1}_{\left|\langle w_{i}^{(t)}, x_{n}\rangle\right| \geq b_{i}^{(t)}}  
\cdot \mathbf{1}_{\left|\langle v_{i}^{(t)}, y_{p}\rangle\right| \geq b_{i}^{(t)}} \\
& + \sum_{y_n \in \mathfrak{N}} \frac{\langle f^{(t)}(x_p), h^{(t)}(y_n) \rangle 
\langle w_i^{(t)}, x_p \rangle y_n}{\tau} 
\cdot \mathbf{1}_{\left|\langle w_{i}^{(t)}, x_{p}\rangle\right| \geq b_{i}^{(t)}}  
\cdot \mathbf{1}_{\left|\langle v_{i}^{(t)}, y_{n}\rangle\right| \geq b_{i}^{(t)}}.
\end{aligned}
\end{equation}


\subsection{Alignment Updates}

We analyze how each neuron \(i \in [m]\) aligns with the feature \(\mathbf{M}_j\) during each iteration of SGD. The alignment can be described by the following update rule:
\begin{equation}
\begin{aligned}
\langle w_i^{(t+1)}, \mathbf{M}_j \rangle &= \langle w_i^{(t)}, \mathbf{M}_j \rangle - \langle \nabla_{w_i} L(f^{(t)}, h^{(t)}), \mathbf{M}_j \rangle\\
&= \langle w_i^{(t)}, \mathbf{M}_j \rangle + \eta z_{x}^j z_{y}^j \langle v_i^{(t)}, \mathbf{H}_j \rangle  
+ \eta z_{x}^j z_{y}^{j^{\prime}} \langle v_i^{(t)}, \mathbf{H}_{j^{\prime}} \rangle \pm Err_t.
\end{aligned}
\end{equation}

Similarly, for \(\langle v_i^{(t+1)}, \mathbf{H}_j \rangle\), the update rule becomes:
\begin{equation}
\begin{aligned}
\langle v_i^{(t+1)}, \mathbf{H}_j \rangle &= \langle v_i^{(t)}, \mathbf{H}_j \rangle - \langle \nabla_{v_i} L(f^{(t)}, h^{(t)}), \mathbf{H}_j \rangle\\
&= \langle v_i^{(t)}, \mathbf{H}_j \rangle + \eta z_{x}^j z_{y}^j \langle w_i^{(t)}, \mathbf{M}_j \rangle 
+ \eta z_{x}^j z_{y}^{j^{\prime}} \langle w_i^{(t)}, \mathbf{M}_{j^{\prime}} \rangle \pm Err_t.
\end{aligned}
\end{equation}
% \textcolor{red}{\textbf{why \(Err_t\) is small!!!}}
Using Lemma~\ref{lemma:inner_product_initialization}, we know that with high probability, \(\sum_{x_n \in \mathfrak{N}}\frac{\langle f^{(t)}(x_n), h^{(t)}(y_p) \rangle}{\tau}\le O(\frac{1}{d} )\), so in Eq~(\ref{eq:gradient_wi}) the sum of second term and third term is always less than the first term, until \(\langle f^{(t)}(x_n), h^{(t)}(y_p) \rangle=\Theta(d)\).



The updates for the components \(\langle w_i^{(t+1)}, \mathbf{M}_j \rangle\), \(\langle v_i^{(t+1)}, \mathbf{H}_j \rangle\), \(\langle w_i^{(t+1)}, \mathbf{M}_{j^{\prime}} \rangle\), and \(\langle v_i^{(t+1)}, \mathbf{H}_{j^{\prime}} \rangle\) (where \(j'\) represents the spurious aligned feature corresponding to \(j\)) can be expressed concisely in matrix form as follows:
\begin{equation}
\label{eq:alignment_update_matrix}
\begin{bmatrix}
\langle w_i^{(t+1)}, \mathbf{M}_j \rangle \\
\langle v_i^{(t+1)}, \mathbf{H}_j \rangle \\
\langle w_i^{(t+1)}, \mathbf{M}_{j^{\prime}} \rangle \\
\langle v_i^{(t+1)}, \mathbf{H}_{j^{\prime}} \rangle
\end{bmatrix}
=
\begin{bmatrix}
a & b & 0 & c \\
b & a & c & 0 \\
0 & c & a & b \\
c & 0 & b & a
\end{bmatrix}
\begin{bmatrix}
\langle w_i^{(t)}, \mathbf{M}_j \rangle \\
\langle v_i^{(t)}, \mathbf{H}_j \rangle \\
\langle w_i^{(t)}, \mathbf{M}_{j^{\prime}} \rangle \\
\langle v_i^{(t)}, \mathbf{H}_{j^{\prime}} \rangle
\end{bmatrix}
\pm Err_t,
\end{equation}
where the coefficients are defined as:
\[
a = 1  , \quad
b = z_x^j z_y^j \cdot \mathbf{1}_{\left|\langle w_{i}^{(t)}, x_{p}\rangle\right| \geq b_{i}^{(t)}} \cdot \mathbf{1}_{\left|\langle v_{i}^{(t)}, y_{p}\rangle\right| \geq b_{i}^{(t)}},
\]
\[
c = z_x^j z_y^{j'} \cdot \mathbf{1}_{\left|\langle w_{i}^{(t)}, x_{p}\rangle\right| \geq b_{i}^{(t)}} \cdot \mathbf{1}_{\left|\langle v_{i}^{(t)}, y_{p}\rangle\right| \geq b_{i}^{(t)}}.
\]

Therefore, we have
\begin{equation}
\label{eq:aligned_feature_update_j}
\begin{aligned}
\langle w_i^{(t)}, \mathbf{M}_j \rangle=\langle v_i^{(t)}, \mathbf{H}_j \rangle &= 
\frac{(a+b+c)^t +(a+b-c)^t}{4} 
\left( \langle w_i^{(0)}, \mathbf{M}_j \rangle + \langle v_i^{(0)}, \mathbf{H}_{j} \rangle \right) \\
&+\frac{(a+b+c)^t -(a+b-c)^t}{4} 
\left(  \langle w_i^{(0)}, \mathbf{M}_{j'} \rangle + \langle v_i^{(0)}, \mathbf{H}_{j'} \rangle \right)
\end{aligned}
\end{equation}
and 
\begin{equation}
\label{eq:aligned_feature_update_jj}
\begin{aligned}
\langle w_i^{(t)}, \mathbf{M}_{j'} \rangle=\langle v_i^{(t)}, \mathbf{H}_{j'} \rangle &= 
\frac{(a+b+c)^t +(a+b-c)^t}{4} 
\left( \langle w_i^{(0)}, \mathbf{M}_{j'} \rangle + \langle v_i^{(0)}, \mathbf{H}_{j'} \rangle \right) \\
&+\frac{(a+b+c)^t -(a+b-c)^t}{4} 
\left(  \langle w_i^{(0)}, \mathbf{M}_{j} \rangle + \langle v_i^{(0)}, \mathbf{H}_{j} \rangle \right)
\end{aligned}
\end{equation}



This matrix representation highlights the interactions between the alignment of true and spurious features during SGD updates. The diagonal elements \(a\) dominate the contribution from existing alignments, while the off-diagonal terms \(b, c\) capture the mutual influence between paired features and spurious alignments. Note that if \(c\) is very small, it indicates that the spurious alignment (\(j'\)) has minimal influence, allowing \(w_i\) to focus on learning purified features. Conversely, if \(c\) is large, the spurious alignment could significantly interfere with the learning process, hindering the purification of features. The error term \(Err_t\) accounts for higher-order noise or unmodeled effects in the update process.

Assuming a single spurious feature is a simplification for presentation that was made for ease of presentation in the proof and can be extended to a more general setting without altering the underlying insights.
If each feature \(j\) has \(K{-}1\) spurious correlates, (\ref{eq:alignment_update_matrix}) becomes a \(2K \times 2K\) matrix, and \(N_i = j, j'\) in the last sentence of Theorem~\ref{theorem:pre-train} contains \(j\) and other \(K{-}1\) features. 
Our analysis relies on the total spurious feature probability (bounded by \(C_s\)), not the number of correlated features, so \textbf{as long as the sum of all spurious feature probabilities is upper bounded by \(C_s\), the core mechanism and insights of the theorem remain unchanged.}

\section{Technical Lemmas}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
\begin{definition}[Neuron Characterization]
\label{definition:Neuron}
Let us define a few notations to characterize each neuron \(w_i^{(t)}\)'s behavior.
For every constant \( c_0 \in (0, 1) \) and \(\gamma \in (0, 0.1)\), by choosing \(c_1 = 2 + 2(1-\gamma)c_0\) and \(c_2 = \gamma c_0\), we define:

1. Let \( \mathcal{S}_{j, \text{sure}}^{(t)} \subseteq [m] \) be those neurons \( i \in [m] \) satisfying  
\begin{itemize}
    \item \( (\frac{1}{n} \sum_{i=1}^n \langle w_i^{(t)}, \mathbf{M}_j \rangle )^2 \geq \frac{(c_1+c_2) \log d}{d} \|\mathbf{MM}^\top w_i^{(t)}\|_2^2 \)
    \item \( (\frac{1}{n} \sum_{i=1}^n \langle w_i^{(t)}, \mathbf{M}_{j'} \rangle )^2 < \frac{(c_1-c_2) \log d}{d} \|\mathbf{MM}^\top w_i^{(t)}\|_2^2\)
\end{itemize}

2. Let \( \mathcal{S}_{j, \text{pot}}^{(t)} \subseteq [m] \) be those neurons \( i \in [m] \) satisfying  
\begin{itemize}
    \item \( \langle w_i^{(t)}, \mathbf{M}_{j} \rangle^2 \geq \frac{(c_1-c_2) \log d}{d} \|\mathbf{MM}^\top w_i^{(t)}\|_2^2\)
\end{itemize}
\end{definition}



\begin{lemma}[Geometry at initialization]
\label{lemma:geometry_initialization}
We initialize the parameters by \(w_i^{(0)} \sim \mathcal{N}(0, \sigma_0^2 \mathbf{I}_{d_1})\), where \(\sigma_0^2 = \Theta\left(\frac{1}{d_1 \text{poly}(d)}\right)\).
We have with probability \(\geq 1 - o(1/d^3)\) over the random initialization, for all \(j \in [d]\):
\begin{align*}
    \left| \mathcal{S}_{j, \text{sure}}^{(0)} \right| &= \Omega\left(d^{\frac{\gamma}{4} c_0}\right) =: \Xi_1 \\
    \left| \mathcal{S}_{j, \text{pot}}^{(0)} \right| &\leq O\left(d^{2 \gamma c_0}\right) =: \Xi_2
\end{align*}
\end{lemma}

\begin{proof}
If \( g \) is standard Gaussian, then for every \( t > 0 \),
\begin{equation}
\frac{1}{\sqrt{2\pi}} \frac{(t)}{t^2 + 1} e^{-t^2/2} 
< \Pr_{g \sim \mathcal{N}(0,1)}[g > t] 
< \frac{1}{\sqrt{2\pi}} \frac{1}{(t)} e^{-t^2/2}.
\end{equation}

We initialize the parameters by \(w_i^{(0)} \sim \mathcal{N}(0, \sigma_0^2 \mathbf{I}_{d_1})\), where \(\sigma_0^2 = \Theta\left(\frac{1}{d_1 \text{poly}(d)}\right)\).
We have \(
\frac{1}{n} \sum_{i=1}^n \langle w_i^{(0)}, \mathbf{M}_i \rangle \sim \mathcal{N}\left(0, \frac{\sigma_0^2}{n}\right)
\).

Therefore, for every \(i \in m \) and \(j \in d\), we have
\begin{equation}
\small
\begin{aligned}
p_1 &= \Pr\left[\left(\frac{1}{n} \sum_{i=1}^n \langle w_i^{(0)}, \mathbf{M}_j \rangle \right)^2
\geq  (c_1 + c_2)\frac{\sigma_0^2}{n} \log d  \right] \\
&= \Theta\left(\frac{1}{\log d}\right) \cdot \frac{1}{d^{(c_1 + c_2)/2}} \\
&= \Theta\left(\frac{1}{\sqrt{\log d}}\right) \cdot \frac{1}{d \cdot d^{(1 - \gamma/2)c_0}}
\end{aligned}
\label{eq p1}
\end{equation}

\begin{equation}
\small
\begin{aligned}
p_2 &= \Pr\left[\left(\frac{1}{n} \sum_{i=1}^n \langle w_i^{(0)}, \mathbf{M}_{j'} \rangle \right)^2
\geq  (c_1 - c_2)\frac{\sigma_0^2}{n} \log d  \right] \\
&= \Theta\left(\frac{1}{\log d}\right) \cdot \frac{1}{d^{(c_1 - c_2)/2}} \\
&= \Theta\left(\frac{1}{\sqrt{\log d}}\right) \cdot \frac{1}{d \cdot d^{(1 - 3\gamma/2)c_0}}
\end{aligned}
\label{eq p2}
\end{equation}


Let \( \mathcal{S}_{j, \text{sure}}^{(0)} \subseteq [m] \) be those neurons \( i \in [m] \) satisfying  
\begin{itemize}
    \item \( (\frac{1}{n} \sum_{i=1}^n \langle w_i^{(0)}, \mathbf{M}_j \rangle )^2 \geq \frac{(c_1+c_2) \log d}{d} \|\mathbf{MM}^\top w_i^{(0)}\|_2^2 \)
    \item \( (\frac{1}{n} \sum_{i=1}^n \langle w_i^{(0)}, \mathbf{M}_{j'} \rangle )^2 < \frac{(c_1-c_2) \log d}{d} \|\mathbf{MM}^\top w_i^{(0)}\|_2^2\)
\end{itemize}
By concentration with respect to all \( m \) choices of \( i \in [m] \), we know with probability at least \( 1 - o\left(\frac{1}{d^3}\right)\)
it satisfies \( \left|\mathcal{S}_{j, \text{sure}}^{(0)}\right| = \Omega\left(d^{\frac{\gamma}{4} c_0}\right) \).

Let \( \mathcal{S}_{j, \text{pot}}^{(0)} \subseteq [m] \) be those neurons \( i \in [m] \) satisfying  
\begin{itemize}
    \item \( \langle w_i^{(0)}, \mathbf{M}_{j} \rangle^2 \geq \frac{(c_1-c_2) \log d}{d} \|\mathbf{MM}^\top w_i^{(0)}\|_2^2\)
\end{itemize}
By concentration with respect to all \( m \) choices of \( i\in [m] \), we know with probability at least \( 1 - o\left(\frac{1}{d^3}\right) \) it satisfies
\( \left|\mathcal{S}_{j, \text{pot}}^{(0)}\right| = O\left(d^{2 \gamma c_0}\right) \).

More details of the proof can be found in Lemma B.2 of \cite{allen2022feature}.
\end{proof}

\begin{lemma}
With high probability \(1 - \frac{1}{\text{poly}(d)}\), for every \(i \in [m]\), the following holds:
\begin{equation}
\Pr\left[(\frac{1}{2n} \sum_{i=1}^n \langle w_i^{(0)}, \mathbf{M}_j \rangle-\langle w_i^{(0)}, \mathbf{M}_{j'} \rangle )^2
\geq  \frac{1}{d} \frac{\sigma_0^2}{2n} \log d  \right] 
\ge 1- O(\frac{1}{\sqrt{d} } )
\label{eq p3}
\end{equation}
\end{lemma}


\begin{lemma}
With high probability \(1 - \frac{1}{\text{poly}(d)}\), for every \(i \in [m]\), the following holds:
\begin{equation}
\|\mathbf{MM}^\top w_i^{(0)}\|_2^2 + \|\mathbf{HH}^\top v_i^{(0)}\|_2^2 \in 2d\sigma_0^2 \left[1 - \widetilde{O}\left(\frac{1}{\sqrt{d}}\right), 1 + \widetilde{O}\left(\frac{1}{\sqrt{d}}\right)\right].
\end{equation}
\end{lemma}

\begin{proof}
Let \( X \sim \chi^2_n \). By standard properties of the chi-squared distribution, we know that with probability at least \( 1 - \delta \),
\begin{equation}
\left| X - n \right| \leq 2\sqrt{n \log(1/\delta)}.
\end{equation}

In our case, we consider 
\(
\frac{\|\mathbf{MM}^\top w_i^{(0)}\|_2^2 + \|\mathbf{HH}^\top v_i^{(0)}\|_2^2}{\sigma_0^2} \sim \chi^2_{2d}.
\)
Setting \(\delta = \frac{1}{\text{poly}(d)}\), we have \(n = 2d\), and thus, with high probability \(1 - \frac{1}{\text{poly}(d)}\), the following holds:
\begin{equation}
\left|\frac{\|\mathbf{MM}^\top w_i^{(0)}\|_2^2 + \|\mathbf{HH}^\top v_i^{(0)}\|_2^2}{\sigma_0^2} - 2d\right| \leq 2\sqrt{2d \log(\text{poly}(d))}.
\end{equation}

Rearranging and incorporating the scaling factor \(\sigma_0^2\), we get:
\begin{equation}
\|\mathbf{MM}^\top w_i^{(0)}\|_2^2 + \|\mathbf{HH}^\top v_i^{(0)}\|_2^2 \in 2d\sigma_0^2 \left[1 - \widetilde{O}\left(\frac{1}{\sqrt{d}}\right), 1 + \widetilde{O}\left(\frac{1}{\sqrt{d}}\right)\right].
\end{equation}

\end{proof}


\begin{lemma}[Noise Projection Bound]
\label{lemma:Noise Projection Bound}
For the spurious dense noise \(\xi_{x_p} \sim \mathcal{N}(0, \sigma_\xi^2 \mathbf{I}_{d_1})\), where the variance satisfies \( \omega\left(\frac{1}{d_1}\right) \leq \sigma_\xi^2 \leq O\left(\frac{1}{d}\right)\), the following holds with high probability \(1 - e^{-\Omega(d_1)}\):
\begin{equation}
|\langle w_i, \xi \rangle|^2 \leq O\left(\frac{\|w_i\|_2^2}{d^{1+c_0}}\right), \quad \forall i \in [m].
\end{equation}
\end{lemma}

\begin{proof}
For all \(j \in [d_1]\), by the properties of the Gaussian distribution, we have:
\begin{equation}
\Pr_{\xi} \left[
\langle \mathbf{M}_j, \xi \rangle^2 \leq O\left(\frac{1}{d^{1+c_0}}\right)
\right] 
\geq 1 - e^{-\Omega(d_1)}.
\end{equation}
Now, consider the term \( |\langle w_i, \xi \rangle|^2 \). We decompose it as:
\begin{equation}
\begin{aligned}
|\langle w_i, \xi \rangle|^2 
&= \sum_{j \in [d]} |\langle w_i, \mathbf{M}_j \rangle|^2 \cdot |\langle \mathbf{M}_j, \xi \rangle|^2 
+ \sum_{j \in [d_1] \setminus [d]} |\langle w_i, \mathbf{M}_j^\perp \rangle|^2 \cdot |\langle \mathbf{M}_j^\perp, \xi \rangle|^2.
\end{aligned}
\end{equation}
For the first term, since \(|\langle \mathbf{M}_j, \xi \rangle|^2 \leq O\left(\frac{1}{d^{1+c_0}}\right)\) with high probability, we have:
\begin{equation}
\sum_{j \in [d]} |\langle w_i, \mathbf{M}_j \rangle|^2 \cdot |\langle \mathbf{M}_j, \xi \rangle|^2 
\leq \sum_{j \in [d]} O\left(\frac{|\langle w_i, \mathbf{M}_j \rangle|^2}{d^{1+c_0}}\right).
\end{equation}
Similarly, for the second term:
\begin{equation}
\sum_{j \in [d_1] \setminus [d]} |\langle w_i, \mathbf{M}_j^\perp \rangle|^2 \cdot |\langle \mathbf{M}_j^\perp, \xi \rangle|^2 
\leq \sum_{j \in [d_1] \setminus [d]} O\left(\frac{|\langle w_i, \mathbf{M}_j^\perp \rangle|^2}{d^{1+c_0}}\right).
\end{equation}
Combining these, we have:
\begin{equation}
|\langle w_i, \xi \rangle|^2 
\leq O\left(\frac{\|\mathbf{MM}^\top w_i\|_2^2}{d^{1+c_0}} + \frac{\|\mathbf{M^\perp}\mathbf{M^\perp}^\top w_i\|_2^2}{d^{1+c_0}}\right).
\end{equation}
Since \(\|\mathbf{MM}^\top w_i\|_2^2 + \|\mathbf{M^\perp}\mathbf{M^\perp}^\top w_i\|_2^2 = \|w_i\|_2^2\), we conclude:
\begin{equation}
|\langle w_i, \xi \rangle|^2 \leq O\left(\frac{\|w_i\|_2^2}{d^{1+c_0}}\right).
\end{equation}
Thus, the lemma holds.
\end{proof}

\begin{lemma}[Tail Bound for Matrix Product]
\label{lemma:random_matrix_product}
Let \(\mathbf{Q} \in \mathbb{R}^{n \times n}\) be a symmetric matrix, and let \(w, v\) be independent zero-mean Gaussian random vectors with covariance matrix \(\mathbf{I}_n\). Define
\begin{equation}
Z := \sum_{i,j=1}^n Q_{ij} w_i v_j.
\end{equation}
Then, for any \(\delta > 0\), the following tail bound holds:
\begin{equation}
\Pr[|Z| \geq \delta] \leq 4 \exp\left( -\frac{\delta^2}{4 \|\mathbf{Q}\|_F^2 + 4\delta \|\mathbf{Q}\|_\text{op}} \right).
\end{equation}
\end{lemma}

\begin{lemma}[Bound Inner Product]
\label{lemma:inner_product_initialization}
Consider the inner product between the feature vectors at initialization:
\begin{equation}
\langle f(x), h(y) \rangle = \langle \mathbf{W}x, \mathbf{V}y \rangle 
= \sum_{l=1}^m w_l^\top x y^\top v_l 
= \sum_{l=1}^m \sum_{i,j=1}^{d_1} (x_i^\top y_j) w_l^\top v_l.
\end{equation}
Here, using Lemma \ref{lemma:random_matrix_product}, \(\mathbf{Q} = x y^\top\), with \(\|\mathbf{Q}\|_\text{op} = \Theta(1)\), \(\|\mathbf{Q}\|_F = \Theta(1)\) and \(\sigma_0^2 = \Theta\left(\frac{1}{d_1 \text{poly}(d)}\right)\). Then, at initialization (\(t = 0\)), the following holds:
\begin{equation}
\Pr[|\langle f^{(t)}(x), h^{(t)}(y) \rangle| \geq \Omega(1)] \leq e^{- \text{poly}(d)},
\end{equation}
\end{lemma}

\begin{lemma}[Concentration bound for empirical loss and gradients]
There exist \(N \ge {poly}(d)\) for some sufficiently large polynomial and all \(\left \| w_i \right \|_2 \le O(d)\), \(i\in [m]\) , it satisfies
\begin{equation}
    \left | \frac{1}{N} \sum_{p\in [N]} L(f^{(t)}, h^{(t)};(x_p,y_p))-\mathbb{E}_{(x_p, y_p) \in D}[L(f^{(t)}, h^{(t)};(x_p,y_p))] \right | \le O(\frac{1}{d} )
\end{equation}

\begin{equation}
    \left \| \frac{1}{N} \sum_{p\in [N]} \nabla_{w_{i}}L(f^{(t)}, h^{(t)};(x_p,y_p))-\mathbb{E}_{(x_p, y_p) \in D}[\nabla_{w_{i}}L(f^{(t)}, h^{(t)};(x_p,y_p))]  \right \|_2  \le O(\frac{1}{d} )
\end{equation}
\end{lemma}
\begin{proof}
The proof  can be done by trivial VC dimension or Rademacher complexity
arguments similarly to Lemma A.2. \cite{allen2022feature}.  
\end{proof}


\begin{lemma}[Misalignment Probability Bound]
\label{lemma:MisalignmentProbabilityBound}
The probability of spurious alignment satisfies:
\begin{equation}
\frac{\log \left(\frac{1}{2\gamma c_0}\right)}{2\log \frac{d_1}{d}} < \Pr(|z_{y_p}^j| = 1 \mid |z_{x_p}^{j'}| = 1) < \frac{1}{2}.
\end{equation}
\end{lemma}

\begin{proof}
By concentration over all \( m \) choices of \( i \in [m] \), we find that with probability at least \( 1 - o\left(\frac{1}{d^3}\right) \), the number of neurons satisfying:
\begin{equation}
\left(\frac{1}{n} \sum_{i=1}^n \langle w_i, \mathbf{M}_j \rangle \right)^2
< (c_1 + 4c_2)\frac{\sigma_0^2}{n} \log d
\end{equation}
is \( o(1) \).

In addition, for all neurons, we have:
\begin{equation}
\max \left( \langle w_i^{(T_1)}, \mathbf{M}_{j'} \rangle^2 \right) \leq \frac{c_1 + 3c_2}{2} \frac{\log d}{d} \cdot 
\frac{\|w_i^{(T_1)}\|_2^2+\|v_i^{(T_1)}\|_2^2}{2}.
\end{equation}

Define:
\begin{equation}
\Delta^{(T_1)}= \frac{(a+b-c)^{T_1}}{4} 
 \left |  
\langle w_i^{(0)}, \mathbf{M}_j \rangle + \langle v_i^{(0)}, \mathbf{H}_{j} \rangle -\langle w_i^{(0)}, \mathbf{M}_{j^{\prime}} \rangle - \langle v_i^{(0)}, \mathbf{H}_{j^{\prime}} \rangle 
\right |.
\end{equation}

Thus:
\begin{equation}
\label{eq:wimax}
\langle w_i^{(T_1)}, \mathbf{M}_{j'} \rangle^2 = \left | \max \left( \langle w_i^{(T_1)}, \mathbf{M}_{j'} \rangle \right)- \Delta^{(T_1)}  \right | ^2 \geq \frac{c_1 - c_2}{2} \frac{\log d}{d} \cdot  \frac{\|w_i^{(T_1)}\|_2^2+\|v_i^{(T_1)}\|_2^2}{2}.
\end{equation}

We begin by expressing \( a+b-c \) and \( a+b+c \) as functions of \( P_1 = \Pr(|z_{y_p}^j| = 1 \mid |z_{x_p}^{j'}| = 1) \) and \( P_2 = \Pr(|z_{y_p}^j| = 1 \mid |z_{x_p}^j| = 1) \), where \( P_1 + P_2 = 1 \):
\begin{equation}
a+b-c = 1 - \eta \lambda + \eta \frac{(P_1 - P_2) C_z \log \log d}{d},
\end{equation}
\begin{equation}
a+b+c = 1 - \eta \lambda + \eta \frac{(P_1 + P_2) C_z \log \log d}{d}.
\end{equation}

Using Eq~(\ref{eq:wimax}), Eq~(\ref{eq:aligned_feature_update_j}) and Eq~(\ref{eq:aligned_feature_update_jj}), we derive:
\begin{equation}
\frac{(a+b-c)^{2T_1}}{(a+b+c)^{2T_1}} \leq \left( \sqrt{\frac{c_1 + 3c_2}{2}} - \sqrt{\frac{c_1 - c_2}{2}} \right)^2 \leq 2c_2^2.
\end{equation}

Substituting back, we find:
\begin{equation}
\frac{\log \left(\frac{1}{2\gamma c_0}\right)}{2\log \frac{d_1}{d}} < P_1 < \frac{1}{2}.
\end{equation}

For example, setting \( c_0 = 0.1 \), \( \gamma = 0.005 \), \( d = 100 \), and \( d_1 = 10000 \), we calculate:
\begin{equation}
\frac{1}{4} \leq \Pr(|z_{y_p}^j| = 1 \mid |z_{x_p}^{j'}| = 1) < \frac{1}{2}.
\end{equation}

This concludes the proof by bounding \( \Pr(|z_{y_p}^j| = 1 \mid |z_{x_p}^{j'}| = 1) \) under the given conditions.
\end{proof}






\section{ITCP on Raw Data I}
\label{appendix:itcp_raw_stage1}
In this section we analyze Phase I of ITCP on Raw Data as the training iterations \( t \leq T_1 \), where \(T_1 = \Theta\left(\frac{d \log d}{\eta}\right)\)
is the iteration when all \(
\frac{\|w_i^{(T_1)}\|_2^2+\|v_i^{(T_1)}\|_2^2}{2}  \ge \|w_i^{(0)}\|_2^2+\|v_i^{(0)}\|_2^2
\).
When \(t \le T_1\), we set $b_{i}^{(t)}=0$. 
For every neuron \( i \in [m] \), the weights \( w_i \) and \( v_i \) exhibit an increase in alignment along the direction of informative features \( \mathbf{M} \) and \( \mathbf{H} \), while showing negligible increase in alignment along the direction of noise features \( \mathbf{M}^\perp \) and \( \mathbf{H}^\perp \).

Based on subsection \ref{subsec:feature_coupling_expected_values}, we have  \( \Pr(|z_{y_p }^j| = 1 \mid |z_{x_p}^{j'}| = 1)=\Theta (1) \), so \( \mathbb{E}\left[ z_x^j z_y^j \right] \) and \( \mathbb{E}\left[ z_x^j z_y^{j'} \right] \) both in \( \Theta\left(\frac{1}{d}\right) \). In this case, \( w_i^{(t+1)} \) is jointly influenced by \( \mathbf{M}_j \) and \( \mathbf{M}_{j'} \), with both features contributing comparably to the updates.

To simplify our analysis, we consider the worse case where  \( \Pr(|z_{y_p}^{j'}| = 1 \mid |z_{x_p}^{j}| = 1) =\Pr(|z_{y_p}^{j}| = 1 \mid |z_{x_p}^{j}| = 1) =\frac{1}{2} \) such that \( \mathbb{E}\left[ z_x^j z_y^j \right] = \mathbb{E}\left[ z_x^j z_y^{j'} \right] = \frac{C_z}{2d} \), so using Eq~(\ref{eq:aligned_feature_update_j}), Eq~(\ref{eq:aligned_feature_update_jj}) and $b_{i}^{(t)}=0$, we have
\begin{equation}
\label{eq:aligned_feature_update}
\begin{aligned}
\langle w_i^{(t)}, \mathbf{M}_j \rangle &= 
\frac{(a+b+c)^t}{4} 
\left( \langle w_i^{(0)}, \mathbf{M}_j \rangle + \langle v_i^{(0)}, \mathbf{H}_{j} \rangle+\langle w_i^{(0)}, \mathbf{M}_{j'} \rangle + \langle v_i^{(0)}, \mathbf{H}_{j'} \rangle \right)
\end{aligned}
\end{equation}

\begin{equation}
\label{eq:spurious_feature_update}
\begin{aligned}
\langle w_i^{(t)}, \mathbf{M}_{j'} \rangle &= 
\frac{(a+b+c)^t}{4} 
\left( \langle w_i^{(0)}, \mathbf{M}_j \rangle + \langle v_i^{(0)}, \mathbf{H}_{j} \rangle+\langle w_i^{(0)}, \mathbf{M}_{j'} \rangle + \langle v_i^{(0)}, \mathbf{H}_{j'} \rangle \right)
\end{aligned}
\end{equation}

This represents the worst-case scenario as the contributions of the aligned feature \(\mathbb{E}\left[ z_x^j z_y^j \right]\) and the spurious feature \(\mathbb{E}\left[ z_x^j z_y^{j'} \right]\) are identical. Under real circumstances, we expect \(\mathbb{E}\left[ z_x^j z_y^j \right] < \mathbb{E}\left[ z_x^j z_y^{j'} \right]\), which would result in \(\langle w_i^{(t+1)}, \mathbf{M}_j \rangle > \langle w_i^{(t+1)}, \mathbf{M}_{j'} \rangle\). However, in this worst-case scenario, the equality of contributions prevents the network from prioritizing purified features, resulting in equal magnitudes for \(\langle w_i^{(t+1)}, \mathbf{M}_j \rangle\) and \(\langle w_i^{(t+1)}, \mathbf{M}_{j'} \rangle\), thereby hindering effective feature separation.

We first provide a lower bound for \( \| \mathbf{MM}^\top w_i^{(t)} \|_2^2 \) for iterations \( t \leq t_1 \). From Eq~(\ref{eq:aligned_feature_update}) and Eq~(\ref{eq:spurious_feature_update})  we have:
\begin{equation}
\small
\begin{aligned}
\|\mathbf{MM}^\top w_i^{(t)}\|_2^2 &= 
\sum_{i=1}^d 
\left[ 
\frac{(a+b+c)^t}{4} 
\left( 
\langle w_i^{(0)}, \mathbf{M}_j \rangle + \langle v_i^{(0)}, \mathbf{H}_{j} \rangle 
\right) 
+ \frac{(a+b+c)^t}{4} 
\left( 
\langle w_i^{(0)}, \mathbf{M}_{j^{\prime}} \rangle + \langle v_i^{0}, \mathbf{H}_{j^{\prime}} \rangle 
\right)
\right]^2 \\
&= 
\left( 
1 + \frac{\eta C_z}{d}   
\right)^{2t} 
\frac{\|\mathbf{MM}^\top w_i^{(0)}\|_2^2 + \|\mathbf{HH}^\top v_i^{(0)}\|_2^2}{8}
\label{eq MMw}
\end{aligned}
\end{equation}

\begin{equation}
\label{eq MMw2}
\begin{aligned}
\|\mathbf{M}^\perp (\mathbf{M}^\perp)^\top w_i^{(t)}\|_2^2 
{\leq} \left(1 + \frac{1}{\text{poly}(d)}\right) \|\mathbf{M}^\perp (\mathbf{M}^\perp)^\top w_i^{(0)}\|_2^2.
\end{aligned}
\end{equation}
The detailed proof of Eq~(\ref{eq MMw2}) can be found in Hypothesis C.4 of \cite{wen2021toward}.

A similar result holds for \(\|\mathbf{HH}^\top v_i^{(t)}\|_2^2\) and \(\|\mathbf{H}^\perp (\mathbf{H}^\perp)^\top v_i^{(t)}\|_2^2\).

Eq~(\ref{eq MMw}) and Eq~(\ref{eq MMw2}) shows that the image and text dictionary features \(\mathbf{M}, \mathbf{H}\) can grow exponentially, while the noisy features \(\mathbf{M}^\perp, \mathbf{H}^\perp\) remain almost unchanged when \(t \leq T_1\).

For \(\mathbf{M}^\perp_j\) where \(j \in [d_1] \setminus [d]\), using Eq~(\ref{eq MMw2}), we obtain:
\begin{equation}
|\langle w_i^{(t+1)}, \mathbf{M}_j^\perp \rangle|^2 \leq O\left(\frac{1}{d_1}\right) \|w_i^{(0)}\|_2^2 \leq O\left(\frac{1}{d_1}\right) \cdot \frac{\|w_i^{(T_1)}\|_2^2 + \|v_i^{(T_1)}\|_2^2}{2}.
\end{equation}
This result demonstrates that the noisy features \(\mathbf{M}^\perp_j\) experience nearly no increase during this phase, remaining insignificant in their contribution to the alignment of \(w_i\).

\subsection{Lower Bound of Alignment for \( i \in S_{j, \text{sure}} \)}
This section provides a analysis of the alignment growth for neurons \( i \in S_{j, \text{sure}} \). 
Specifically, we demonstrate that for every \( j \in [d] \), if \( i \in S_{j, \text{sure}} \), the alignment \(\langle \mathbf{M}_j, w_i^{(t)} \rangle^2\) and its spurious alignment \(\langle \mathbf{M}_j', w_i^{(t)} \rangle^2\) increase exponentially when \( t \le T_1 \).

We now prove the lower bound of \(|\langle w_i^{(T_1)}, \mathbf{M}_j \rangle|^2\) for \( i \in S_{j, \text{sure}} \): 
\begin{equation}
\begin{aligned}
|\langle w_i^{(T_1)}, \mathbf{M}_j \rangle|^2 &= \left(1   + \eta \frac{C_z}{d} \right)^{2T_1} 
\left( 
 \frac{ \langle w_i^{(0)}, \mathbf{M}_j \rangle + \langle v_i^{(0)}, \mathbf{H}_{j'} \rangle + \langle w_i^{(0)}, \mathbf{M_{j'}} \rangle + \langle v_i^{(0)}, \mathbf{H}_j \rangle}{4} 
\right)^{2}  \\
&\overset{\diamondsuit }{\geq} \left(1   + \eta \frac{C_z}{d} \right)^{2T_1} \cdot \frac{(c_1+c_2)\log d}{d} 
\cdot \frac{\|\mathbf{MM}^\top w_i^{(0)}\|_2^2 + \|\mathbf{HH}^\top v_i^{(0)}\|_2^2}{8} \\
&\overset{\heartsuit  }{=} \frac{(c_1+c_2)\log d}{d} 
\cdot \frac{\|\mathbf{MM}^\top w_i^{(T_1)}\|_2^2 + \|\mathbf{HH}^\top v_i^{(T_1)}\|_2^2}{2}\\
&\overset{\clubsuit  }{\geq} \frac{(c_1+c_2) \log d}{d} \cdot \frac{\|w_i^{(T_1)}\|_2^2+\|v_i^{(T_1)}\|_2^2 -\|w_i^{(0)}\|_2^2-\|v_i^{(0)}\|_2^2}{2} \\
&\overset{\spadesuit  }{>} \frac{(1+c_0-\gamma c_0) \log d}{d} \cdot 
\frac{\|w_i^{(T_1)}\|_2^2+\|v_i^{(T_1)}\|_2^2}{2} 
\end{aligned}
\end{equation}
In \( \diamondsuit \) we use Definition~\ref{definition:Neuron}. In \(\heartsuit\) we use Eq~(\ref{eq MMw}). In \(\clubsuit\) we use 
\(
\frac{\|w_i^{(T_1)}\|_2^2+\|v_i^{(T_1)}\|_2^2}{2}  \ge \|w_i^{(0)}\|_2^2+\|v_i^{(0)}\|_2^2
\).
In \( \spadesuit\) we use \(c_1+c_2 > 2(1+c_0-\gamma c_0) \).

Similarly, \(|\langle w_i^{(T_1)}, \mathbf{M}_{j'} \rangle|^2\) have the same lower bound.



\subsection{Upper Bound of Alignment for \( i \notin S_{j, \text{pot}} \)}
In this subsection, we analyze the alignment of neuron \( i \notin S_{j, \text{pot}} \) with the feature \(\mathbf{M}_j\) and provide an upper bound for \(|\langle w_i^{(T_1)}, \mathbf{M}_j \rangle|^2\). While neurons \( i \notin S_{j, \text{pot}} \) still exhibit exponential growth in their alignment, their weaker initialization results in significantly smaller alignment compared to neurons in \( S_{j, \text{sure}} \), limiting their contribution to learning the feature \(\mathbf{M}_j\).

To establish the bound, we begin with the following expression:
\begin{equation}
\begin{aligned}
|\langle w_i^{(T_1)}, \mathbf{M}_j \rangle|^2 &= \left(1   + \eta \frac{C_z}{d} \right)^{2T_1}
\left( 
 \frac{\langle w_i^{(0)}, \mathbf{M}_j \rangle + \langle v_i^{(0)}, \mathbf{H}_{j'} \rangle + \langle w_i^{(0)}, \mathbf{M_{j'}} \rangle + \langle v_i^{(0)}, \mathbf{H}_j \rangle}{4} 
\right)^2 \\
&\overset{\diamondsuit }{\leq} \left(1   + \eta \frac{C_z}{d} \right)^{2T_1} \cdot \frac{(c_1 - c_2)\log d}{d} 
\cdot \frac{\|\mathbf{MM}^\top w_i^{(0)}\|_2^2 + \|\mathbf{HH}^\top v_i^{(0)}\|_2^2}{8} \\
&= \frac{(c_1 - c_2)\log d}{d} 
\cdot \frac{\|\mathbf{MM}^\top w_i^{(T_1)}\|_2^2 + \|\mathbf{HH}^\top v_i^{(T_1)}\|_2^2}{2}.
\end{aligned}
\end{equation}

Here, in \( \diamondsuit \), we use Lemma~\ref{definition:Neuron}, which captures the reduced alignment for neurons outside \( S_{j, \text{pot}} \). 

Similar to the analysis for \( i \in S_{j, \text{sure}} \), the alignment strength for \( i \notin S_{j, \text{pot}} \) is weaker, as \( c_1 - c_2 \) is less than \( 2(1+c_0-\gamma c_0) \), leading to:
\begin{equation}
|\langle w_i^{(T_1)}, \mathbf{M}_j \rangle|^2 < \frac{(1+c_0-3\gamma c_0)\log d}{d} \cdot 
\frac{\|w_i^{(T_1)}\|_2^2+\|v_i^{(T_1)}\|_2^2}{2}.
\end{equation}

This inequality highlights the slower alignment for neurons outside \( S_{j, \text{pot}} \), distinguishing their behavior from neurons in \( S_{j, \text{sure}} \). Consequently, \( i \notin S_{j, \text{pot}} \) contributes less significantly to the alignment of \(\mathbf{M}_j\), reinforcing the importance of initial affinity for effective alignment.


\subsection{Summary}
At this stage (\(t \leq T_1\)), we do not consider the worst-case scenario where the probability bounds for feature coupling satisfy 
\[
\frac{\log (1/c_0)}{2\log d} < \Pr(|z_{y_p}^{j'}| = 1 \mid |z_{x_p}^j| = 1) < \frac{1}{2} < \Pr(|z_{y_p}^j| = 1 \mid |z_{x_p}^j| = 1) < 1
\]
(as assumed in SubSection~\ref{subsec:feature_coupling_expected_values}). Thus, we summarize the results when \(t\le T_1\) as follows:

1. For \(i \in S_{j, \text{sure}}\), the alignment strength satisfies:
\begin{equation}
\label{eq:alignment_strength_sure1}
|\langle w_i^{(T_1)}, \mathbf{M}_j \rangle|^2 > |\langle w_i^{(T_1)}, \mathbf{M}_{j'} \rangle|^2 > \frac{(1 + c_0 - \gamma c_0) \log d}{d} \cdot \frac{\|w_i^{(T_1)}\|_2^2 + \|v_i^{(T_1)}\|_2^2}{2},
\end{equation}
where \(j'\) represents the corresponding spurious alignment feature.

2. For \(i \notin S_{j, \text{pot}}\), the alignment strength satisfies:
\begin{equation}
\label{eq:alignment_strength_not_sure1}
|\langle w_i^{(T_1)}, \mathbf{M}_j \rangle|^2 < \frac{(1 + c_0 - 3\gamma c_0) \log d}{d} \cdot \frac{\|w_i^{(T_1)}\|_2^2 + \|v_i^{(T_1)}\|_2^2}{2}.
\end{equation}

3. For \(\mathbf{M}_j^\perp\) where \(j \in [d_1] \setminus [d]\), we have:
\begin{equation}
|\langle w_i^{(t+1)}, \mathbf{M}_j^\perp \rangle|^2 < O\left(\frac{1}{d_1}\right) \cdot \frac{\|w_i^{(T_1)}\|_2^2 + \|v_i^{(T_1)}\|_2^2}{2}.
\end{equation}

These results demonstrate that when \(t \le T_1\), all features in \(\mathbf{M}\) increase, but the alignment for \(i \in S_{j, \text{sure}}\), including the corresponding spurious alignment, grows significantly larger due to favorable initialization. In contrast, noisy features \(\mathbf{M}^\perp\) remain unchanged.


\section{ITCP on Raw Data II}
\label{appendix:itcp_raw_stage2}
The Phase II of ITCP on Raw Data is defined as the training iterations \( T_1 < t \leq T_2 \), where \(T_2-T_1 = \Theta\left(\frac{d \log d}{\eta}\right)\).

At the beginning of this phase, we set the bias threshold as:
\begin{equation}
\label{eq:bias_threshold_T1}
b_{i}^{(T_1)} = \sqrt{\frac{(1 + c_0 - 2\gamma c_0)\log d}{d} \cdot \frac{\|w_i^{(T_1)}\|_2^2 + \|v_i^{(T_1)}\|_2^2}{2}}.
\end{equation}

During training, the bias threshold is iteratively updated as:
\begin{equation}
\label{eq:bias_update_rule1}
b_{i}^{(t+1)} = \left(1 + \frac{\eta}{d}\right) b_{i}^{(t)},
\end{equation}
until all neurons satisfy:
\begin{equation}
\label{eq:neuron_growth_condition_T2}
\|w_i^{(T_2)}\|_2^2 \geq \Omega(d) \|w_i^{(T_1)}\|_2^2.
\end{equation}

In this phase, the dynamics of alignment vary depending on whether a neuron belongs to \( S_{j, \text{sure}} \) or not:
\begin{itemize}
    \item For \( i \notin S_{j, \text{pot}} \): The weights \( w_i \) and \( v_i \) show negligible alignment growth with both the informative features \(\mathbf{M}_j\), \(\mathbf{H}_j\) and the noise features \(\mathbf{M}^\perp\), \(\mathbf{H}^\perp\).
    This is due to their weaker initialization, as shown in Phase I, and the effect of the indicator function when \(t \ge T_1\) which prevents them from being activated. As a result, their capacity to learn meaningful alignments during this phase is significantly limited.
    
    \item For \( i \in S_{j, \text{sure}} \): The weights \( w_i \) and \( v_i \) exhibit continued alignment growth with the informative features \(\mathbf{M}_j\), \(\mathbf{H}_j\). Additionally, their alignment with the corresponding spurious features \(\mathbf{M}_{j'}\), \(\mathbf{H}_{j'}\) also increases due to their strong initialization, as shown in Phase I, and the effect of the indicator function when \(t \ge T_1\), which ensures they are always activated.

\end{itemize}

By the end of this stage (\(t = T_2\)), the weights \( w_i \), \( v_i \) will predominantly focus on the features \( \mathbf{M}_j \), \( \mathbf{H}_j \) if \( i \in S_{j, \text{sure}} \), while largely ignoring the features \( \mathbf{M}_j \), \( \mathbf{H}_j \) if \( i \notin S_{j, \text{pot}} \), as well as the noise features \( \mathbf{M}^\perp \), \( \mathbf{H}^\perp \).
This separation lays the foundation for the Phase II of ITCP on Raw Data, where spurious alignments are expected to further diminish due to the dominance of true feature alignments.

Similarly to the proof of \(t \le T_1\) To simplify our analysis, we still consider the worse case where  \( \Pr(|z_{y_p}^{j'}| = 1 \mid |z_{x_p}^{j}| = 1) =\Pr(|z_{y_p}^{j}| = 1 \mid |z_{x_p}^{j}| = 1) =\frac{1}{2} \) such that \( \mathbb{E}\left[ z_x^j z_y^j \right] = \mathbb{E}\left[ z_x^j z_y^{j'} \right] = \frac{C_z}{2d} \).

\subsection{Alignment for \( i \in S_{j, \text{sure}} \)}
This section provides a analysis of the alignment growth for neurons \( i \in S_{j, \text{sure}} \). 
Specifically, we demonstrate that for every \( j \in [d] \), if \( i \in S_{j, \text{sure}} \), the alignment \(\langle \mathbf{M}_j, w_i^{(t)} \rangle^2\) and its spurious alignment \(\langle \mathbf{M}_j', w_i^{(t)} \rangle^2\) increase exponentially when \( T_1 < t \le T_2 \).

% In other words, if a neuron \( i \) "wins the lottery ticket" at random initialization (i.e., \( i \in S_{j, \text{sure}} \)), it will deviate from random initialization and grow towards capturing the purified feature \(\mathbf{M}_j\), which becomes the dominant direction of alignment. Later, we will analyze neurons not in \( S_{j, \text{sure}} \) to contrast their dynamics with the neurons that achieve this accelerated alignment.

For \( i \in S_{j, \text{sure}} \),  using Lemma~\ref{lemma:Noise Projection Bound}, the following holds with high probability \(1 - e^{-\Omega(d_1)}\) when 
\( T_1<t\le T_2\) :
\begin{equation}
\label{noise1}
\left| \langle w_i^{(t)}, \xi \rangle \right|^2 
\leq O\left(\frac{\left \| w_i^{(t)} \right \|_2^2 }{d^{1+c_0}} \right)<b_{i}^{(t)}
\end{equation}

Therefore, with high probability \(1 - e^{-\Omega(d_1)}\), using Eq~(\ref{eq:alignment_strength_sure1}) and  Eq~(\ref{eq:bias_threshold_T1}) the indicator function satisfies the condition when \(t = T_1\):
\begin{equation}
\mathbf{1}_{\left|\left\langle w_{i}^{(t)}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}}  
\cdot   \mathbf{1}_{\left|\left\langle v_{i}^{(t)}, y_{p}\right\rangle\right| \geq b_{i}^{(t)}} = 1,
\end{equation}
we can ensure that:
\begin{equation}
\mathbb{E} \left[ z_x^j z_y^j \cdot  \mathbf{1}_{\left|\left\langle w_{i}^{(t)}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}}  
\cdot  \mathbf{1}_{\left|\left\langle v_{i}^{(t)}, y_{p}\right\rangle\right| \geq b_{i}^{(t)}} \right] = \frac{C_z}{d}.
\end{equation}

Using Eq~(\ref{eq:bias_update_rule1}) we know that \(\left(1   + \eta \frac{C_z}{2d} \right) > \left(1 + \frac{\eta}{d}\right)\) and using Eq~(\ref{eq:alignment_update_matrix}) we have 
\begin{equation}
|\langle w_i^{(t+1)}, \mathbf{M}_j \rangle| > (1+\frac{\eta }{d}) b_{i}^{(t)}=b_{i}^{(t+1)}.
\end{equation}
This implies that when \(t > T_1\), the alignment strength of informative features surpasses the updated bias threshold \(b_i^{(t)}\). Consequently, the indicator functions become consistently activated \(T_1 < t \le T_2\) such that
\begin{equation}
\label{eq:indicator_activation_T2}
\mathbf{1}_{\left|\left\langle w_{i}^{(t)}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}}  
\cdot   \mathbf{1}_{\left|\left\langle v_{i}^{(t)}, y_{p}\right\rangle\right| \geq b_{i}^{(t)}} = 1,
\end{equation}

Using Eq~(\ref{eq:alignment_update_matrix}), the weight dynamics for \(|\langle w_i^{(t+1)}, \mathbf{M}_j \rangle|\) can be expressed as when \(T_1 < t \le T_2\):
\begin{equation}
\begin{aligned}
|\langle w_i^{(t+1)}, \mathbf{M}_j \rangle| &= \left(1   + \eta \frac{C_z}{d} \right)
\left( 
 \frac{\langle w_i^{(t)}, \mathbf{M}_j \rangle + \langle v_i^{(t)}, \mathbf{H}_{j'} \rangle + \langle w_i^{(t)}, \mathbf{M_{j}^\perp} \rangle + \langle v_i^{(t)}, \mathbf{H}_j \rangle}{4} 
\right).
\end{aligned}
\end{equation}

Similarly, \(|\langle w_i^{(T_1)}, \mathbf{M}_{j'} \rangle|^2\) have the same result.

\subsection{Alignment for \( i \notin S_{j, \text{pot}} \)}
In this section, we analyze the alignment behavior for neurons \( i \notin S_{j, \text{pot}} \). Specifically, we demonstrate that for every \( j \in [d] \), if \( i \notin S_{j, \text{pot}} \), the alignment \(\langle \mathbf{M}_j, w_i^{(t)} \rangle^2\) exhibits negligible growth during the interval \( T_1 < t \le T_2 \).

For \( i \notin S_{j, \text{pot}} \), using Eq~(\ref{noise1}), Eq~(\ref{eq:bias_threshold_T1}) and Eq~(\ref{eq:alignment_strength_sure1}), we have with high probability \(1 - e^{-\Omega(d_1)}\), similarly to the proof of \( i \in S_{j, \text{sure}} \), the indicator function satisfies the condition when \(t = T_1\):
\begin{equation}
\mathbf{1}_{\left|\left\langle w_{i}^{(t)}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}}  
\cdot   \mathbf{1}_{\left|\left\langle v_{i}^{(t)}, y_{p}\right\rangle\right| \geq b_{i}^{(t)}} = 0,
\end{equation}
We can ensure that:
\begin{equation}
\mathbb{E} \left[ z_x^j z_y^j \cdot  \mathbf{1}_{\left|\left\langle w_{i}^{(t)}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}}  
\cdot  \mathbf{1}_{\left|\left\langle v_{i}^{(t)}, y_{p}\right\rangle\right| \geq b_{i}^{(t)}} \right] \le o\left(\frac{1}{d^2} \right).
\end{equation}

Using Eq~(\ref{eq:bias_update_rule1}) we know that \(\left(1   + o(\frac{\eta}{d^2}) \right) < \left(1 + \frac{\eta}{d}\right)\)  and using Eq~(\ref{eq:alignment_update_matrix}) we have 
\begin{equation}
|\langle w_i^{(t+1)}, \mathbf{M}_j \rangle| < (1+\frac{\eta }{d}) b_{i}^{(t)}=b_{i}^{(t+1)}.
\end{equation}
This implies that when \(t > T_1\), the alignment strength of informative features does not surpass the updated bias threshold \(b_i^{(t)}\). Consequently, the indicator functions become consistently not activated \(T_1 < t \le T_2\) such that
\begin{equation}
\mathbf{1}_{\left|\left\langle w_{i}^{(t)}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}}  
\cdot   \mathbf{1}_{\left|\left\langle v_{i}^{(t)}, y_{p}\right\rangle\right| \geq b_{i}^{(t)}} = 0,
\end{equation}

Using Eq~(\ref{eq:alignment_update_matrix}), the weight dynamics for \(|\langle w_i^{(t+1)}, \mathbf{M}_j \rangle|\) can be expressed as when \(T_1 < t \le T_2\):
\begin{equation}
|\langle w_i^{(t+1)}, \mathbf{M}_j \rangle| \leq \left(1   + o\left(\frac{\eta}{d^2}\right) \right)^t 
\left( \frac{ \langle w_i^{(T_1)}, \mathbf{M}_j \rangle + \langle v_i^{(T_1)}, \mathbf{H}_{j'} \rangle + \langle w_i^{(T_1)}, \mathbf{M_{j'}} \rangle + \langle v_i^{(T_1)}, \mathbf{H}_j \rangle}{4} 
\right)
\end{equation}
Because \(\left(1   + o\left(\frac{\eta}{d^2}\right)\right)^{T_2} \leq 1 + o\left(\frac{1}{d}\right)\), the growth in \(|\langle w_i^{(T_2)}, \mathbf{M}_j \rangle|\) is negligible. Consequently, we have:
\begin{equation}
\label{eq:negligible_growth^{(t)}2}
|\langle w_i^{(T_2)}, \mathbf{M}_j \rangle|^2 
 \leq \left(1 + o\left(\frac{1}{d}\right)\right)|\langle w_i^{(T_1)}, \mathbf{M}_j \rangle|^2.
\end{equation}


\subsection{Summary}
When \( T_2 = \Theta\left(\frac{d \log d}{\eta}\right) \), we know \(\left(1   + \eta \frac{C_z}{d} \right)^{T_2} = poly(d) \). Using Eq~(\ref{eq:alignment_strength_sure1}), we can ensure that when all neurons satisfy the following condition:
\begin{equation}
\|w_i^{(T_2)}\|_2 \geq \Omega(d) \|w_i^{(T_1)}\|_2,
\end{equation}
we terminate the training process at \( T_2 = \Theta\left(\frac{d \log d}{\eta}\right) \). This ensures that the alignment has sufficiently progressed for effective learning.

Thus, using Eq~(\ref{eq:negligible_growth^{(t)}2}) and Eq~(\ref{eq MMw2}) we have
\begin{equation}
\small
\label{eq:alignment_pair_sum_T2}
\begin{aligned}
|\langle w_i^{(T_{2})}, \mathbf{M}_j \rangle|^2+ |\langle w_i^{(T_{2})}, \mathbf{M}_{j^{\prime}} \rangle|^2
&= \|w_i^{(T_{2})}\|_2^2 - \sum_{j \in [d], j \notin \mathcal{N}_i} \langle w_i^{(T_{2})}, \mathbf{M}_j \rangle^2 - \sum_{j \in [d_1] \setminus [d]} \langle w_i^{(T_{2})}, \mathbf{M}_j^\perp \rangle^2 \\
&\ge \|w_i^{(T_2)}\|_2^2 - (1 + o(\frac{1}{d}))(\|w_i^{(T_1)}\|_2^2-|\langle w_i^{(T_{1})}, \mathbf{M}_j \rangle|^2- |\langle w_i^{(T_{1})}, \mathbf{M}_{j^{\prime}} \rangle|^2) \\
&\ge \|w_i^{(T_2)}\|_2^2 -\|w_i^{(T_1)}\|_2^2 - o(\frac{\|w_i^{(T_1)}\|_2^2}{d} )
\end{aligned}
\end{equation}

Thus,  at this stage (\(T_1 <t \leq T_2\)), we do not consider the worst-case scenario where the probability bounds for feature coupling satisfy 
\[
\frac{\log (1/c_0)}{2\log d} < \Pr(|z_{y_p}^{j'}| = 1 \mid |z_{x_p}^j| = 1) < \frac{1}{2} < \Pr(|z_{y_p}^j| = 1 \mid |z_{x_p}^j| = 1) < 1
\]
We summarize the results when \(T_1 < t\le T_2\) as follows:

1. For \(i \in S_{j, \text{sure}}\), the alignment strength satisfies:
\begin{equation}
\label{eq:alignment_strength_sure2}
|\langle w_i^{(T_2)}, \mathbf{M}_j \rangle|^2 > |\langle w_i^{(T_2)}, \mathbf{M}_{j'} \rangle|^2 \ge \frac{1}{4}  \frac{\|w_i^{(T_2)}\|_2^2 + \|v_i^{(T_2)}\|_2^2}{2}
\end{equation}
where \(j'\) represents the corresponding spurious alignment feature.

2. For \(i \notin S_{j, \text{pot}}\), the alignment strength satisfies:
\begin{equation}
\label{eq:alignment_strength_not_sure2}
|\langle w_i^{(T_1)}, \mathbf{M}_j \rangle|^2 \le O(\frac{1}{d} ) \cdot \frac{\|w_i^{(T_2)}\|_2^2 + \|v_i^{(T_2)}\|_2^2}{2}
\end{equation}

3. For \(\mathbf{M}_j^\perp\) where \(j \in [d_1] \setminus [d]\), we have:
\begin{equation}
|\langle w_i^{(t+1)}, \mathbf{M}_j^\perp \rangle|^2 < O\left(\frac{1}{d_1}\right) \cdot \frac{\|w_i^{(T_2)}\|_2^2 + \|v_i^{(T_2)}\|_2^2}{2}.
\end{equation}

These results demonstrate that when \(T_1 < t \le T_2\), the alignment  for \(i \in S_{j, \text{sure}}\), including the corresponding spurious alignment, grows significantly larger. In contrast, the alignment strength for \(i \notin S_{j, \text{pot}}\) and noisy features \(\mathbf{M}^\perp\) remains unchanged.
Similar results also hold for \(v_i\).

% \section{Pre-train II Convergence}
% In the previous section, we demonstrated that for \( t < T_2 \), the neurons \((w_i, v_i)\) are sparsely activated and remain consistently activated. Base on this result, this section establishes the convergence of these neurons to sparse solutions, as stated in our convergence theorem below.

% Using Eq~\((\ref{eq:indicator_activation_T2})\) , we know that \(t\le T_2 = \Theta\left(\frac{d \log d}{\eta}\right)\)
% for \( i \in S_{j, \text{sure}} \), when \( (x_{p}, y_p)\) and \((x_n,y_n)\) contain the true feature \(j\), the indicator functions remain consistently activated. 

% Consequently, using Eq~(\(\ref{eq:loss_L}\)), Eq~\((\ref{eq:gradient_wi})\) and Eq~\((\ref{eq:gradient_vi})\), we know the loss function \( L\) becomes convex with respect to \( w_i \) and \( v_i \) independently when \( (x_{p}, y_p)\) and \((x_n,y_n)\) contain the true feature \(j\).

% In details, based on Eq~(\(\ref{eq:loss_L}\))
% it is easy to know that \(-\langle f^{(t)}(x_p), h^{(t)}(y_p) \rangle\) is convex and \(l_{i,j,1}= \|x\|_2 \|y\|_2=\Theta(1)
% \) smooth. 

% And
% \(L_{i,j,2}=\left( \langle f^{(t)}(x_n), h^{(t)}(y_p) \rangle \right)^2/(2\tau)\) is convex funtion, and we prove the smooth.

% To analyze the \(l_{i,j,2}\)-smoothness, The goal is to find an upper bound such that:
% \begin{equation}
% \|\nabla_{w_i,v_i} L_2(w_{i,1}, v_{i,1}) - \nabla_{w_i,v_i} L_2(w_{i,2}, v_{i,2})\|_2 
% \leq l_{i,j,2} \|(w_{i,1} - w_{i,2}, v_{i,1} - v_{i,2})\|_2
% \end{equation}

% % L = \langle Wx, Vy \rangle^2 , we examine the Lipschitz continuity of its gradient with respect to the parameters  W  and  V . The goal is to find an upper bound (smoothness constant)  L_{\text{smooth}}  such that:


% % \|\nabla L(W_1, V_1) - \nabla L(W_2, V_2)\| \leq L_{\text{smooth}} \|(W_1 - W_2, V_1 - V_2)\|.


% the gradient difference for \(w_i\):
% \begin{equation}
% \begin{aligned}
% \|\nabla_{w_i} L_{i,j,2}(w_{i,1}, v_{i,1}) - \nabla_{w_i} L_{i,j,2}(w_{i,2}, v_{i,2})\|_2 &=  \Big\| \left( x^\top W_1^\top V_1 y \right) x (v_{i,1} y)^\top - \left( x^\top W_2^\top V_2 y \right) x (v_{i,2} y)^\top \Big\|_2/(2\tau) \\
% &\le l_{w_i,1}/(2\tau)\|w_{i,1} - w_{i,2}\|_2 
% + l_{w_i,2}/(2\tau)\|v_{i,1} - v_{i,2}\|_2
% \end{aligned}
% \end{equation}
% where \(l_{w_i,1}=\left \| x_n \right \| _2^2
% \left \| y_p \right \| _2^2
% \left \| v_{i,1} \right \| _2
% \left \| v_{i,2} \right \| _2
% \leq O(d)\) and 
% \(l_{w_i,2}=\left \| x_n \right \| _2^2
% \left \| y_p \right \| _2^2
% (
% \left \| v_{i,1} \right \| _2
% \left \| w_{i,2} \right \| _2
% +\left \| w_{i,1} \right \| _2
% \left \| v_{i,1} \right \| _2
% )
% \leq O(d)\)
% Similarly, the gradient difference for \(w_i\):
% \begin{equation}
% \begin{aligned}
% \|\nabla_{w_i} L_{i,j,2}(w_{i,1}, v_{i,1}) - \nabla_{w_i} L_{i,j,2}(w_{i,2}, v_{i,2})\|_2 
% &\le l_{v_i,1}/(2\tau)\|w_{i,1} - w_{i,2}\|_2 
% + l_{v_i,2}/(2\tau)\|v_{i,1} - v_{i,2}\|_2
% \end{aligned}
% \end{equation}
% where \(l_{v_i,1}\le O(d)\) and \(l_{v_i,2}\le O(d)\).

% Therefore, \(l_{i,j,2}=\sqrt[]{l_{w_i,1}^2+l_{w_i,2}^2+l_{v_i,1}^2+l_{v_i,2}^2}/(2\tau)\le O(1) \) and \(l_{i,j}=l_{i,j,1}+l_{i,j,2}=\Theta(1)\)


\section{ITCP on Raw Data III Convergence}
\label{appendix:itcp_raw_stage3}
In the previous section, we demonstrated that for \( t \le T_2 \), the neurons \((w_i, v_i)\) are sparsely activated and remain consistently activated for \( i \in S_{j, \text{sure}} \). Building on this result, this section establishes the convergence of these neurons to sparse solutions, providing a detailed analysis of their behavior during Phase III of ITCP on Raw Data. 
The following theorem outlines the convergence guarantees under these conditions. 

The Phase III of ITCP on Raw Data is defined as the training iterations \(T_2<t\le T_3\), where \(T_3-T_2=\Theta(d)\).
At the beginning of this phase, we fix the bias threshold as \(b_i^{(t)}=b_i^{T_2}\) for \(T_2<t\le T_3\).  
Because \(b_{i}^{(T_2)} = \left(1 + \frac{\eta}{d}\right)^{\Theta(d\log d /\eta)} b_{i}^{(T_1)}\),
it is easy to know that 
 for \( t \geq T_2 \), only when \( (x_{p}, y_p) \) and \( (x_n, y_n) \) contain the true feature \( j \) and its corresponding spurious feature \(j'\), the indicator functions remain consistently activated for \( i \in S_{j, \text{sure}} \).

Consequently, using Eq~(\ref{eq:loss_L}), Eq~(\ref{eq:gradient_wi}), and Eq~(\ref{eq:gradient_vi}), the loss function \( L \) becomes convex with respect to \( w_i \) and \( v_i \) independently when \( (x_{p}, y_p) \) and \( (x_n, y_n) \) contain the true feature \( j \) and its corresponding spurious
feature \( j' \) .

At the end of Phase II, using Eq~\((\ref{eq:neuron_growth_condition_T2})\), we know that \( \|w_i^{(T_2)}\|_2 \geq \Omega(d) \). Consequently, we cannot only consider \(-\langle f^{(t)}(x_p), h^{(t)}(y_p) \rangle\), and the error term \(Err_t\) becomes non-negligible.

Specifically, based on Eq~(\ref{eq:loss_L}), it can be observed that the term \(-\langle f^{(t)}(x_p), h^{(t)}(y_p) \rangle\) is convex and \( l_{i,j,1} = \|x_p\|_2 \|y_p\|_2 = \Theta(1) \)-smooth. This ensures that the true features contribute consistently to the optimization process.

Additionally, \( L_{i,j,2} = \frac{\left( \langle f^{(t)}(x_n), h^{(t)}(y_p) \rangle \right)^2}{2\tau} \) is also convex, and we further establish its smoothness to provide a rigorous understanding of its behavior.

To analyze the \( l_{i,j,2} \)-smoothness, we aim to find an upper bound that satisfies:
\begin{equation}
\|\nabla_{w_i,v_i} L_2(w_{i,1}, v_{i,1}) - \nabla_{w_i,v_i} L_2(w_{i,2}, v_{i,2})\|_2 
\leq l_{i,j,2} \|(w_{i,1} - w_{i,2}, v_{i,1} - v_{i,2})\|_2.
\end{equation}

The gradient difference for \( w_i \) is given by:
\begin{equation}
\small
\begin{aligned}
\|\nabla_{w_i} L_{i,j,2}(w_{i,1}, v_{i,1}) - \nabla_{w_i} L_{i,j,2}(w_{i,2}, v_{i,2})\|_2 &=  \frac{\Big\| \left( x^\top W_1^\top V_1 y \right) x (v_{i,1} y)^\top - \left( x^\top W_2^\top V_2 y \right) x (v_{i,2} y)^\top \Big\|_2}{2\tau} \\
&\leq \frac{l_{w_i,1}}{2\tau} \|w_{i,1} - w_{i,2}\|_2 
+ \frac{l_{w_i,2}}{2\tau} \|v_{i,1} - v_{i,2}\|_2,
\end{aligned}
\end{equation}
where \( l_{w_i,1} = \|x_n\|_2^2 \|y_p\|_2^2 \|v_{i,1}\|_2 \|v_{i,2}\|_2 \leq O(d) \) and 
\( l_{w_i,2} = \|x_n\|_2^2 \|y_p\|_2^2 \big(\|v_{i,1}\|_2 \|w_{i,2}\|_2 + \|w_{i,1}\|_2 \|v_{i,1}\|_2\big) \leq O(d) \).

Similarly, the gradient difference for \( v_i \) is:
\begin{equation}
\begin{aligned}
\|\nabla_{v_i} L_{i,j,2}(w_{i,1}, v_{i,1}) - \nabla_{v_i} L_{i,j,2}(w_{i,2}, v_{i,2})\|_2 
&\leq \frac{l_{v_i,1}}{2\tau} \|w_{i,1} - w_{i,2}\|_2 
+ \frac{l_{v_i,2}}{2\tau} \|v_{i,1} - v_{i,2}\|_2,
\end{aligned}
\end{equation}
where \( l_{v_i,1} \leq O(d) \) and \( l_{v_i,2} \leq O(d) \).

Combining the results, we find:
\begin{equation}
l_{i,j,2} = \frac{\sqrt{l_{w_i,1}^2 + l_{w_i,2}^2 + l_{v_i,1}^2 + l_{v_i,2}^2}}{2\tau} \leq O(1).
\end{equation}
Thus, the total smoothness constant is:
\begin{equation}
l_{i,j} = l_{i,j,1} + l_{i,j,2} = \Theta(1).
\end{equation}

These results demonstrate that the loss function \( L \) remains convex and \(l_{i,j}\)-smooth for neurons \( (w_i, v_i) \) when \( (x_{p}, y_p) \) and \( (x_n, y_n) \) contain the true feature \( j \) and its corresponding spurious feature \(j'\) during Phase III of ITCP on Raw Data, ensuring their convergence to sparse solutions while maintaining consistency in their activation patterns.

We verify that the following inequality holds
\begin{equation}
\label{eq:loss_convexity_independent_T2}
\begin{aligned}
L_{j}(w_i^{(t+1)}, v_i^{(t+1)}) 
&\leq L_{j}(w_i^{(t)}, v_i^{(t)}) \\
&\quad + \left\langle \nabla L_{j}(w_i^{(t)}, v_i^{(t)}), 
\left(w_i^{(t+1)} - w_i^{(t)},\; v_i^{(t+1)} - v_i^{(t)}\right) \right\rangle \\
&\quad + \frac{l_{i,j}}{2} 
\left\|\left(w_i^{(t+1)} - w_i^{(t)},\; v_i^{(t+1)} - v_i^{(t)}\right)\right\|^2
\end{aligned}
\end{equation}

Let \(L=\max_{i\in m} (l_{i,j}/(2\tau))= \Theta(1)\) and \(\eta = \frac{1}{L}\) to ensure a monotonic decrease, plug Eq~(\ref{eq:image_encoder_update}) and Eq~(\ref{eq:text_encoder_update}) into Eq~(\ref{eq:loss_convexity_independent_T2}),  we have
\begin{equation}
L_{j}(w_i^{(t+1)}, v_i^{(t+1)}) \leq L_{j}(w_i^{(t)}, v_i^{(t)}) - \frac{\eta}{2} \|\nabla L_{j}(w_i^{(t)}, v_i^{(t)})\|^2.
\end{equation}

Under our data assumptions for \(S_w\) and conclusion in Eq~\((\ref{eq:alignment_strength_sure2})\) , we define \(w_i^* = \alpha_{i,j}^* \mathbf{M}_j + \alpha_{i,j'}^* \mathbf{M}_{j'}, 
v_i^* = \alpha_{i,j}^* \mathbf{H}_j + \alpha_{i,j'}^* \mathbf{H}_{j'}\). 
Thus, \(L_{j}(w_i^*, v_i^*)\) captures both the alignment with the true feature \(\mathbf{M}_j, \mathbf{H}_j\) and the spurious feature \(\mathbf{M}_{j'}, \mathbf{H}_{j'}\), representing the minimal loss achievable under the influence of both true and spurious features in the optimization process.
Using Eq~\((\ref{eq:neuron_growth_condition_T2})\), we know \(w_i^{(T_2)}=\Theta(d)\), so \(L_{j}(w_i^*, v_i^*)=-\Theta(d)\).

By the property of smoothness, we have
\begin{equation}
\|\nabla L_{j}(w_i^{(t)}, v_i^{(t)})\|_2^2 \geq \frac{2}{L} \left( L_{j}(w_i^{(t)}, v_i^{(t)}) - L_{j}(w_i^*, v_i^*) \right)
\end{equation}

Take the telescope sum of from \(T_2\) to \(T_3\), we have
\begin{equation}
\begin{aligned}
\frac{1}{T_3-T_2} \sum_{t=T_2}^{T_3} L_{j}(w_i^{(t)}, v_i^{(t)}) 
&{\leq} L_{j}(w_i^*, v_i^*) + \frac{L^2\Delta_0 }{T_3-T_2}  \\
&\overset{\diamondsuit }{\leq} L_{j}(w_i^*, v_i^*) + \Theta(1) 
\end{aligned}
\end{equation}
where \(\Delta_0=L_{j}(w_i^{(T_1)}, v_i^{(T_1)})-L_{j}(w_i^*, v_i^*)=\Theta(d)\).
In \(\diamondsuit\), we use  \(T_3-T_2=\Theta(d)\), and \(L=\Theta(1)\) . 
% In \(\heartsuit\), we use \(w_i^*=\alpha^*_{i,j}\mathbf{M}_j, V_i^*=\alpha^*_{i,j}\mathbf{H}_j\) and \(L_{C,j}(w_i^*, v_i^*)=\Theta(\frac{1}{d})\) if \( x_{p} \) contains the true feature \( \mathbf{M}_j \). 


Generalized to every \(j \in d\), the same convergence holds for all \(i \in S_{j, \text{sure}}\) when \((x_p,y_p)\) and \((x_n,y_n)\) contain feature \(j,j'\). 
For all \((x_p,y_p)\) and \((x_n,y_n)\) in \(S_w\), the following inequality holds:
\begin{equation}
\begin{aligned}
\frac{1}{T_3-T_2} \sum_{t=T_2}^{T_3} L(f^{(T_3)}, h^{(T_3)}) 
&\leq L(f^*, h^*) + \Theta(1).
\end{aligned}
\end{equation}

As a result, the relative difference is bounded by:
\begin{equation}
\frac{L(f^{(T_3)}, h^{(T_3)}) -L(f^*, h^*)}{\left | L(f^*, h^*) \right |} \le \Theta\left(\frac{1}{d}\right).
\end{equation}

\subsection{Summary}
\label{sec:summary_raw}
ITCP trained on raw data \(S\) undergoes Stages~\ref{appendix:itcp_raw_stage1}–\ref{appendix:itcp_raw_stage3}. After \(T = \Theta(d^2 \log d)\) SGD iterations with batch size \(B = \Omega(d)\) and learning rate \(\eta = O(1)\), the resulting weights \((\overline{\mathbf{W}}, \overline{\mathbf{V}})\) minimize the contrastive loss in Eq.~(\ref{eqn:contrastive}) up to a vanishing relative error:
\begin{equation}
\label{eq:summary_loss}
\frac{L(f_{\overline{\mathbf{W}}}, h_{\overline{\mathbf{V}}}) - L^*}{\left| L^* \right|} \le o(1).
\end{equation}

However, each neuron pair \((\bar{w}_i, \bar{v}_i)\) in \((\overline{\mathbf{W}}, \overline{\mathbf{V}})\), for \(i \in [m]\), predominantly encodes a mixture of features indexed by a subset \(N_i \subseteq [d]\), with \(|N_i| \ge 2\). Specifically, we have:
\begin{equation}
\begin{aligned}
\bar{w}_i &= \sum_{j \in N_i} \alpha_{i,j} \mathbf{M}_j 
+ \sum_{j \in [d] \setminus N_i} \beta_{i,j} \mathbf{M}_j 
+ \sum_{j \in [d_1] \setminus [d]} \gamma_{i,j} \mathbf{M}_j^\perp, \\
\bar{v}_i &= \sum_{j \in N_i} \alpha_{i,j} \mathbf{H}_j 
+ \sum_{j \in [d] \setminus N_i} \beta_{i,j} \mathbf{H}_j 
+ \sum_{j \in [d_1] \setminus [d]} \gamma_{i,j} \mathbf{H}_j^\perp,
\end{aligned}
\end{equation}
\normalsize
where \(\alpha_{i,j}^2 = \Theta( \|\bar{w}_i\|_2^2 + \|\bar{v}_i\|_2^2 )\), and the interference from other features is small: \(\beta_{i,j}/\alpha_{i,j} \le O(1/\sqrt{d})\), \(\gamma_{i,j}/\alpha_{i,j} \le O(1/\sqrt{d_1})\).

Moreover, for every spuriously correlated feature pair \((j, j')\) satisfying Assumption~\ref{assumption:low}, there exists at least an \(\Omega(1)\) many of neurons \(i \in [m]\) with \(N_i = \{j, j'\}\), indicating the prevalence of feature mixing due to data misalignment.






\section{Captioning}
\label{appendix:captioning}
In this stage, the model fine-tunes the pre-trained encoder parameters \( \mathbf{W} \) and \( \mathbf{V} \) to obtain the updated parameters \( \hat{\mathbf{W}} \) and \( \hat{\mathbf{V}} \) through Image-Text Contrastive Pre-training (ITCP) on raw data. 

Given an image-text pair \((x_p, y_p)\) in \( S_w \), the decoder generates synthetic captions \(\hat{y}_p = \hat{\mathbf{V}}^T \sigma(\hat{\mathbf{W}}x_p)\), where \(\sigma(\cdot)\) denotes the activation function. The Image-Grounded Text Decoder, initialized with \( \mathbf{W} \) and \( \mathbf{V} \) from the pre-trained encoders, is fine-tuned on \( S_h \) by minimizing the following loss function:
\begin{equation}
L_C = \mathbb{E}_{(x_p, y_p) \in S_h} \left[ \frac{1}{2} \left\| \mathbf{V}^T \sigma(\mathbf{W}x_p) - y_p \right\|_2^2 \right],
\end{equation}
where \(\| \cdot \|_2\) denotes the Euclidean norm. This fine-tuning process refines the model to generate captions that are more closely aligned with the target text data in \( S_h \).

During the captioning, we sample a batch of image-text pairs $S_h^{(t)} = \{(x_p, y_p)\}_{p=1}^B \subseteq S_h$.
We perform stochastic gradient descent on \(L_C\). At each iteration, we update as
\begin{equation}
w_i^{(t+1)} \leftarrow w_i^{(t)} - \eta \nabla_{w_i} L_C^{(t)}
\end{equation}
\begin{equation}
v_i^{(t+1)} \leftarrow v_i^{(t)} - \eta \nabla_{v_i} L_C^{(t)}
\end{equation}

At the beginning of this phase, we set the bias threshold as:
\begin{equation}
\label{eq:bias_threshold_T3}
b_{i}^{(0)} = \sqrt{\frac{\|w_i^{(T_2)}\|_2^2 -\|w_i^{(T_1)}\|_2^2}{2}} 
\end{equation}

During training, the bias threshold is iteratively updated as:
\begin{equation}
\label{eq:bias_update_rule1}
b_{i}^{(t+1)} = \left(1 + \frac{\eta}{d}\right) b_{i}^{(t)},
\end{equation}


The gradient of \(L_C\) with respect to \(w_i^{(t)}\), \(v_i^{(t)}\), \(\mathbf{W}\), and \(\mathbf{V}\) at iteration \(t\) is given by:
\begin{equation}
\label{eq:gradient_wi_T_C}
\nabla_{w_{i}^{(t)}} L_C 
= v_i^{(t)} (y_p - \mathbf{V}^T \mathbf{W} x_p) x_p^T
\cdot  \mathbf{1}_{\left|\left\langle w_{i}^{(t)}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}}  
\end{equation}
\begin{equation}
\label{eq:gradient_vi_T_C}
\nabla_{v_{i}^{(t)}} L_C  
= w_i^{(t)} x_p (y_p - \mathbf{V}^T \mathbf{W} x_p)^T 
\cdot  \mathbf{1}_{\left|\left\langle w_{i}^{(t)}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}}  
\end{equation}

The alignment can be described by the following update rule:
\begin{equation}
\label{eq:alignment_update_rule_w}
\begin{aligned}
\langle w_i^{(t+1)}, \mathbf{M}_j \rangle &= \langle w_i^{(t)}, \mathbf{M}_j \rangle - \langle \nabla_{w_i} L_C, \mathbf{M}_j \rangle \\
&= \langle w_i^{(t)}, \mathbf{M}_j \rangle + \eta \cdot \mathrm{tr}(v_i^{(t)\top} (y_p-\mathbf{V}^T\mathbf{W} x_p)x_p^T \mathbf{M}_j 
\cdot \mathbf{1}_{\left|\left\langle w_{i}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}} ) 
\end{aligned}
\end{equation}

\begin{equation}
\label{eq:alignment_update_rule_v}
\begin{aligned}
\langle v_i^{(t+1)}, \mathbf{H}_j \rangle &= \langle v_i^{(t)}, \mathbf{H}_j \rangle - \langle \nabla_{v_i} L_C, \mathbf{H}_j \rangle  \\
&= \langle v_i^{(t)}, \mathbf{H}_j \rangle + \eta \cdot \mathrm{tr}(w_i^{(t)\top} x_p(y_p-\mathbf{V}^T\mathbf{W} x_p)^T \mathbf{H}_j
\cdot \mathbf{1}_{\left|\left\langle w_{i}^{(t)}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}}
)
\end{aligned}
\end{equation}


% Similarly to Eq~(\ref{eq:alignment_update_matrix}), the updates for the components can be expressed concisely in matrix form.
\subsection{Alignment for \( i \in S_{j, \text{sure}} \)}

This section analyzes the alignment growth for neurons \( i \in S_{j, \text{sure}} \). Specifically, we show that when \( t \leq T_C \), the alignment with the true feature \( \mathbf{M}_j \) grows exponentially if \( x_p \) contains the true feature \( \mathbf{M}_j \). In contrast, the alignment with the spurious feature \( \mathbf{M}_{j'} \) exhibits negligible growth, even for neurons \( i \in S_{j, \text{sure}} \). Specially,

1. For the true feature \( \mathbf{M}_j \), based on the result in Eq~(\ref{eq:alignment_strength_sure2}) and the bias threshold in Eq~(\ref{eq:bias_threshold_T3}), the indicator functions are always activated. This ensures that the neuron can consistently increase its alignment in the direction of the true feature \( \mathbf{M}_j \).

2. For the spurious feature \( \mathbf{M}_{j'} \), based on the result in Eq~(\ref{eq:alignment_strength_sure2}) and the bias threshold in Eq~(\ref{eq:bias_threshold_T3}), the indicator functions remain non-activated. This prevents the neuron from increasing its alignment in the direction of the spurious feature \( \mathbf{M}_{j'} \).

The details of proof as follow:


Using Eq~(\ref{eq:alignment_pair_sum_T2}), we know 
\begin{equation}
\|w_i^{(T_2)}\|_2^2 -\|w_i^{(T_1)}\|_2^2 \ge
|\langle w_i^{(T_{2})}, \mathbf{M}_j \rangle|^2+ |\langle w_i^{(T_{2})}, \mathbf{M}_{j^{\prime}} \rangle|^2\ge \|w_i^{(T_2)}\|_2^2 -\|w_i^{(T_1)}\|_2^2 - o(\frac{\|w_i^{(T_1)}\|_2^2}{d} )
\end{equation}

Using Eq~(\ref{eq:aligned_feature_update_j}) and  Eq~(\ref{eq:aligned_feature_update_jj}), we have
\begin{equation}
\small
\label{eq:aligned_feature_update}
\begin{aligned}
\langle w_i^{(t)}, \mathbf{M}_{j} \rangle -\langle w_i^{(t)}, \mathbf{M}_{j'} \rangle &= 
\frac{(a+b-c)^t}{2} 
\left(\langle w_i^{(0)}, \mathbf{M}_{j} \rangle + \langle v_i^{(0)}, \mathbf{H}_{j} \rangle-\langle w_i^{(0)}, \mathbf{M}_{j'} \rangle - \langle v_i^{(0)}, \mathbf{H}_{j'} \rangle \right)+Err_t
\end{aligned}
\end{equation}


Using Eq~(\ref{eq p3}) and \(\left(a+b-c \right)^{T_1+T_2} \ge \Omega (d^2)\),  with high probability \(1- O(\frac{1}{\sqrt{d} } )\) we have,
\begin{equation}
|\langle w_i^{(T_{2})}, \mathbf{M}_j \rangle|^2- |\langle w_i^{(T_{2})}, \mathbf{M}_{j^{\prime}} \rangle|^2\ge \Omega  (\frac{\|w_i^{(T_1)}\|_2^2}{d} )    
\end{equation}


Therefore,  with high probability \(1- O(\frac{1}{\sqrt{d} } )\) we have 
\begin{equation}
\label{eq:alignment_strength_final_T2}
    |\langle w_i^{(T_{2})}, \mathbf{M}_j \rangle|^2>\frac{\|w_i^{(T_2)}\|_2^2 -\|w_i^{(T_1)}\|_2^2}{2}> |\langle w_i^{(T_{2})}, \mathbf{M}_{j'} \rangle|^2
\end{equation}

We set \( b_{i}^{(0)} = \sqrt{\frac{\|w_i^{(T_2)}\|_2^2 -\|w_i^{(T_1)}\|_2^2}{2}}  \), and using Eq~(\ref{eq:alignment_strength_final_T2}), so similarly to the proof of Eq~(\ref{eq:indicator_activation_T2})
we can prove:

1. For \( i \in S_{j, \text{sure}} \) and \(x_{p}\) contain the true feature \(\mathbf{M}_j\), with high probability \(1- O(\frac{1}{\sqrt{d} } )\) the indicator functions become consistently activated \(0 \le t \le T_C\)  such that:
\begin{equation}
\mathbf{1}_{\left|\left\langle w_{i}^{(t)}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}}  = 1
\end{equation}

2. For \( i \in S_{j, \text{sure}} \) and \(x_{p}\) contain the corresponding spurious aligned feature \(\mathbf{M}_{j'}\), with high probability \(1- O(\frac{1}{\sqrt{d} } )\) the indicator functions become consistently activated \(0 \le t \le T_C\) such that:
\begin{equation}
\label{eq:indicator_activation_condition_T3}
\mathbf{1}_{\left|\left\langle w_{i}^{(t)}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}}  = 0
\end{equation}

3. For \( i \notin S_{j, \text{pot}} \) and \(\mathbf{M}_j^\perp\) where \(j \in [d_1] \setminus [d]\), we have:
\begin{equation}
\mathbf{1}_{\left|\left\langle w_{i}^{(t)}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}}  = 0
\label{eq:indicator_noactivation_condition_T3}
\end{equation}


For the residual loss in Eq~(\ref{eq:alignment_update_rule_w}) and Eq~(\ref{eq:alignment_update_rule_v}), we bound the difference if \(\mathbf{1}_{\left|\left\langle w_{i}^{(t)}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}}  = 1\):
\begin{equation}
\label{eq:alignment_difference_bound}
\begin{aligned}
\mathbf{H}_j z_{x_p}^j z_{y_p}^j 
&\overset{\diamondsuit }{\geq} (y_p-\mathbf{V}^T\mathbf{W} x_p)x_p^T \mathbf{M}_j 
\cdot \mathbf{1}_{\left|\left\langle w_{i}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}} \\
&= (\mathbf{H}_j z_{x_p}^j z_{y_p}^j  - \sum_{i=1}^m \langle v_i, \mathbf{H}_j \rangle \langle w_i, \mathbf{M}_j \rangle \mathbf{H}_j z_{x_p}^j z_{y_p}^j)  \cdot \mathbf{1}_{\langle w_i, x_p \rangle \geq b} \\
&\overset{\heartsuit}{\geq} \mathbf{H}_j z_{x_p}^j z_{y_p}^j  - O(d^{\gamma c_0}) \langle v_i, \mathbf{H}_j \rangle \langle w_i, \mathbf{M}_j \rangle \mathbf{H}_j z_{x_p}^j z_{y_p}^j 
\end{aligned}
\end{equation}
In \(\diamondsuit\), we employ the approximation \(y_p x_p^\top \mathbf{M}_j \approx \mathbf{H}_j z_{x_p}^j z_{y_p}^j\), based on the observation that \(z_{x_p}^j z_{y_p}^{j'} \ll z_{x_p}^j z_{y_p}^j\) when \(j \neq j'\).
In \(\heartsuit\), we utilize Eq~(\ref{eq p1}). There are at most \(O(d^{\gamma c_0})\) neurons capable of learning \(\mathbf{M}_j\), which satisfy the condition \(\mathbf{1}_{\langle w_i, x_p \rangle \geq b}\).
% \textcolor{red}{\textbf{FIX THIS!!!}}

For \( i \in S_{j, \text{sure}} \) and for \(x_{p}\) contain \(\mathbf{M}_j\), using Eq~(\ref{eq:alignment_difference_bound}), Eq~(\ref{eq:alignment_update_rule_w}) and Eq~(\ref{eq:indicator_activation_condition_T3})
we have:
\begin{equation}
\begin{aligned}
\langle w_i^{(t+1)}, \mathbf{M}_j \rangle &\geq \langle w_i^{(t)}, \mathbf{M}_j \rangle + \eta \cdot \mathrm{tr} \left( v_i^{(t)} \cdot (1 - \alpha_t^2) \mathbf{H}_j \mathbb{E}\left[ z_{x_{p}}^j z_{y_{p}}^j \right] \right) \\
&\geq \langle w_i^{(t)}, \mathbf{M}_j \rangle + \eta \frac{C_z (1 - \alpha_t^2)}{d} \langle v_i^{(t)}, \mathbf{H}_j \rangle,
\end{aligned}
\end{equation}

Similar to Eq~(\ref{eq:aligned_feature_update_j}), we have
\begin{equation}
|\langle w_i^{(t)}, \mathbf{M}_j \rangle| \geq \left(1 + \eta \frac{C_z \cdot (1 - \alpha_t^2)}{d} \right)^t 
\left( \frac{ \langle w_i^{(0)}, \mathbf{M}_j \rangle + \langle v_i^{(0)}, \mathbf{H}_{j} \rangle }{2} 
\right)
\end{equation}

Similarly, for \( i \in S_{j, \text{sure}} \) and \(x_{p}\) contain the corresponding spurious aligned feature \(\mathbf{M}_{j'}\), because \(\Pr[\mathbf{1}_{\left|\left\langle w_{i}^{(t)}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}}  = 0]\ge1- O(\frac{1}{\sqrt{d} } )\),
we have
\begin{equation}
\begin{aligned}
\langle w_i^{(t+1)}, \mathbf{M}_{j^{\prime}} \rangle \leq \langle w_i^{(t)}, \mathbf{M}_{j^{\prime}} \rangle +  O(\frac{\eta}{d^{1.5}}) \langle v_i^{(t)}, \mathbf{H}_{j^{\prime}} \rangle
\end{aligned}
\end{equation}
and 
\begin{equation}
|\langle w_i^{(t)}, \mathbf{M}_{j^{\prime}} \rangle| \leq \left(1 + O(\frac{\eta}{d^{1.5}}) \right)^t 
\left( \frac{ \langle w_i^{(T_2)}, \mathbf{M}_{j^{\prime}} \rangle + \langle v_i^{(T_2)}, \mathbf{H}_{j^{\prime}} \rangle }{2} 
\right)
\end{equation}

At \( T_C = \Theta\left(\frac{d \log(d)}{\eta}\right) \), we have:
\begin{equation}
\frac{|\langle w_i^{(T_C)}, \mathbf{M}_{j} \rangle|}{|\langle w_i^{(T_C)}, \mathbf{M}_{j^{\prime}} \rangle|} >  \frac{\left(1 + \eta \frac{C_z \cdot (1 - \alpha_t^2)}{d} \right)^{T_C}}{\left(1 + O(\frac{\eta}{d^{1.5}}) \right)^{T_C} } \ge \Omega(d)
\end{equation}

Therefore, we summarize that when \( t = T_C \), the alignment with the true feature \( \mathbf{M}_j \) dominates, satisfying:
\begin{equation}
\label{eq:alignment_dominance_T3}
\frac{|\langle w_i^{(T_C)}, \mathbf{M}_{j} \rangle|}{|\langle w_i^{(T_C)}, \mathbf{M}_{j^{\prime}} \rangle|} \geq \Omega(d),
\end{equation}
highlighting the significant separation between the true feature \( \mathbf{M}_j \) and the spurious feature \( \mathbf{M}_{j'} \) for neurons \( i \in S_{j, \text{sure}} \).  
A similar result holds for \( v_i \), where the alignment with the true feature \( \mathbf{H}_j \) similarly dominates over the spurious feature \( \mathbf{H}_{j'} \).


\subsection{Convergence}
For \( i \in S_{j, \text{sure}} \), when \( x_{p}, y_p \) contains the true feature \(j\), the indicator functions remain consistently activated. Consequently, the loss function \( L_C \) becomes convex with respect to \( w_i \) and \( v_i \) independently. We verify that the following inequality holds 
\begin{equation}
\label{eq:loss_convexity_independent_T3}
\begin{aligned}
L_{C,j}(w_i^{(t+1)}, v_i^{(t+1)}) 
&\leq L_{C,j}(w_i^{(t)}, v_i^{(t)}) \\
&\quad + \left\langle \nabla L_{C,j}(w_i^{(t)}, v_i^{(t)}), 
\big(w_i^{(t+1)} - w_i^{(t)}, v_i^{(t+1)} - v_i^{(t)}\big) \right\rangle \\
&\quad + \frac{l_i}{2} \left\| 
\big(w_i^{(t+1)} - w_i^{(t)}, v_i^{(t+1)} - v_i^{(t)}\big) 
\right\|^2
\end{aligned}
\end{equation}
where \(
l_i = O(C_zd^{2\gamma c_0}) (\left \| v_i^{(t)} \right \|_2^2 \left \| x_p \right \|_2^2 + \left \| v_i^{(t)} \right \|_2^2 \left \| x_p \right \|_2^2)=\Theta(1)
\).
This means \(L_{C,j}(w_i^{(t)}, v_i^{(t)}) \) is \(l_i\)-smooth for all \( i \in S_{j, \text{sure}} \) when \( x_{p}, y_p \) contains the true feature \(j\). Let \(L=\max_{i\in m} (l_i)=\Theta(1)\)

Let \(\eta = \frac{1}{L}\) to ensure a monotonic decrease, plug Eq~(\ref{eq:gradient_wi_T_C}) and Eq~(\ref{eq:gradient_vi_T_C}) into Eq~(\ref{eq:loss_convexity_independent_T3}),  we have
\begin{equation}
L_{C,j}(w_i^{(t+1)}, v_i^{(t+1)}) \leq L_{C,j}(w_i^{(t)}, v_i^{(t)}) - \frac{\eta}{2} \|\nabla L_{C,j}(w_i^{(t)}, v_i^{(t)})\|^2.
\end{equation}

By the property of smoothness, we have
\begin{equation}
\|\nabla L_{C,j}(w_i^{(t)}, v_i^{(t)})\|_2^2 \geq \frac{2}{L} \left( L_{C,j}(w_i^{(t)}, v_i^{(t)}) - L_{C,j}(w_i^*, v_i^*) \right).
\end{equation}

Take the telescope sum of from \(0\) to \(T_C\), we have
\begin{equation}
\begin{aligned}
\frac{1}{T_C} \sum_{t=0}^{T_C} L_{C,j}(w_i^{(t)}, v_i^{(t)}) 
&{\leq} L_{C,j}(w_i^*, v_i^*) + \frac{L^2\Delta_0 }{T_C}  \\
&\overset{\diamondsuit }{\leq} L_{C,j}(w_i^*, v_i^*) + \Theta(\frac{1}{d}) \\
&\overset{\heartsuit  }{=} \Theta(\frac{1}{d})
\end{aligned}
\end{equation}



% \begin{equation}
% \left \| \nabla_{w_{i}} L_{C,j}  \right \|_2
% = \left \| v_i^{(t)} (y_p - \mathbf{V}^T \mathbf{W} x_p) x_p^T
% \cdot  \mathbf{1}_{\left|\left\langle w_{i}^{(t)}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}}  \right \| _2
% \le \left \| v_i^{(t)} \right \|_2   \left \| y_p \right \|_2     \left \| x_p \right \|_2=L_{w_i}=\Theta (1) 
% \end{equation}
% Similarly, \(\left \| \nabla_{v_{i}} L_{C,j}  \right \|_2=L=\Theta (1) \).

% \begin{equation}
% \begin{aligned}
% \|w_i^{(t)} - w_i^*\|_2^2 - \|w_i^{(t+1)} - w_i^*\|_2^2
% &= 2\eta \left\langle \nabla_{w_i} L_{C,j}(w_i^{(t)}, v_i^{(t)}), w_i^{(t)} - w_i^* \right\rangle 
% - \eta^2 \|\nabla_{w_i} L_{C,j}(w_i^{(t)}, v_i^{(t)})\|_2^2 \\
% &\geq 2\eta \left( L_{C,j}(w_i^{(t)}, v_i^{(t)}) - L_{C,j}(w_i^*, v_i^{(t)}) \right)  - \eta^2 L^2 \\
% &\geq 2\eta \left( L_{C,j}(w_i^{(t)}, v_i^{(t)}) - L_{C,j}(w_i^*, v_i^*) \right)  - \eta^2 L^2 
% \end{aligned}
% \label{eq:gradient_descent_wi}
% \end{equation}

% \begin{equation}
% \begin{aligned}
% \|v_i^{(t)} - v_i^*\|_2^2 - \|v_i^{(t+1)} - v_i^*\|_2^2
% &= 2\eta \left\langle \nabla_{v_i} L_{C,j}(v_i^{(t)}, w_i^{(t)}), v_i^{(t)} - v_i^* \right\rangle 
% - \eta^2 \|\nabla_{v_i} L_{C,j}(v_i^{(t)}, w_i^{(t)})\|_2^2 \\
% &\geq 2\eta \left( L_{C,j}(v_i^{(t)}, w_i^{(t)}) - L_{C,j}(v_i^*, w_i^{(t)}) \right)  - \eta^2 L^2 \\
% &\geq 2\eta \left( L_{C,j}(v_i^{(t)}, w_i^{(t)}) - L_{C,j}(v_i^*, w_i^*) \right)  - \eta^2 L^2 
% \end{aligned}
% \label{eq:gradient_descent_vi}
% \end{equation}

% Take the telescope sum of Eq~(\ref{eq:gradient_descent_wi}) and Eq~(\ref{eq:gradient_descent_vi}) from \(0\) to \(T_C\), we have that:
% \begin{equation}
% \begin{aligned}w_i^*=\alpha^*_{i,j}\mathbf{M}_j, V_i^*=\alpha^*_{i,j}\mathbf{H}_j
% \frac{1}{T_C} \sum_{t=0}^{T_C} L_{C,j}(w_i^{(t)}, v_i^{(t)}) 
% &{\leq} L_{C,j}(w_i^*, v_i^*) + \eta L^2 + \frac{\|w_i^{(t)} - w_i^*\|_2^2 - \|w_i^{(t+1)} - w_i^*\|_2^2+\|v_i^{(t)} - v_i^*\|_2^2 - \|v_i^{(t+1)} - v_i^*\|_2^2}{2\eta T_C}  \\
% &\overset{\diamondsuit }{\leq} L_{C,j}(w_i^*, v_i^*) + \Theta(\frac{1}{d}) \\
% &\overset{\heartsuit  }{=} \Theta(\frac{1}{d})
% \end{aligned}
% \end{equation}
where \(\Delta_0=L_{C,j}(w_i^{(0)}, v_i^{(0)})-L_{C,j}(w_i^*, v_i^*)\).
In \(\diamondsuit\), we use  \(T_C=\Theta(d)\), and \(\|w_i^{(t)} \|_2^2=\|v_i^{(t)} \|_2^2 =\Theta(1)\). 
In \(\heartsuit\), we use \(w_i^*=\alpha^*_{i,j}\mathbf{M}_j, V_i^*=\alpha^*_{i,j}\mathbf{H}_j\) and \(L_{C,j}(w_i^*, v_i^*)=\Theta(\frac{1}{d})\) if \( x_{p} \) contains the true feature \( \mathbf{M}_j \). 

Therefore, for all \(j \in d\) and all \((x_p,y_p) \in S_h\), when \(T_C=\Theta(d^2)\), we can ensure
\begin{equation}
L_C = \mathbb{E}_{(x_p, y_p) \in S_h} \left[ \frac{1}{2} \left\| \mathbf{V}^T \sigma(\mathbf{W}x_p) - y_p \right\|_2^2 \right]\le \Theta(\frac{1}{d})
\end{equation}








\subsection{Summary}
After \(T_C\) iterations, the parameters \(\mathbf{W}\) and \(\mathbf{V}\) are updated to \(\mathbf{W}^{T_C}= \hat{\mathbf{W}}\) and \(\mathbf{V}^{T_C}=\hat{\mathbf{V}}\), respectively, using the dataset \(S_h\). The generated caption is given by:
\begin{equation}
\hat{y}_p = \hat{\mathbf{V}}^T \sigma(\hat{\mathbf{W}}x_p),
\end{equation}
where the expected loss satisfies:
\begin{equation}
\label{loss_LC}
\mathbb{E} \left[ \frac{1}{2} \left\| \hat{y}_p - y_p \right\|_2^2 \right] = L_C \leq \Theta\left(\frac{1}{d}\right).
\end{equation}

1. For \(i \in S_{j, \text{sure}}\), the alignment strength satisfies:
\begin{equation}
|\langle w_i^{(T_C)}, \mathbf{M}_j \rangle|^2 = \Theta(1) \left \| w_i^{(T_C)}  \right \|_2^2 
\end{equation}
and 
\begin{equation}
|\langle w_i^{(T_C)}, \mathbf{M}_j' \rangle|^2 \le O(\frac{1}{d} ) \left \| w_i^{(T_C)}  \right \|_2^2 
\end{equation}
where \(j'\) represents the corresponding spurious alignment feature.

2. For \(i \notin S_{j, \text{pot}}\), the alignment strength satisfies:
\begin{equation}
\label{eq:alignment_strength_not_sure2}
|\langle w_i^{(T_1)}, \mathbf{M}_j \rangle|^2 \le O(\frac{1}{d} ) \left \| w_i^{(T_C)}  \right \|_2^2 
\end{equation}

3. For \(\mathbf{M}_j^\perp\) where \(j \in [d_1] \setminus [d]\), we have:
\begin{equation}
|\langle w_i^{(t+1)}, \mathbf{M}_j^\perp \rangle|^2 < O(\frac{1}{d_1} ) \left \| w_i^{(T_C)}  \right \|_2^2 
\end{equation}




\section{Filtering}
\label{appendix:filtering}
During filtering, we sample the synthetic image-text pair \((x_p, \hat{y}_p)\) in \(\hat{S}_w\) and the corresponding image-text pair \((x_p, y_p)\) in \(S_w\). The image encoder \(f\) and text encoder \(h\) trained on raw data are employed to obtain the corresponding embeddings. 
\begin{equation}
z_{x_p}^{\prime} = f(x_p), \quad \hat{z}_{y_p} = h(\hat{y_p}), \quad z_{y_p}^{\prime} = h(y_p)
\end{equation}
Then, we calculate the cosine similarity of $\langle z_{x_p}^{\prime}, \hat{z}_{y_p} \rangle$ and $\langle z_{x_p}^{\prime}, z_{y_p}^{\prime} \rangle$, and select the image-text pair with higher cosine similarity denoted as $(x,\tilde{y})$. In this way, we replace the noisy pairs in $S_w$ with synthetic pairs in $\hat{S_w}$.
The resulting dataset is denoted as $\tilde{S} = \tilde{S}_w \cup S_h$.

The decoder generates synthetic captions \(\hat{y}_p = \hat{\mathbf{V}}^T \sigma(\hat{\mathbf{W}}x_p)\). Using 
Eq~(\ref{loss_LC}), for each data pair \((x_p,y_p)\) which contain feature \((\mathbf{M}_j,\mathbf{H}_j)\) in \(S_h\) we have
\begin{equation}
\small
\mathbb{E}_{(x_p, y_p)}\left [ \mathbb{E}_{j\in d} \left[ \frac{1}{2} \left\| \mathbf{H}_j z_{\hat{y}_p}^j - \mathbf{H}_j z_{y_p}^j \right\|_2^2 \right] ||z_{y_p}^j|=1  \right ]  \leq \mathbb{E}_{(x_p, y_p)} \left[ \frac{1}{2} \left\| \hat{y}_p - y_p \right\|_2^2 | |z_{y_p}^j|=1\right]=L_C\le \Theta(\frac{1}{d} )
\end{equation}

Therefore, using \(\left \| \mathbf{H}_j \right \|_2 =1\) and \(z_{x_p}=z_{y_p}\) in \(S_h\), we have
\begin{equation}
\mathbb{E}_{x_p,j\in d} \left[ z_{\hat{y}_p}^j z_{x_p}^{j} | |z_{x_p}^j|=1\right]  \ge 1-\Theta(\frac{1}{d})
\end{equation}

Base on Assumption~\ref{assumption:data}  \(z_{x_p}^j \sim \text{Bernoulli}\left(\frac{C_z}{d}\right)\), we have
\begin{equation}
\label{eq:conditional_probability_true_feature_T_C}
\Pr(z_{\hat{y}_p}^{j} = 1 \mid |z_{x_p}^j| = 1)\ge1-\Theta(\frac{1}{d})
\end{equation}

Using Eq~(\ref{eq:alignment_dominance_T3}) and Eq~(\ref{eq:conditional_probability_true_feature_T_C}), we have
\begin{equation}
    \Pr(z_{\hat{y}_p}^{j'} = 1 \mid |z_{x_p}^j| = 1)\le\Theta(\frac{1}{d})
\end{equation}

Therefore, after replace all noisy text \(y_p\) in \(S_w\) by synthetic caption \(\hat{y}_p\) in $\hat{S_w}$

1. for a positive pair \((x_p, y_p)\), we have
\begin{equation}
\mathbb{E}\left[ z_{\tilde{x}_p}^j z_{\tilde{y}_p}^j \right] = \Theta(\frac{1}{d} ) , \quad \mathbb{E}\left[ z_{\tilde{x}_p}^j z_{\tilde{y}_p}^{j'} \right] = \Theta\left(\frac{1}{d^2}\right), \quad \forall j' \neq j.
\label{eq:positive_pair_clean}
\end{equation}
2. for negative pairs \((x_p, y_q)\), where \(p \neq q\), we have:
\begin{equation}
\mathbb{E}\left[ z_{x_p}^j z_{y_q}^{j'} \right] = \Theta\left(\frac{1}{d^2}\right), \quad \forall j, j' \in [d].
\end{equation}



\section{ITCP on Synthetic (Recaptioned) Data}
\label{appendix:itcp_synthetic}
During ITCP on Raw Data, we use a noisy dataset \( S \). Based on SubSection~\ref{subsec:feature_coupling_expected_values}, we have \( \mathbb{E}\left[ z_x^j z_y^j \right] \) and \( \mathbb{E}\left[ z_x^j z_y^{j'} \right] \) both in \( \Theta\left(\frac{1}{d}\right) \). In this scenario, for \( i \in S_{j, \text{sure}} \), \( w_i^{(t)} \) is jointly influenced by \( \mathbf{M}_j \) and \( \mathbf{M}_{j'} \), with both features contributing comparably to the updates.
However, during ITCP on  recaptioned data, we sample image-text pairs from the  dataset \( \tilde{S} \). Using Eq.~(\ref{eq:positive_pair_clean}), we find that 
\(\mathbb{E}\left[ z_{\tilde{x}_p}^j z_{\tilde{y}_p}^{j'} \right] = \Theta\left(\frac{1}{d^2}\right)\). In this case, for \( i \in S_{j, \text{sure}} \), \( w_i^{(t)} \) is influenced solely by \( \mathbf{M}_j \), without interference from spurious features, ensuring purified representations. 

The only difference between ITCP on Raw Data and  Data lies in the \(\mathbb{E}\left[ z_{\tilde{x}_p}^j z_{\tilde{y}_p}^{j'} \right]\); all other training processes remain largely the same. Therefore, we simplify our proof accordingly.


\subsection{Phase I  of ITCP on Synthetic Data}

The Phase I of ITCP on  Data is defined as the training iterations \( t \leq T_1 \), where \(T_1 = \Theta\left(\frac{d \log d}{\eta  }\right)\)
is the iteration when all \(\|w_i^{(T_2)}\|_2^2 = 2\|w_i^{(0)}\|_2^2\).
Before \(T_1\), we set $b_{i}^{(t)}=0$. 
For every neuron \( i \in [m] \), the weights \( w_i \), \( v_i \) will mostly ignore the noise features \( \mathbf{M}^\perp \), \( \mathbf{H}^\perp \) and learn to emphasize the features \( \mathbf{M} \), \( \mathbf{H} \).

If \( \Pr(|z_{y_p }^j |= 1 \mid |z_{x_p}^{j'}| = 1) < 0.1 \), we have \( \mathbb{E}\left[ z_x^j z_y^j \right] \gg \mathbb{E}\left[ z_x^j z_y^{j'} \right] \) and \( (a+b+c)^t \approx (a+b-c)^t \). In this case, \( w_i^{(t+1)} \) is predominantly influenced by \(\mathbf{M}_j\), with minimal contributions from \(\mathbf{M}_{j'}\). The updates are thus primarily driven by the single feature \(\mathbf{M}_j\), ensuring that spurious interactions from \(\mathbf{M}_{j'}\) are negligible.

\begin{equation}
\begin{aligned}
\|\mathbf{MM}^\top w_i^{(t)}\|_2^2 &= 
\sum_{i=1}^d 
\left[ 
\frac{(a+b+c)^t}{2} 
\left( 
\langle w_i^{(t)}, \mathbf{M}_j \rangle + \langle v_i^{(t)}, \mathbf{H}_{j} \rangle 
\right) 
\right]^2 \\
&= 
\left( 
1 + \frac{\eta C_z  }{d}   
\right)^{2t} 
\frac{\|\mathbf{MM}^\top w_i^{(0)}\|_2^2 + \|\mathbf{HH}^\top v_i^{(0)}\|_2^2}{4}.
\end{aligned}
\end{equation}


\( i \in S_{j, \text{sure}} \): 
\begin{equation}
\begin{aligned}
|\langle w_i^{(T_1)}, \mathbf{M}_j \rangle|^2 &= \left(1   + \eta \frac{C_z  }{d} \right)^{2T_1} 
\left( 
 \frac{ \langle w_i^{(0)}, \mathbf{M}_j \rangle + \langle v_i^{(0)}, \mathbf{H}_j \rangle}{2} 
\right)^{2}  \\
&\geq \left(1   + \eta \frac{C_z  }{d} \right)^{2T_1} \cdot \frac{c_1 \log d}{d} 
\cdot \frac{\|\mathbf{MM}^\top w_i^{(0)}\|_2^2 + \|\mathbf{HH}^\top v_i^{(0)}\|_2^2}{4} \\
&= \frac{c_1 \log d}{d} 
\cdot \frac{\|\mathbf{MM}^\top w_i^{(T_1)}\|_2^2 + \|\mathbf{HH}^\top v_i^{(T_1)}\|_2^2}{2}\\
&\geq \frac{c_1 \log d}{d} \cdot \frac{\|w_i^{(T_1)}\|_2^2+\|v_i^{(T_1)}\|_2^2 -\|w_i^{(0)}\|_2^2-\|v_i^{(0)}\|_2^2}{2} \\
&\geq \frac{(1+c_0) \log d}{d} \cdot 
\frac{\|w_i^{(T_1)}\|_2^2+\|v_i^{(T_1)}\|_2^2}{2} 
\end{aligned}
\end{equation}
Because
\(
\frac{\|w_i^{(T_1)}\|_2^2+\|v_i^{(T_1)}\|_2^2}{2}  = \|w_i^{(0)}\|_2^2+\|v_i^{(0)}\|_2^2
\) and 
\(c_1 > 2(1+c_0)\)


\( i \notin S_{j, \text{sure}} \): 
\begin{equation}
\begin{aligned}
|\langle w_i^{(T_1)}, \mathbf{M}_j \rangle|^2 &= \left(1   + \eta \frac{C_z  }{d} \right)^{2T_1}
\left( 
 \frac{ \langle w_i^{(0)}, \mathbf{M}_j \rangle  + \langle v_i^{(0)}, \mathbf{H}_j \rangle}{2} 
\right)^2  \\
&\leq \left(1   + \eta \frac{C_z  }{d} \right)^{2T_1} \cdot \frac{c_2 \log d}{d} 
\cdot \frac{\|\mathbf{MM}^\top w_i^{(0)}\|_2^2 + \|\mathbf{HH}^\top v_i^{(0)}\|_2^2}{4} \\
&= \frac{c_2 \log d}{d} 
\cdot \frac{\|\mathbf{MM}^\top w_i^{(T_1)}\|_2^2 + \|\mathbf{HH}^\top v_i^{(T_1)}\|_2^2}{2}\\
&\leq \frac{\log d}{d} \cdot 
\frac{\|w_i^{(T_1)}\|_2^2+\|v_i^{(T_1)}\|_2^2}{2} 
\end{aligned}
\end{equation}

\(
|\langle w_i^{(t+1)}, \mathbf{M_j^\perp} \rangle|^2 \leq O(\frac{1}{d_1} )\frac{\|w_i^{(T_1)}\|_2^2+\|v_i^{(T_1)}\|_2^2}{2} 
\)


\subsection{Phase II:}
The Phase II of ITCP on Synthetic Data is defined as the training iterations \( T_1\leq t \leq T_2 \), where \(T_2-T_1 = \Theta\left(\frac{d \log d}{\eta  }\right)\)
is the iteration.

We set \( b_{i}^{(t)}=\sqrt{\frac{\log d}{d} \cdot \frac{\|w_i^{(T_1)}\|_2^2+\|v_i^{(T_1)}\|_2^2}{2} }
 \) and \( b_{i}^{(t+1)}=(1+\frac{\eta }{d}) b_{i}^{(t)}
 \) until all \(\|\|w_i^{(T_2)}\|_2 \geq \Omega(d) \|w_i^{(T_1)}\|_2,
\). In this phase, the weights \( (w_i, v_i) \) will mostly ignore the features \( \mathbf{M}_j \), \( \mathbf{H}_j \) if \( i \notin S_{j, \text{sure}} \) and  the noise features \( \mathbf{M}^\perp \), \( \mathbf{H}^\perp \),  and learn to emphasize the features \( \mathbf{M}_j \), \( \mathbf{H}_j \) if \( i \in S_{j, \text{sure}} \). 


% For \( i \in S_{j, \text{sure}} \), the projection of weights onto a generic feature \(\xi\) at iteration \( T_1 \) satisfies using lemma \ref{eq wxi}:
% \begin{equation}
% \left| \langle w_i^{(T_1)}, \xi \rangle \right|^2 
% \leq o\left(\frac{\left \| w_i^{(T_1)} \right \|_2^2 }{d} \right) < b_{i}^{(T_1)}.
% \end{equation}

For \( i \in S_{j, \text{sure}} \),  using Lemma~\ref{lemma:Noise Projection Bound}, the following holds with high probability \(1 - e^{-\Omega(d_1)}\) when 
\( T_1<t\le T_2\) :
\begin{equation}
\label{noise1}
\left| \langle w_i^{(t)}, \xi \rangle \right|^2 
\leq O\left(\frac{\left \| w_i^{(t)} \right \|_2^2 }{d^{1+c_0}} \right)<b_{i}^{(t)}
\end{equation}


Under the assumption that, with high probability, the indicator function satisfies the condition when \(t = T_1\):
\begin{equation}
\mathbf{1}_{\left|\left\langle w_{i}^{(t)}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}}  
\cdot   \mathbf{1}_{\left|\left\langle v_{i}^{(t)}, y_{p}\right\rangle\right| \geq b_{i}^{(t)}} = 1,
\end{equation}
we can ensure that:
\begin{equation}
\mathbb{E} \left[ z_x^j z_y^j \cdot  \mathbf{1}_{\left|\left\langle w_{i}^{(t)}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}}  
\cdot  \mathbf{1}_{\left|\left\langle v_{i}^{(t)}, y_{p}\right\rangle\right| \geq b_{i}^{(t)}} \right] = \frac{C_z  }{d}.
\end{equation}

The weight dynamics for \(|\langle w_i^{(t+1)}, \mathbf{M}_j \rangle|\) can be expressed as:
\begin{equation}
\begin{aligned}
|\langle w_i^{(t+1)}, \mathbf{M}_j \rangle| &= \left(1   + \eta \frac{C_z}{d} \right)
\left( 
 \frac{\langle w_i^{(t)}, \mathbf{M}_j \rangle +  \langle v_i^{(t)}, \mathbf{H}_j \rangle}{2} 
\right).
\end{aligned}
\end{equation}

Given that \(\left(1   + \eta \frac{C_z  }{d} \right) > \left(1 + \frac{\eta}{d}\right)\), and 
\(
\frac{\langle w_i^{(t)}, \mathbf{M}_j \rangle + \langle v_i^{(t)}, \mathbf{H}_j \rangle}{2} > b_{i}^{(t)},
\)
it follows that:
\begin{equation}
|\langle w_i^{(t+1)}, \mathbf{M}_j \rangle| > b_{i}^{(t+1)}.
\end{equation}

Thus, with high probability, for \(t \le T_2\), we have:
\begin{equation}
\mathbf{1}_{\left|\left\langle w_{i}^{(t)}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}} 
\cdot 
\mathbf{1}_{\left|\left\langle v_{i}^{(t)}, y_{p}\right\rangle\right| \geq b_{i}^{(t)}} = 1.
\end{equation}

so for \( T_1< t \leq T_2 \) we have
\begin{equation}
|\langle w_i^{(t+1)}, \mathbf{M}_j \rangle| = \left(1   + \eta \frac{C_z  }{d} \right)^t 
\left( \frac{ \langle w_i^{(T_1)}, \mathbf{M}_j \rangle + \langle v_i^{(T_1)}, \mathbf{H}_j \rangle}{2} 
\right)
\end{equation}




For \( i \notin S_{j, \text{sure}} \), the projection of weights onto a generic feature \(\xi\) at iteration \( T_1 \) satisfies:
\begin{equation}
\Pr\left(
\mathbf{1}_{\left|\left\langle w_{i}^{(t)}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}} 
\cdot 
\mathbf{1}_{\left|\left\langle v_{i}^{(t)}, y_{p}\right\rangle\right| \geq b_{i}^{(t)}} 
= 1
\right) \leq o\left(\frac{1}{d}\right).
\end{equation}

We can ensure that:
\begin{equation}
\mathbb{E} \left[ z_x^j z_y^j \cdot  \mathbf{1}_{\left|\left\langle w_{i}^{(t)}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}}  
\cdot  \mathbf{1}_{\left|\left\langle v_{i}^{(t)}, y_{p}\right\rangle\right| \geq b_{i}^{(t)}} \right] = o\left(\frac{1}{d^2} \right).
\end{equation}

The weight dynamics for \(|\langle w_i^{(t+1)}, \mathbf{M}_j \rangle|\) can now be expressed as:
\begin{equation}
\begin{aligned}
|\langle w_i^{(t+1)}, \mathbf{M}_j \rangle| &= \left(1   + o\left(\frac{\eta}{d^2}\right) \right)
\left( 
 \frac{ \langle w_i^{(t)}, \mathbf{M}_j \rangle + \langle v_i^{(t)}, \mathbf{H}_j \rangle}{2} 
\right).
\end{aligned}
\end{equation}

Given that \(\left(1   + o\left(\frac{\eta}{d^2}\right) \right) < \left(1 + \frac{\eta}{d}\right)\), and 
 \(
\frac{\langle w_i^{(t)}, \mathbf{M}_j \rangle +  \langle v_i^{(t)}, \mathbf{H}_j \rangle}{2} < b_{i}^{(t)},
\)
it follows that:
\begin{equation}
|\langle w_i^{(t+1)}, \mathbf{M}_j \rangle| < b_{i}^{(t+1)}.
\end{equation}

If \(|\langle w_i^{(T_1)}, \mathbf{M}_j \rangle| < b_{i}^{(T_1)}\), then \(|\langle w_i^{(t)}, \mathbf{M}_j \rangle| < b_{i}^{(t)}\) for \(t \le T_2\). Thus, with high probability, for \(t \le T_2\), we have:
\begin{equation}
\mathbf{1}_{\left|\left\langle w_{i}^{(t)}, x_{p}\right\rangle\right| \geq b_{i}^{(t)}} 
\cdot 
\mathbf{1}_{\left|\left\langle v_{i}^{(t)}, y_{p}\right\rangle\right| \geq b_{i}^{(t)}} = 0.
\end{equation}


\begin{equation}
|\langle w_i^{(t+1)}, \mathbf{M}_j \rangle| \leq \left(1   + o\left(\frac{\eta}{d^2}\right) \right)^t 
\left( \frac{ \langle w_i^{(T_1)}, \mathbf{M}_j \rangle + \langle v_i^{(T_1)}, \mathbf{H}_j \rangle}{2} 
\right)
\end{equation}


There exists \( T_2 = \Theta\left(\frac{d \log d}{\eta  }\right) \) such that the following conditions hold:
\begin{equation}
\left(1   + \eta \frac{C_z  }{d} \right)^{T_2} = \Theta(d),
\end{equation}
indicating that \(|\langle w_i^{(t+1)}, \mathbf{M}_j \rangle|\) for \( i \in S_{j, \text{sure}} \) increase iteratively until:
\begin{equation}
\|w_i^{(T_2)}\|_2 \geq \Omega(d) \|w_i^{(T_1)}\|_2
\end{equation}
while, for \( i \notin S_{j, \text{sure}} \), the updates diminish, such that:
\begin{equation}
\left(1   + o\left(\frac{\eta}{d^2}\right) \right)^{T_2} \leq 1 + o\left(\frac{1}{d}\right),
\end{equation}
indicating negligible growth in \(|\langle w_i^{(t+1)}, \mathbf{M}_j \rangle|\).




Thus we have
\begin{equation}
\begin{aligned}
|\langle w_i^{(T_{2})}, \mathbf{M}_j \rangle|^2
&= \|w_i^{(T_{2})}\|_2^2 - \sum_{j \in [d], j \notin \mathcal{N}_i} \langle w_i^{(T_{2})}, \mathbf{M}_j \rangle^2 - \sum_{j \in [d_1] \setminus [d]} \langle w_i^{(T_{2})}, \mathbf{M}_j^\perp \rangle^2 \\
&\ge \|w_i^{(T_2)}\|_2^2 - (1 + o(1))\|w_i^{(T_1)}\|_2^2-(1 + o(1))\|w_i^{(0)}\|_2^2 \\
&\ge (1-o(1))  \|w_i^{(T_2)}\|_2^2.
\end{aligned}
\end{equation}

Finally, for \( i \notin S_{j, \text{sure}} \), we have:
\begin{equation}
\|w_i^{(T_2)}, \mathbf{M}_j\|_2 
\leq (1 + o(\frac{1}{d})) \cdot O\left(\frac{\|w_i^{(T_1)}\|_2}{\sqrt{d}}\right)
\leq O\left(\frac{\|w_i^{(T_2)}\|_2}{\sqrt{d}}\right),
\end{equation}
and for noise components:
\begin{equation}
|\langle w_i^{(T_2)}, \mathbf{M}_{j}^\perp \rangle|_2 \leq O\left(\frac{\|w_i^{(T_2)}\|_2}{\sqrt{d_1}}\right).
\end{equation}

We summarize the results when \(T_1 < t\le T_2\) as follows:

1. For \(i \in S_{j, \text{sure}}\), the alignment strength satisfies:
\begin{equation}
\label{eq:re_alignment_strength_sure2}
|\langle w_i^{(T_2)}, \mathbf{M}_j \rangle|^2 > (1-o(1))\frac{\|w_i^{(T_2)}\|_2^2 + \|v_i^{(T_2)}\|_2^2}{2}
\end{equation}
without \(j'\) that represents the corresponding spurious alignment feature.

2. For \(i \notin S_{j, \text{pot}}\), the alignment strength satisfies:
\begin{equation}
\label{eq:re_alignment_strength_not_sure2}
|\langle w_i^{(T_1)}, \mathbf{M}_j \rangle|^2 \le O(\frac{1}{d} ) \cdot \frac{\|w_i^{(T_2)}\|_2^2 + \|v_i^{(T_2)}\|_2^2}{2}
\end{equation}

3. For \(\mathbf{M}_j^\perp\) where \(j \in [d_1] \setminus [d]\), we have:
\begin{equation}
|\langle w_i^{(t+1)}, \mathbf{M}_j^\perp \rangle|^2 < O\left(\frac{1}{d_1}\right) \cdot \frac{\|w_i^{(T_2)}\|_2^2 + \|v_i^{(T_2)}\|_2^2}{2}.
\end{equation}
Similar results also hold for \(v_i\).


\subsection{Phase III Convergence of ITCP on Synthetic Data}
Similarly to convergence Phase III in ITCP on Raw Data when \(T_2 \leq t \leq T_3\), using Eq~(\ref{eq:loss_L}), Eq~(\ref{eq:gradient_wi}), and Eq~(\ref{eq:gradient_vi}), the loss function \( L \) becomes convex with respect to \( w_i \) and \( v_i \) independently when \( (x_{p}, y_p) \) and \( (x_n, y_n) \) contain the true feature \( j \). 

We verify that the following inequality holds
\begin{equation}
\label{eq:loss_convexity_independent_T2}
\begin{aligned}
L_{j}(w_i^{(t+1)}, v_i^{(t+1)}) 
&\leq L_{j}(w_i^{(t)}, v_i^{(t)}) \\
&\quad + \left\langle \nabla L_{j}(w_i^{(t)}, v_i^{(t)}), 
\big(w_i^{(t+1)} - w_i^{(t)},\ v_i^{(t+1)} - v_i^{(t)}\big) \right\rangle \\
&\quad + \frac{l_{i,j}}{2} \left\| 
\big(w_i^{(t+1)} - w_i^{(t)},\ v_i^{(t+1)} - v_i^{(t)}\big) 
\right\|^2
\end{aligned}
\end{equation}

Let \(L=\max_{i\in m} (l_{i,j}/(2\tau))= \Theta(1)\) and \(\eta = \frac{1}{L}\) to ensure a monotonic decrease, plug Eq~(\ref{eq:image_encoder_update}) and Eq~(\ref{eq:text_encoder_update}) into Eq~(\ref{eq:loss_convexity_independent_T2}),  we have
\begin{equation}
L_{j}(w_i^{(t+1)}, v_i^{(t+1)}) \leq L_{j}(w_i^{(t)}, v_i^{(t)}) - \frac{\eta}{2} \|\nabla L_{j}(w_i^{(t)}, v_i^{(t)})\|^2.
\end{equation}

Under our data assumptions for \(S_w\) and conclusion in Eq~\((\ref{eq:alignment_strength_sure2})\) , we define \(w_i^* = \alpha_{i,j}^* \mathbf{M}_j, 
v_i^* = \alpha_{i,j}^* \mathbf{H}_j\). 
Thus, \(L_{j}(w_i^*, v_i^*)\) captures both the alignment with the true feature \(\mathbf{M}_j, \mathbf{H}_j\) and the spurious feature \(\mathbf{M}_{j'}, \mathbf{H}_{j'}\), representing the minimal loss achievable under the influence of both true and spurious features in the optimization process.
Using Eq~\((\ref{eq:neuron_growth_condition_T2})\), we know \(w_i^{(T_2)}=\Theta(d)\), so \(L_{j}(w_i^*, v_i^*)=-\Theta(d)\).

By the property of smoothness, we have
\begin{equation}
\|\nabla L_{j}(w_i^{(t)}, v_i^{(t)})\|_2^2 \geq \frac{2}{L} \left( L_{j}(w_i^{(t)}, v_i^{(t)}) - L_{j}(w_i^*, v_i^*) \right)
\end{equation}

Take the telescope sum of from \(T_2\) to \(T_3\), we have
\begin{equation}
\begin{aligned}
\frac{1}{T_3-T_2} \sum_{t=T_2}^{T_3} L_{j}(w_i^{(t)}, v_i^{(t)}) 
&{\leq} L_{j}(w_i^*, v_i^*) + \frac{L^2\Delta_0 }{T_3-T_2}  \\
&\overset{\diamondsuit }{\leq} L_{j}(w_i^*, v_i^*) + \Theta(1) 
\end{aligned}
\end{equation}
where \(\Delta_0=L_{j}(w_i^{(T_1)}, v_i^{(T_1)})-L_{j}(w_i^*, v_i^*)=\Theta(1)\).
In \(\diamondsuit\), we use  \(T_2=\Theta(d)\), and \(L=\Theta(\frac{1}{d} )\) . 
% In \(\heartsuit\), we use \(w_i^*=\alpha^*_{i,j}\mathbf{M}_j, V_i^*=\alpha^*_{i,j}\mathbf{H}_j\) and \(L_{C,j}(w_i^*, v_i^*)=\Theta(\frac{1}{d})\) if \( x_{p} \) contains the true feature \( \mathbf{M}_j \). 


Generalized to every \(j \in d\), the same convergence holds for all \(i \in S_{j, \text{sure}}\) when \((x_p,y_p)\) and \((x_n,y_n)\) contain feature \(j,j'\). 
For all \((x_p,y_p)\) and \((x_n,y_n)\) in \(S_w\), the following inequality holds:
\begin{equation}
\begin{aligned}
\frac{1}{T_3-T_2} \sum_{t=T_2}^{T_3} L(f^{(T_3)}, h^{(T_3)}) 
&\leq L(f^*, h^*) + \Theta(1).
\end{aligned}
\end{equation}

\subsection{Summary}
\label{sec:summary_recaptioned}

ITCP trained on recaptioned data \(\tilde{S}\) proceeds according to Eq.~(\ref{eqn:contrastive}). After \(T = \Theta(d^2 \log d)\) SGD iterations with batch size \(B = \Omega(d)\) and learning rate \(\eta = O(1)\), the returned weights \((\widetilde{\mathbf{W}}, \widetilde{\mathbf{V}})\) achieve a contrastive loss that is asymptotically optimal:
\begin{equation}
\frac{\tilde{L}(f_{\widetilde{\mathbf{W}}}, h_{\widetilde{\mathbf{V}}}) - \tilde{L}^*}{\left| \tilde{L}^* \right|} \le o(1).
\end{equation}

Each neuron pair \((\tilde{w}_i, \tilde{v}_i)\) in \((\widetilde{\mathbf{W}}, \widetilde{\mathbf{V}})\), for \(i \in [m]\), primarily encodes a single aligned feature indexed by a set \(\tilde{N}_i \subseteq [d]\), with \(|\tilde{N}_i| = 1\). Specifically, we have:
\begin{equation}
\begin{aligned}
\tilde{w}_i &= \sum_{j \in \tilde{N}_i} \tilde{\alpha}_{i,j} \mathbf{M}_j 
+ \sum_{j \in [d] \setminus \tilde{N}_i} \tilde{\beta}_{i,j} \mathbf{M}_j 
+ \sum_{j \in [d_1] \setminus [d]} \tilde{\gamma}_{i,j} \mathbf{M}_j^\perp, \\
\tilde{v}_i &= \sum_{j \in \tilde{N}_i} \tilde{\alpha}_{i,j} \mathbf{H}_j 
+ \sum_{j \in [d] \setminus \tilde{N}_i} \tilde{\beta}_{i,j} \mathbf{H}_j 
+ \sum_{j \in [d_1] \setminus [d]} \tilde{\gamma}_{i,j} \mathbf{H}_j^\perp,
\end{aligned}
\end{equation}
\normalsize
where \(\tilde{\alpha}_{i,j}^2 = \Theta(\|\tilde{w}_i\|_2^2 + \|\tilde{v}_i\|_2^2)\), and the residual terms satisfy \(\tilde{\beta}_{i,j}/\tilde{\alpha}_{i,j} \le O(1/\sqrt{d})\), \(\tilde{\gamma}_{i,j}/\tilde{\alpha}_{i,j} \le O(1/\sqrt{d_1})\).

Moreover, for every feature index \(j \in [d]\), there exists an \(\Omega(1)\) many of neurons \(i \in [m]\) such that \(\tilde{N}_i = \{j\}\), indicating that each semantic concept is distinctly captured by dedicated neuron pairs.




\section{Downstream Task}
\label{appendix:downstream}

We consider the same zero-shot classification task as in Section~\ref{sec:zeroshot}, where the image \(x\) and the class-wise text prompts \(\{y_k\}_{k=1}^K\) are given. Each prompt \(y_k\) corresponds to one of \(K\) class labels, and the goal is to classify \(x\) into the class with the best matching prompt.

Each text prompt \(y_k\) is generated as:
\begin{equation}
y_k = \mathbf{H} z_{y_k}' + \xi_{y_k}, \quad \| z_{y_k}' \|_0 = \Theta(1), \quad \| z_{y_k}' \|_{\max} = \Theta(1).
\end{equation}

Each test image \(x\) is generated as:
\begin{equation}
x = \mathbf{M}' z_x' + \xi_x, \quad \| z_x' \|_0 = \Theta(1), \quad \| z_x' \|_{\max} = \Theta(1),
\end{equation}
where \(\mathbf{M}' = \mathbf{M} \mathbf{P}_1\), and
\begin{equation}
\max_{i,j} |(\mathbf{P}_1)_{ij} - \delta_{ij}| \le O(1/\sqrt{d}).
\end{equation}

If \(x\) belongs to class \(k\), then:
\begin{equation}
\left\| (z_x')^{\top} z_{y_k}' \right\|_2 > \left\| (z_x')^{\top} z_{y_{k'}}' \right\|_2, \quad \forall k' \neq k.
\end{equation}

Using Eq.~(\ref{eq:alignment_strength_sure2}) and Eq.~(\ref{eq:alignment_strength_not_sure2}), let \(f(x)\) and \(h(y)\) represent the image encoder and text encoder of ITCP on raw data, respectively. Given a data sample \(x\) containing \(\mathbf{M}_j\) and \(y\) containing \(\mathbf{H}_{j'}\), where \(j'\) is the spurious feature corresponding to \(j\), it holds with high probability that:
\begin{equation}
\left\langle \frac{f(x)}{\| f(x) \|_2}, \frac{h(y)}{\| h(y) \|_2} \right\rangle = \Theta(1).
\end{equation}
This result implies that the image and text encoders of ITCP on raw data struggle to distinguish between features \(j\) and \(j'\), leading to misclassification caused by spurious correlations.

However, using Eq.~(\ref{eq:re_alignment_strength_sure2}) and Eq.~(\ref{eq:re_alignment_strength_not_sure2}), let \(\tilde{f}(x)\) and \(\tilde{g}(y_k)\) denote the image and text encoders of ITCP on recaptioned data. Given \(x\) containing \(\mathbf{M}_j\) and \(y\) containing spurious \(\mathbf{H}_{j'}\), it holds with high probability \(1 - \Theta\left(\frac{1}{d}\right)\) that:
\begin{equation}
\left\langle \frac{\tilde{f}(x)}{\|\tilde{f}(x)\|_2}, \frac{\tilde{g}(y)}{\|\tilde{g}(y)\|_2} \right\rangle \leq \Theta\left(\frac{1}{d}\right).
\end{equation}
This result implies that the image and text encoders of ITCP on synthetic data are capable of effectively distinguishing the true feature from the spurious feature.

Because \(K = \Theta(1)\) and \(\|z_{y_k}\|_0 = \Theta(1)\), we only have constant class classification and constant features in images. Thus, we have:

1. For the image encoder \(f(x)\) and text encoder \(h(y_k)\) of ITCP on raw data:
\begin{equation}
\Pr\left( \arg\max_k \langle f(x), h(y_k) \rangle = k_x \right) = 1 - \Theta(1),
\end{equation}

2. For the image encoder \(\tilde{f}(x)\) and text encoder \(\tilde{g}(y_k)\) of ITCP on synthetic data:
\begin{equation}
\Pr\left( \arg\max_k \langle \tilde{f}(x), \tilde{g}(y_k) \rangle = k_x \right) = 1 - o(1).
\end{equation}




\section*{NeurIPS Paper Checklist}

\begin{enumerate}

\item {\bf Claims}
    \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope?
    \item[] Answer: \answerYes{}
    \item[] Justification: The claims regarding the improvement of feature purity and zero-shot performance through recaption are stated clearly in the abstract and introduction, and validated by theoretical and empirical analysis.

\item {\bf Limitations}
    \item[] Question: Does the paper discuss the limitations of the work performed by the authors?
    \item[] Answer: \answerYes{}
    \item[] Justification: We discuss the limitations in the Conclusion.

\item {\bf Theory assumptions and proofs}
    \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
    \item[] Answer: \answerYes{}
    \item[] Justification: All theoretical results are stated with full assumptions, and complete proofs are provided in the Appendix. Intuitive proof sketches are also included to aid understanding.

\item {\bf Experimental result reproducibility}
    \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
    \item[] Answer: \answerYes{}
    \item[] Justification: We outline all datasets, models, and training procedures in Section~\ref{sec:synthetic}.

\item {\bf Open access to data and code}
    \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
    \item[] Answer: \answerYes{}
    \item[] Justification: We will provide public GitHub access post-review.

\item {\bf Experimental setting/details}
    \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
    \item[] Answer: \answerYes{}
    \item[] Justification: We provide the details of data generation, dimensionality, noise setup, model size, and SGD training parameters including batch size and learning rate in Section~\ref{sec:synthetic}.

\item {\bf Experiment statistical significance}
    \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
    \item[] Answer: \answerYes{}
    \item[] Justification: Each reported metric (accuracy, Silhouette Score, feature purity, random selection of neurons) is averaged over 20 random seeds with 1-sigma standard deviation in Section~\ref{sec:synthetic}.

\item {\bf Experiments compute resources}
    \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
    \item[] Answer: \answerYes{}
    \item[] Justification: The experimental resources are introduced in Appendix~\ref{sec:clip-laclip-extra}.

\item {\bf Code of ethics}
    \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics \url{https://neurips.cc/public/EthicsGuidelines}?
    \item[] Answer: \answerYes{}
    \item[] Justification: We comply with all ethical guidelines. All data used is publicly available under appropriate licenses; no human subjects or sensitive data are involved.

\item {\bf Broader impacts}
    \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
    \item[] Answer: \answerNA{}
    \item[] Justification: There is no societal impact of the work performed.

\item {\bf Safeguards}
    \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
    \item[] Answer: \answerNA{}
    \item[] Justification: Our work does not involve generative models or web-scraped datasets with safety concerns.

\item {\bf Licenses for existing assets}
    \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
    \item[] Answer: \answerYes{}
    \item[] Justification: We cite and respect the licenses of models and datasets.

\item {\bf New assets}
    \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
    \item[] Answer: \answerNA{}
    \item[] Justification: This paper does not release new assets.

\item {\bf Crowdsourcing and research with human subjects}
    \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?
    \item[] Answer: \answerNA{}
    \item[] Justification: This paper does not involve crowdsourcing nor research with human subjects.

\item {\bf Institutional review board (IRB) approvals or equivalent for research with human subjects}
    \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
    \item[] Answer: \answerNA{}
    \item[] Justification: No human subject research is conducted.

\item {\bf Declaration of LLM usage}
    \item[] Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research?
    \item[] Answer: \answerNA{}
    \item[] Justification: No LLMs were used in this work.
\end{enumerate}
































\iffalse

\section{Submission of papers to NeurIPS 2025}


Please read the instructions below carefully and follow them faithfully.


\subsection{Style}


Papers to be submitted to NeurIPS 2025 must be prepared according to the
instructions presented here. Papers may only be up to {\bf nine} pages long,
including figures.
% Additional pages \emph{containing only acknowledgments and references} are allowed.
Additional pages \emph{containing references, checklist, and the optional technical appendices} do not count as content pages.
Papers that exceed the page limit will not be
reviewed, or in any other way considered for presentation at the conference.


The margins in 2025 are the same as those in previous years.


Authors are required to use the NeurIPS \LaTeX{} style files obtainable at the
NeurIPS website as indicated below. Please make sure you use the current files
and not previous versions. Tweaking the style files may be grounds for
rejection.


\subsection{Retrieval of style files}


The style files for NeurIPS and other conference information are available on
the website at
\begin{center}
  \url{https://neurips.cc}
\end{center}
The file \verb+neurips_2025.pdf+ contains these instructions and illustrates the
various formatting requirements your NeurIPS paper must satisfy.


The only supported style file for NeurIPS 2025 is \verb+neurips_2025.sty+,
rewritten for \LaTeXe{}.  \textbf{Previous style files for \LaTeX{} 2.09,
  Microsoft Word, and RTF are no longer supported!}


The \LaTeX{} style file contains three optional arguments: \verb+final+, which
creates a camera-ready copy, \verb+preprint+, which creates a preprint for
submission to, e.g., arXiv, and \verb+nonatbib+, which will not load the
\verb+natbib+ package for you in case of package clash.


\paragraph{Preprint option}
If you wish to post a preprint of your work online, e.g., on arXiv, using the
NeurIPS style, please use the \verb+preprint+ option. This will create a
nonanonymized version of your work with the text ``Preprint. Work in progress.''
in the footer. This version may be distributed as you see fit, as long as you do not say which conference it was submitted to. Please \textbf{do
  not} use the \verb+final+ option, which should \textbf{only} be used for
papers accepted to NeurIPS.


At submission time, please omit the \verb+final+ and \verb+preprint+
options. This will anonymize your submission and add line numbers to aid
review. Please do \emph{not} refer to these line numbers in your paper as they
will be removed during generation of camera-ready copies.


The file \verb+neurips_2025.tex+ may be used as a ``shell'' for writing your
paper. All you have to do is replace the author, title, abstract, and text of
the paper with your own.


The formatting instructions contained in these style files are summarized in
Sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.


\section{General formatting instructions}
\label{gen_inst}


The text must be confined within a rectangle 5.5~inches (33~picas) wide and
9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).  Use 10~point
type with a vertical spacing (leading) of 11~points.  Times New Roman is the
preferred typeface throughout, and will be selected for you by default.
Paragraphs are separated by \nicefrac{1}{2}~line space (5.5 points), with no
indentation.


The paper title should be 17~point, initial caps/lower case, bold, centered
between two horizontal rules. The top rule should be 4~points thick and the
bottom rule should be 1~point thick. Allow \nicefrac{1}{4}~inch space above and
below the title to rules. All pages should start at 1~inch (6~picas) from the
top of the page.


For the final version, authors' names are set in boldface, and each name is
centered above the corresponding address. The lead author's name is to be listed
first (left-most), and the co-authors' names (if different address) are set to
follow. If there is only one co-author, list both author and co-author side by
side.


Please pay special attention to the instructions in Section \ref{others}
regarding figures, tables, acknowledgments, and references.


\section{Headings: first level}
\label{headings}


All headings should be lower case (except for first word and proper nouns),
flush left, and bold.


First-level headings should be in 12-point type.


\subsection{Headings: second level}


Second-level headings should be in 10-point type.


\subsubsection{Headings: third level}


Third-level headings should be in 10-point type.


\paragraph{Paragraphs}


There is also a \verb+\paragraph+ command available, which sets the heading in
bold, flush left, and inline with the text, with the heading followed by 1\,em
of space.


\section{Citations, figures, tables, references}
\label{others}


These instructions apply to everyone.


\subsection{Citations within the text}


The \verb+natbib+ package will be loaded for you by default.  Citations may be
author/year or numeric, as long as you maintain internal consistency.  As to the
format of the references themselves, any style is acceptable as long as it is
used consistently.


The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\cite+, which produces citations appropriate for
use in inline text.  For example,
\begin{verbatim}
   \cite{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}


If you wish to load the \verb+natbib+ package with options, you may add the
following before loading the \verb+neurips_2025+ package:
\begin{verbatim}
   \PassOptionsToPackage{options}{natbib}
\end{verbatim}


If \verb+natbib+ clashes with another package you load, you can add the optional
argument \verb+nonatbib+ when loading the style file:
\begin{verbatim}
   \usepackage[nonatbib]{neurips_2025}
\end{verbatim}


As submission is double blind, refer to your own published work in the third
person. That is, use ``In the previous work of Jones et al.\ [4],'' not ``In our
previous work [4].'' If you cite your other papers that are not widely available
(e.g., a journal paper under review), use anonymous author names in the
citation, e.g., an author of the form ``A.\ Anonymous'' and include a copy of the anonymized paper in the supplementary material.


\subsection{Footnotes}


Footnotes should be used sparingly.  If you do require a footnote, indicate
footnotes with a number\footnote{Sample of the first footnote.} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches (12~picas).


Note that footnotes are properly typeset \emph{after} punctuation
marks.\footnote{As in this example.}


\subsection{Figures}


\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
\end{figure}


All artwork must be neat, clean, and legible. Lines should be dark enough for
purposes of reproduction. The figure number and caption always appear after the
figure. Place one line space before the figure caption and one line space after
the figure. The figure caption should be lower case (except for first word and
proper nouns); figures are numbered consecutively.


You may use color figures.  However, it is best for the figure captions and the
paper body to be legible if the paper is printed in either black/white or in
color.


\subsection{Tables}


All tables must be centered, neat, clean and legible.  The table number and
title always appear before the table.  See Table~\ref{sample-table}.


Place one line space before the table title, one line space after the
table title, and one line space after the table. The table title must
be lower case (except for first word and proper nouns); tables are
numbered consecutively.


Note that publication-quality tables \emph{do not contain vertical rules.} We
strongly suggest the use of the \verb+booktabs+ package, which allows for
typesetting high-quality, professional tables:
\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}
This package was used to typeset Table~\ref{sample-table}.


\begin{table}
  \caption{Sample table title}
  \label{sample-table}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Math}
Note that display math in bare TeX commands will not create correct line numbers for submission. Please use LaTeX (or AMSTeX) commands for unnumbered display math. (You really shouldn't be using \$\$ anyway; see \url{https://tex.stackexchange.com/questions/503/why-is-preferable-to} and \url{https://tex.stackexchange.com/questions/40492/what-are-the-differences-between-align-equation-and-displaymath} for more information.)

\subsection{Final instructions}

Do not change any aspects of the formatting parameters in the style files.  In
particular, do not modify the width or length of the rectangle the text should
fit into, and do not change font sizes (except perhaps in the
\textbf{References} section; see below). Please note that pages should be
numbered.


\section{Preparing PDF files}


Please prepare submission files with paper size ``US Letter,'' and not, for
example, ``A4.''


Fonts were the main cause of problems in the past years. Your PDF file must only
contain Type 1 or Embedded TrueType fonts. Here are a few instructions to
achieve this.


\begin{itemize}


\item You should directly generate PDF files using \verb+pdflatex+.


\item You can check which fonts a PDF files uses.  In Acrobat Reader, select the
  menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can
  also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is
  available out-of-the-box on most Linux machines.


\item \verb+xfig+ "patterned" shapes are implemented with bitmap fonts.  Use
  "solid" shapes instead.


\item The \verb+\bbold+ package almost always uses bitmap fonts.  You should use
  the equivalent AMS Fonts:
\begin{verbatim}
   \usepackage{amsfonts}
\end{verbatim}
followed by, e.g., \verb+\mathbb{R}+, \verb+\mathbb{N}+, or \verb+\mathbb{C}+
for $\mathbb{R}$, $\mathbb{N}$ or $\mathbb{C}$.  You can also use the following
workaround for reals, natural and complex:
\begin{verbatim}
   \newcommand{\RR}{I\!\!R} %real numbers
   \newcommand{\Nat}{I\!\!N} %natural numbers
   \newcommand{\CC}{I\!\!\!\!C} %complex numbers
\end{verbatim}
Note that \verb+amsfonts+ is automatically loaded by the \verb+amssymb+ package.


\end{itemize}


If your file contains type 3 fonts or non embedded TrueType fonts, we will ask
you to fix it.


\subsection{Margins in \LaTeX{}}


Most of the margin problems come from figures positioned by hand using
\verb+\special+ or other commands. We suggest using the command
\verb+\includegraphics+ from the \verb+graphicx+ package. Always specify the
figure width as a multiple of the line width as in the example below:
\begin{verbatim}
   \usepackage[pdftex]{graphicx} ...
   \includegraphics[width=0.8\linewidth]{myfile.pdf}
\end{verbatim}
See Section 4.4 in the graphics bundle documentation
(\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})


A number of width problems arise when \LaTeX{} cannot properly hyphenate a
line. Please give LaTeX hyphenation hints using the \verb+\-+ command when
necessary.

\begin{ack}
Use unnumbered first level headings for the acknowledgments. All acknowledgments
go at the end of the paper before the list of references. Moreover, you are required to declare
funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).
More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2025/PaperInformation/FundingDisclosure}.


Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the \texttt{ack} environment provided in the style file to automatically hide this section in the anonymized submission.
\end{ack}

\section*{References}


References follow the acknowledgments in the camera-ready paper. Use unnumbered first-level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
when listing the references.
Note that the Reference section does not count towards the page limit.
\medskip


{
\small


[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
(eds.), {\it Advances in Neural Information Processing Systems 7},
pp.\ 609--616. Cambridge, MA: MIT Press.


[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
  Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
TELOS/Springer--Verlag.


[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
recall at excitatory recurrent synapses and cholinergic modulation in rat
hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

\section{Technical Appendices and Supplementary Material}
Technical appendices with additional results, figures, graphs and proofs may be submitted with the paper submission before the full submission deadline (see above), or as a separate PDF in the ZIP file below before the supplementary material deadline. There is no page limit for the technical appendices.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section*{NeurIPS Paper Checklist}

%%% BEGIN INSTRUCTIONS %%%
The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: {\bf The papers not including the checklist will be desk rejected.} The checklist should follow the references and follow the (optional) supplemental material.  The checklist does NOT count towards the page
limit. 

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:
\begin{itemize}
    \item You should answer \answerYes{}, \answerNo{}, or \answerNA{}.
    \item \answerNA{} means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
    \item Please provide a short (1–2 sentence) justification right after your answer (even for NA). 
   % \item {\bf The papers not including the checklist will be desk rejected.}
\end{itemize}

{\bf The checklist answers are an integral part of your paper submission.} They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "\answerYes{}" is generally preferable to "\answerNo{}", it is perfectly acceptable to answer "\answerNo{}" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "\answerNo{}" or "\answerNA{}" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer \answerYes{} to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:
\begin{itemize}
    \item {\bf Delete this instruction block, but keep the section heading ``NeurIPS Paper Checklist"},
    \item  {\bf Keep the checklist subsection headings, questions/answers and guidelines below.}
    \item {\bf Do not modify the questions and only use the provided macros for your answers}.
\end{itemize} 
 

%%% END INSTRUCTIONS %%%


\begin{enumerate}

\item {\bf Claims}
    \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the abstract and introduction do not include the claims made in the paper.
        \item The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. 
        \item The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. 
        \item It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 
    \end{itemize}

\item {\bf Limitations}
    \item[] Question: Does the paper discuss the limitations of the work performed by the authors?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 
        \item The authors are encouraged to create a separate "Limitations" section in their paper.
        \item The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
        \item The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
        \item The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
        \item The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
        \item If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
        \item While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
    \end{itemize}

\item {\bf Theory assumptions and proofs}
    \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include theoretical results. 
        \item All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
        \item All assumptions should be clearly stated or referenced in the statement of any theorems.
        \item The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. 
        \item Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
        \item Theorems and Lemmas that the proof relies upon should be properly referenced. 
    \end{itemize}

    \item {\bf Experimental result reproducibility}
    \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
        \item If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 
        \item Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
        \item While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
        \begin{enumerate}
            \item If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
            \item If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
            \item If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
            \item We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
        \end{enumerate}
    \end{itemize}


\item {\bf Open access to data and code}
    \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that paper does not include experiments requiring code.
        \item Please see the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
        \item While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
        \item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
        \item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
        \item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
        \item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
        \item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
    \end{itemize}


\item {\bf Experimental setting/details}
    \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
        \item The full details can be provided either with the code, in appendix, or as supplemental material.
    \end{itemize}

\item {\bf Experiment statistical significance}
    \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
        \item The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
        \item The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
        \item The assumptions made should be given (e.g., Normally distributed errors).
        \item It should be clear whether the error bar is the standard deviation or the standard error of the mean.
        \item It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96\% CI, if the hypothesis of Normality of errors is not verified.
        \item For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
        \item If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
    \end{itemize}

\item {\bf Experiments compute resources}
    \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
        \item The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 
        \item The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 
    \end{itemize}
    
\item {\bf Code of ethics}
    \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics \url{https://neurips.cc/public/EthicsGuidelines}?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
        \item If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
        \item The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
    \end{itemize}


\item {\bf Broader impacts}
    \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that there is no societal impact of the work performed.
        \item If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
        \item Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
        \item The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
        \item The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
        \item If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
    \end{itemize}
    
\item {\bf Safeguards}
    \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper poses no such risks.
        \item Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 
        \item Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
        \item We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
    \end{itemize}

\item {\bf Licenses for existing assets}
    \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not use existing assets.
        \item The authors should cite the original paper that produced the code package or dataset.
        \item The authors should state which version of the asset is used and, if possible, include a URL.
        \item The name of the license (e.g., CC-BY 4.0) should be included for each asset.
        \item For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
        \item If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, \url{paperswithcode.com/datasets} has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
        \item For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
        \item If this information is not available online, the authors are encouraged to reach out to the asset's creators.
    \end{itemize}

\item {\bf New assets}
    \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not release new assets.
        \item Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. 
        \item The paper should discuss whether and how consent was obtained from people whose asset is used.
        \item At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
    \end{itemize}

\item {\bf Crowdsourcing and research with human subjects}
    \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? 
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
        \item Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. 
        \item According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 
    \end{itemize}

\item {\bf Institutional review board (IRB) approvals or equivalent for research with human subjects}
    \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
        \item Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 
        \item We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. 
        \item For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
    \end{itemize}

\item {\bf Declaration of LLM usage}
    \item[] Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required.
    %this research? 
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components.
        \item Please refer to our LLM policy (\url{https://neurips.cc/Conferences/2025/LLM}) for what should or should not be described.
    \end{itemize}

\end{enumerate}

\fi







\end{document}